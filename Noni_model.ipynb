{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43cc344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34caca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4860e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_labels = {0: 'Ｘ', 1: 'Ｏ', 2: 'Ｖ', -100:'IGN'}\n",
    "labels_to_ids = {'Ｘ': 0, 'Ｏ': 1, 'Ｖ':2,  'IGN':-100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb6bc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ｘ', 1: 'Ｏ', 2: 'Ｖ', -100: 'IGN'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1787914b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ｘ': 0, 'Ｏ': 1, 'Ｖ': 2, 'IGN': -100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a1ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizerFast.from_pretrained('cl-tohoku/bert-large-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53930828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Noni_data_ipa_all_tag.txt', 'r') as filein:\n",
    "    wiki_tag = filein.readlines()\n",
    "    filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03fac487",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tag_all = [''.join(wiki_tag[i].strip()) for i in range(len(wiki_tag))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b804bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ＸＸＸＸＸＸＸＸＸＸＯＯＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tag_all[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962da953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "jp_train_tag, jp_test_tag = train_test_split(wiki_tag_all, random_state=55, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db66e3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jp_train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a0cf645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4370\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_train_tag:\n",
    "    if 'Ｏ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6434e4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4337\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_train_tag:\n",
    "    if 'Ｖ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45466d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jp_test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5223e8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_test_tag:\n",
    "    if 'Ｏ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0665e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1913\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_test_tag:\n",
    "    if 'Ｖ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a7b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Noni_data_ipa_all.txt', 'r') as filein:\n",
    "    wiki_sent = filein.readlines()\n",
    "    filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563df8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sent_all = [wiki_sent[i].strip() for i in range(len(wiki_sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0721637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_train_texts, jp_test_texts = train_test_split(wiki_sent_all, random_state=55, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33284c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['また、逆に、係助詞を使わないのに文末が連体形で結ばれる例も多くなってくる。',\n",
       " '日本の蘚苔類学会のある年に行なわれた観察会では、山間部の渓谷にコースを設定してあったのにその入り口の駐車場周辺だけで1日を過ごしてしまったとの伝説がある。',\n",
       " '彼はそこで、（後に弟子のプラトンがオルペウス教（ピタゴラス教団）的な輪廻転生説に嵌っていくのとは対照的に）死後のことについては一切わからないという不可知論の立場を採る（死刑確定前の弁明においても、「死後のことを知っている者など誰もいないのに人々はそれを最大の悪であるかのように恐れる。',\n",
       " 'これを自動化することは、技術的には可能であるが、セキュリティの確保と利便性の向上は常にトレードオフの関係になり、ちょうど良い頃合いを見定めるのにしばらく時間がかかりそうである。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sent_all[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8d912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for num in range(len(wiki_sent_all)):\n",
    "    text.append(len(wiki_sent_all[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0500b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "count  25000.000000\n",
      "mean      54.886480\n",
      "std       43.239797\n",
      "min        2.000000\n",
      "25%       26.000000\n",
      "50%       41.000000\n",
      "75%       71.000000\n",
      "max      682.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "df = pd.DataFrame(text)\n",
    "print(df.describe())\n",
    "df.plot.box(title=\"Box Chart\")\n",
    "plt.grid(linestyle=\"--\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ea91563",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for num in range(len(wiki_sent_all)):\n",
    "    text.append(len(wiki_sent_all[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1776d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e772ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'そうした中で、比定の大きな根拠となっていた『神護寺略記』と「大英博物館本源頼朝像」は、神護寺三像との関係は否定されているのに像主名だけは変更されずにいる不条理を指摘する。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18462836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 11168, 828, 5215, 893, 828, 1191, 1427, 4896, 932, 1169, 929, 14860, 6462, 6416, 2686, 2827, 861, 5231, 1153, 2271, 888, 4176, 897, 11304, 1174, 916, 1842, 865, 12241, 6632, 25327, 829, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(wiki_sent_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36832c99",
   "metadata": {},
   "source": [
    "## 檢查是否有None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4f7851f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(wiki_sent_all)):\n",
    "    data_sent =[wiki_sent_all[i][word] for word in range(len(wiki_sent_all[i]))]\n",
    "    data_label =[wiki_tag_all[i][word] for word in range(len(wiki_tag_all[i]))]\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    if len(wiki_sent_all[i]) == len(wiki_tag_all[i]):\n",
    "#         print(i,'OK')\n",
    "        continue\n",
    "    else:\n",
    "        df = pd.DataFrame([data_sent, data_label], index=[\"sent\", \"label\"])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "122f7fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['パークゴルフ場で風の強さや向きを知るのに使われている。',\n",
       " '当該歓楽街の中心的客層が、深夜割増料金タクシーで自宅に帰るのに必要な金額とほぼ同じかやや安く中心価格帯が設定されている。']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sent_all[8000:8002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9494d0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>能</td>\n",
       "      <td>力</td>\n",
       "      <td>と</td>\n",
       "      <td>才</td>\n",
       "      <td>能</td>\n",
       "      <td>が</td>\n",
       "      <td>足</td>\n",
       "      <td>ら</td>\n",
       "      <td>な</td>\n",
       "      <td>い</td>\n",
       "      <td>の</td>\n",
       "      <td>に</td>\n",
       "      <td>強</td>\n",
       "      <td>い</td>\n",
       "      <td>て</td>\n",
       "      <td>他</td>\n",
       "      <td>人</td>\n",
       "      <td>の</td>\n",
       "      <td>働</td>\n",
       "      <td>き</td>\n",
       "      <td>に</td>\n",
       "      <td>頼</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       "sent   能  力  と  才  能  が  足  ら  な  い  の  に  強  い  て  他  人  の  働  き  に  頼  る  。\n",
       "label  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_try = [wiki_sent_all[457][word] for word in range(len(wiki_sent_all[457]))]\n",
    "label_try = [wiki_tag_all[457][word] for word in range(len(wiki_tag_all[457]))]\n",
    "df = pd.DataFrame([sent_try, label_try], index=[\"sent\", \"label\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "221cddae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>こ</td>\n",
       "      <td>の</td>\n",
       "      <td>結</td>\n",
       "      <td>果</td>\n",
       "      <td>、</td>\n",
       "      <td>例</td>\n",
       "      <td>え</td>\n",
       "      <td>ば</td>\n",
       "      <td>旧</td>\n",
       "      <td>暦</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>「</td>\n",
       "      <td>秋</td>\n",
       "      <td>」</td>\n",
       "      <td>で</td>\n",
       "      <td>あ</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>「</td>\n",
       "      <td>文</td>\n",
       "      <td>月</td>\n",
       "      <td>（</td>\n",
       "      <td>7</td>\n",
       "      <td>月</td>\n",
       "      <td>）</td>\n",
       "      <td>」</td>\n",
       "      <td>が</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>「</td>\n",
       "      <td>夏</td>\n",
       "      <td>」</td>\n",
       "      <td>に</td>\n",
       "      <td>な</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>、</td>\n",
       "      <td>7</td>\n",
       "      <td>月</td>\n",
       "      <td>9</td>\n",
       "      <td>日</td>\n",
       "      <td>頃</td>\n",
       "      <td>か</td>\n",
       "      <td>ら</td>\n",
       "      <td>8</td>\n",
       "      <td>月</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>日</td>\n",
       "      <td>頃</td>\n",
       "      <td>ま</td>\n",
       "      <td>で</td>\n",
       "      <td>で</td>\n",
       "      <td>あ</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>二</td>\n",
       "      <td>百</td>\n",
       "      <td>十</td>\n",
       "      <td>日</td>\n",
       "      <td>が</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>9</td>\n",
       "      <td>月</td>\n",
       "      <td>1</td>\n",
       "      <td>日</td>\n",
       "      <td>に</td>\n",
       "      <td>な</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>、</td>\n",
       "      <td>盆</td>\n",
       "      <td>の</td>\n",
       "      <td>節</td>\n",
       "      <td>会</td>\n",
       "      <td>を</td>\n",
       "      <td>行</td>\n",
       "      <td>う</td>\n",
       "      <td>時</td>\n",
       "      <td>期</td>\n",
       "      <td>が</td>\n",
       "      <td>地</td>\n",
       "      <td>域</td>\n",
       "      <td>に</td>\n",
       "      <td>よ</td>\n",
       "      <td>っ</td>\n",
       "      <td>て</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>7</td>\n",
       "      <td>月</td>\n",
       "      <td>と</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>8</td>\n",
       "      <td>月</td>\n",
       "      <td>に</td>\n",
       "      <td>別</td>\n",
       "      <td>れ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>な</td>\n",
       "      <td>ど</td>\n",
       "      <td>、</td>\n",
       "      <td>月</td>\n",
       "      <td>遅</td>\n",
       "      <td>れ</td>\n",
       "      <td>に</td>\n",
       "      <td>よ</td>\n",
       "      <td>る</td>\n",
       "      <td>そ</td>\n",
       "      <td>れ</td>\n",
       "      <td>ま</td>\n",
       "      <td>で</td>\n",
       "      <td>の</td>\n",
       "      <td>慣</td>\n",
       "      <td>習</td>\n",
       "      <td>と</td>\n",
       "      <td>の</td>\n",
       "      <td>相</td>\n",
       "      <td>違</td>\n",
       "      <td>が</td>\n",
       "      <td>発</td>\n",
       "      <td>生</td>\n",
       "      <td>し</td>\n",
       "      <td>て</td>\n",
       "      <td>い</td>\n",
       "      <td>る</td>\n",
       "      <td>ほ</td>\n",
       "      <td>か</td>\n",
       "      <td>、</td>\n",
       "      <td>前</td>\n",
       "      <td>記</td>\n",
       "      <td>の</td>\n",
       "      <td>よ</td>\n",
       "      <td>う</td>\n",
       "      <td>な</td>\n",
       "      <td>元</td>\n",
       "      <td>々</td>\n",
       "      <td>の</td>\n",
       "      <td>中</td>\n",
       "      <td>国</td>\n",
       "      <td>風</td>\n",
       "      <td>の</td>\n",
       "      <td>定</td>\n",
       "      <td>義</td>\n",
       "      <td>も</td>\n",
       "      <td>絡</td>\n",
       "      <td>み</td>\n",
       "      <td>、</td>\n",
       "      <td>現</td>\n",
       "      <td>在</td>\n",
       "      <td>で</td>\n",
       "      <td>も</td>\n",
       "      <td>若</td>\n",
       "      <td>干</td>\n",
       "      <td>の</td>\n",
       "      <td>違</td>\n",
       "      <td>和</td>\n",
       "      <td>感</td>\n",
       "      <td>が</td>\n",
       "      <td>存</td>\n",
       "      <td>在</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>こ</td>\n",
       "      <td>と</td>\n",
       "      <td>か</td>\n",
       "      <td>ら</td>\n",
       "      <td>、</td>\n",
       "      <td>日</td>\n",
       "      <td>本</td>\n",
       "      <td>の</td>\n",
       "      <td>メ</td>\n",
       "      <td>デ</td>\n",
       "      <td>ィ</td>\n",
       "      <td>ア</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>「</td>\n",
       "      <td>暦</td>\n",
       "      <td>の</td>\n",
       "      <td>上</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>」</td>\n",
       "      <td>\\t</td>\n",
       "      <td>と</td>\n",
       "      <td>前</td>\n",
       "      <td>置</td>\n",
       "      <td>き</td>\n",
       "      <td>し</td>\n",
       "      <td>て</td>\n",
       "      <td>説</td>\n",
       "      <td>明</td>\n",
       "      <td>さ</td>\n",
       "      <td>れ</td>\n",
       "      <td>る</td>\n",
       "      <td>こ</td>\n",
       "      <td>と</td>\n",
       "      <td>が</td>\n",
       "      <td>あ</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17   \\\n",
       "sent    こ   の   結   果   、   例   え   ば   旧   暦   で   は   「   秋   」   で   あ   っ   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35   \\\n",
       "sent    た   「   文   月   （   7   月   ）   」   が   新   暦   で   は   「   夏   」   に   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53   \\\n",
       "sent    な   っ   た   り   、   7   月   9   日   頃   か   ら   8   月   1   1   日   頃   \n",
       "label   Ｏ   Ｏ   Ｏ   Ｏ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71   \\\n",
       "sent    ま   で   で   あ   っ   た   二   百   十   日   が   新   暦   9   月   1   日   に   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89   \\\n",
       "sent    な   っ   た   り   、   盆   の   節   会   を   行   う   時   期   が   地   域   に   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      90  91  92  93  94  95  96  97  98  99  100 101 102 103 104 105 106 107  \\\n",
       "sent    よ   っ   て   新   暦   7   月   と   新   暦   8   月   に   別   れ   た   り   す   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｏ   Ｏ   Ｏ   Ｏ   Ｏ   \n",
       "\n",
       "      108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125  \\\n",
       "sent    る   な   ど   、   月   遅   れ   に   よ   る   そ   れ   ま   で   の   慣   習   と   \n",
       "label   Ｏ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143  \\\n",
       "sent    の   相   違   が   発   生   し   て   い   る   ほ   か   、   前   記   の   よ   う   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161  \\\n",
       "sent    な   元   々   の   中   国   風   の   定   義   も   絡   み   、   現   在   で   も   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179  \\\n",
       "sent    若   干   の   違   和   感   が   存   在   す   る   こ   と   か   ら   、   日   本   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197  \\\n",
       "sent    の   メ   デ   ィ   ア   で   は   「   暦   の   上   で   は   …   …   」  \\t   と   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213  \n",
       "sent    前   置   き   し   て   説   明   さ   れ   る   こ   と   が   あ   る   。  \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('Ｘ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c2e68d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-b738ffe15d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ｘ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfileout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   4325\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m             \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4328\u001b[0m         )\n\u001b[1;32m   4329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6081\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6082\u001b[0m                 new_data = self._mgr.fillna(\n\u001b[0;32m-> 6083\u001b[0;31m                     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6084\u001b[0m                 )\n\u001b[1;32m   6085\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"BlockManager\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         return self.apply(\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0;34m\"fillna\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         )\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# equivalent: _try_coerce_args(value) would not raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_downcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# we can't process the value, but nothing to do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_maybe_downcast\u001b[0;34m(self, blocks, downcast)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0;31m# split and convert the blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_hold_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0;31m# split and convert the blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_hold_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, copy, datetime, numeric, timedelta, coerce)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m             \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_and_operate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2444\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36msplit_and_operate\u001b[0;34m(self, mask, f, inplace)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# need a new block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(mask, val, idx)\u001b[0m\n\u001b[1;32m   2431\u001b[0m                 \u001b[0mtimedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m                 \u001b[0mcoerce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m             )\n\u001b[1;32m   2435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36msoft_convert_objects\u001b[0;34m(values, datetime, numeric, timedelta, coerce, copy)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# bound of nanosecond-resolution 64-bit integers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_datetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('check_Tari.txt', 'w') as fileout:\n",
    "    for i in range(len(wiki_sent_all)):\n",
    "        data_sent =[wiki_sent_all[i][word] for word in range(len(wiki_sent_all[i]))]\n",
    "        data_label =[wiki_tag_all[i][word] for word in range(len(wiki_tag_all[i]))]\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        dd = pd.DataFrame([data_sent, data_label], index=[\"sent\", \"label\"])\n",
    "        df = dd.fillna('Ｘ')\n",
    "        answer = ''.join(df.loc['label'])\n",
    "        fileout.write(answer+'\\n')\n",
    "fileout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0b500",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d40935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "#         sentence = self.data.sentence[index].strip().split()\n",
    "        sentence = [i for i in self.data.sentence[index]]\n",
    "#         word_tag_n = [''.join(label) for label in word_labels[index]]\n",
    "#         word_labels = self.data.word_labels[index].split() \n",
    "    \n",
    "        word_labels = [j for j in self.data.word_labels[index]]    \n",
    "\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_pretokenized=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "\n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ba734a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame([jp_train_texts, jp_train_tag], index=[\"sentence\", \"word_labels\"]).T\n",
    "test_dataset = pd.DataFrame([jp_test_texts, jp_test_tag], index=[\"sentence\", \"word_labels\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f098c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>2019年3月16日ダイヤ改正より、東村山駅周辺の高架化工事に伴い、国分寺線で運用する車両の...</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>だから、大人でも子供でもただ一度この機械を回わればすぐ引かれるようになった。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>*2014年2月13日石川県警本部公安課の巡査部長が酒を飲んで当て逃げを起こしていたのに県警...</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＸＸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>新学期が始まった。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>29歳の私は高層ビルにいて、窓から外を見ています。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>焼き畑農業のような最も原始的な農業でも、棒きれのようなものが農具として使われ、土を掘り返すの...</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＶＶＶＶＶ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>緊急援助が行き渡るのにしたがって、次第に活動範囲を広げて1953年に正式名称が現在のものに変...</td>\n",
       "      <td>ＸＸＸＸＸＶＶＶＶＶＶＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1回切るのに5分かかり、1回切るごとに1分休憩すると、何分で切れるか。</td>\n",
       "      <td>ＸＸＶＶＶＶＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>同じ奨学金を取得するのにケベック州民の学生は、CEGEPRスコアは最低35.5が要求される。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>幼い頃から寡黙で、母が病がちだったため祖父母の許で育ったが、両親の許に戻った後は気を使ってい...</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>以前は文芸性の高い小説が好きだったが、今は視野を広げ、経済から科学、推理などの本も読みます。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>クレタ島のミチウリがよく知られており、そこでは小さなパイを作るのに使われている。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＶＶＶＶＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>そして、「海軍が大戦果を上げているのに陸軍が後れをとってはならない」との空気の下、次のような...</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>宣伝期間になると、候補者の旗、ポスター、コマーシャルなどがあちこちにあふれています。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>お金の面から見ると、自分で作るより外の値段は高いです。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "555  2019年3月16日ダイヤ改正より、東村山駅周辺の高架化工事に伴い、国分寺線で運用する車両の...   \n",
       "556             だから、大人でも子供でもただ一度この機械を回わればすぐ引かれるようになった。   \n",
       "557  *2014年2月13日石川県警本部公安課の巡査部長が酒を飲んで当て逃げを起こしていたのに県警...   \n",
       "558                                          新学期が始まった。   \n",
       "559                          29歳の私は高層ビルにいて、窓から外を見ています。   \n",
       "560  焼き畑農業のような最も原始的な農業でも、棒きれのようなものが農具として使われ、土を掘り返すの...   \n",
       "561  緊急援助が行き渡るのにしたがって、次第に活動範囲を広げて1953年に正式名称が現在のものに変...   \n",
       "562                1回切るのに5分かかり、1回切るごとに1分休憩すると、何分で切れるか。   \n",
       "563     同じ奨学金を取得するのにケベック州民の学生は、CEGEPRスコアは最低35.5が要求される。   \n",
       "564  幼い頃から寡黙で、母が病がちだったため祖父母の許で育ったが、両親の許に戻った後は気を使ってい...   \n",
       "565     以前は文芸性の高い小説が好きだったが、今は視野を広げ、経済から科学、推理などの本も読みます。   \n",
       "566           クレタ島のミチウリがよく知られており、そこでは小さなパイを作るのに使われている。   \n",
       "567  そして、「海軍が大戦果を上げているのに陸軍が後れをとってはならない」との空気の下、次のような...   \n",
       "568         宣伝期間になると、候補者の旗、ポスター、コマーシャルなどがあちこちにあふれています。   \n",
       "569                        お金の面から見ると、自分で作るより外の値段は高いです。   \n",
       "\n",
       "                                           word_labels  \n",
       "555  ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...  \n",
       "556             ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "557  ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＸＸ...  \n",
       "558                                          ＸＸＸＸＸＸＸＸＸ  \n",
       "559                          ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "560  ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＶＶＶＶＶ...  \n",
       "561  ＸＸＸＸＸＶＶＶＶＶＶＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...  \n",
       "562                ＸＸＶＶＶＶＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "563     ＸＸＸＸＸＸＸＸＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "564  ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...  \n",
       "565     ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "566           ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＶＶＶＶＸＸＸＸＸＸＸ  \n",
       "567  ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...  \n",
       "568         ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "569                        ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[555:570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0044bb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "のに型 TRAIN Dataset: (17500, 2)\n",
      "のに型 TEST Dataset: (7500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"のに型 TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"のに型 TEST Dataset: {}\".format(test_dataset.shape))\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af2796cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   2,  879,  856,  873,  881, 1052,  888,  828, 3127, 1985,  896, 1846,\n",
       "          863,  892, 2914, 2527,  890,  892,  885,  888,  854,  881,  840, 3918,\n",
       "         4989, 2029, 3712, 4874,  841,  890,  838, 1846, 4518, 1493, 3539, 5729,\n",
       "         2828, 3324, 5685, 2821, 1266,  839,  897,  828, 3918, 4989, 2029, 1036,\n",
       "         1266,  890,  896, 5493, 1191,  897, 1574, 1985,  871,  926,  888,  854,\n",
       "          925,  896,  893, 1266, 1056, 1564,  881,  867,  897, 1836, 2801,  871,\n",
       "          926,  875,  893,  854,  925, 1039, 2848, 3614,  932, 2543, 2618,  875,\n",
       "          925,  829,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'offset_mapping': tensor([[0, 0],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]),\n",
       " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "            1,    1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c0e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "そ           0\n",
      "う           0\n",
      "し           0\n",
      "た           0\n",
      "中           0\n",
      "て           0\n",
      "、           0\n",
      "比           0\n",
      "定           0\n",
      "の           0\n",
      "大           0\n",
      "き           0\n",
      "な           0\n",
      "根           0\n",
      "拠           0\n",
      "と           0\n",
      "な           0\n",
      "っ           0\n",
      "て           0\n",
      "い           0\n",
      "た           0\n",
      "『           0\n",
      "神           0\n",
      "護           0\n",
      "寺           0\n",
      "略           0\n",
      "記           0\n",
      "』           0\n",
      "と           0\n",
      "「           0\n",
      "大           0\n",
      "英           0\n",
      "博           0\n",
      "物           0\n",
      "館           0\n",
      "本           0\n",
      "源           0\n",
      "頼           0\n",
      "朝           0\n",
      "像           0\n",
      "」           0\n",
      "は           0\n",
      "、           0\n",
      "神           0\n",
      "護           0\n",
      "寺           0\n",
      "三           0\n",
      "像           0\n",
      "と           0\n",
      "の           0\n",
      "関           0\n",
      "係           0\n",
      "は           0\n",
      "否           0\n",
      "定           0\n",
      "さ           0\n",
      "れ           0\n",
      "て           0\n",
      "い           1\n",
      "る           1\n",
      "の           1\n",
      "に           1\n",
      "像           0\n",
      "主           0\n",
      "名           0\n",
      "た           0\n",
      "け           0\n",
      "は           0\n",
      "変           0\n",
      "更           0\n",
      "さ           0\n",
      "れ           0\n",
      "す           0\n",
      "に           0\n",
      "い           0\n",
      "る           0\n",
      "不           0\n",
      "条           0\n",
      "理           0\n",
      "を           0\n",
      "指           0\n",
      "摘           0\n",
      "す           0\n",
      "る           0\n",
      "。           0\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
    "    print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4ac2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params) ## **表示字典 一批取4\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0280c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32768, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('cl-tohoku/bert-large-japanese', num_labels=len(labels_to_ids))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a26a6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9838, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = training_set[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d1247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53518d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7353699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model,\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy, tr_opt_acc = 0, 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_opt_zero_steps, tr_opt_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader): # 一次取一批\n",
    "#         print(idx)\n",
    "#         if idx == 193:\n",
    "#             print(batch['input_ids'])\n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels) ## 輸入 算出損失值\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "\n",
    "#         if idx % 100==0:\n",
    "#             loss_step = tr_loss/nb_tr_steps\n",
    "#             print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,) ## 算每一批預測值\n",
    "\n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "        tn, fo, fv, fn, to, fv2, fn2, fo2, tv = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1, 2]).ravel()\n",
    "        tmp_tr_opt_acc = (to+tv)/(fo+fv+fn+fv2+fn2+fo2+to+tv)\n",
    "        if np.isnan(tmp_tr_opt_acc) == True:\n",
    "            tmp_tr_opt_acc = 1\n",
    "            tr_opt_zero_steps += 1\n",
    "        else:\n",
    "            tr_opt_steps += 1\n",
    "            tr_opt_acc += tmp_tr_opt_acc            \n",
    "            \n",
    "            if idx % 100==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "                opt_acc_step = tr_opt_acc / tr_opt_steps\n",
    "                print(f\"Training Opt_acc epoch: {opt_acc_step}\")\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() ## T度值 清空\n",
    "        loss.backward() ## 反向回饋\"修正權重\"\n",
    "        optimizer.step() ## 透過優化器(先算損失值得反向回饋)執行修正\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    tr_opt_acc = tr_opt_acc / tr_opt_steps\n",
    "    \n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training Opt_acc epoch: {tr_opt_acc}\")\n",
    "    print('tr_zero_steps:',tr_opt_zero_steps)\n",
    "    print('tr_opt_steps:',tr_opt_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c5d8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 0.007747213821858168\n",
      "Training Opt_acc epoch: 1.0\n",
      "Training loss per 100 training steps: 0.009488284386042333\n",
      "Training Opt_acc epoch: 0.916702062712701\n",
      "Training loss per 100 training steps: 0.011147295359831387\n",
      "Training Opt_acc epoch: 0.9255767550842396\n",
      "Training loss per 100 training steps: 0.010364967423164704\n",
      "Training Opt_acc epoch: 0.9300504925553807\n",
      "Training loss per 100 training steps: 0.011880030902299457\n",
      "Training Opt_acc epoch: 0.9178325607549317\n",
      "Training loss per 100 training steps: 0.01093976528789938\n",
      "Training Opt_acc epoch: 0.9221632837936938\n",
      "Training loss per 100 training steps: 0.011915411441445024\n",
      "Training Opt_acc epoch: 0.9190874968844228\n",
      "Training loss per 100 training steps: 0.01124170451168977\n",
      "Training Opt_acc epoch: 0.9245832434810457\n",
      "Training loss per 100 training steps: 0.01224364222060571\n",
      "Training Opt_acc epoch: 0.9219462239503079\n",
      "Training loss per 100 training steps: 0.01233806135741392\n",
      "Training Opt_acc epoch: 0.9230333607269141\n",
      "Training loss per 100 training steps: 0.012206486023993724\n",
      "Training Opt_acc epoch: 0.9201883121763776\n",
      "Training loss per 100 training steps: 0.011687906488568395\n",
      "Training Opt_acc epoch: 0.9233287057276988\n",
      "Training loss per 100 training steps: 0.011746584832752078\n",
      "Training Opt_acc epoch: 0.9202445423228253\n",
      "Training loss per 100 training steps: 0.011715988859551354\n",
      "Training Opt_acc epoch: 0.9180647211783104\n",
      "Training loss per 100 training steps: 0.011641299720006326\n",
      "Training Opt_acc epoch: 0.9185644939572849\n",
      "Training loss per 100 training steps: 0.011846150576438244\n",
      "Training Opt_acc epoch: 0.9168561310489532\n",
      "Training loss per 100 training steps: 0.011861979725933277\n",
      "Training Opt_acc epoch: 0.9165889016207178\n",
      "Training loss per 100 training steps: 0.011861374994018214\n",
      "Training Opt_acc epoch: 0.9170283729624135\n",
      "Training loss per 100 training steps: 0.011849819001467872\n",
      "Training Opt_acc epoch: 0.9167107215034761\n",
      "Training loss per 100 training steps: 0.011736406556869432\n",
      "Training Opt_acc epoch: 0.9172518654357402\n",
      "Training loss per 100 training steps: 0.011399933781946571\n",
      "Training Opt_acc epoch: 0.9190129654448336\n",
      "Training loss per 100 training steps: 0.011115970111420334\n",
      "Training Opt_acc epoch: 0.9204428156731987\n",
      "Training loss per 100 training steps: 0.011932448995268284\n",
      "Training Opt_acc epoch: 0.9144224654063203\n",
      "Training loss per 100 training steps: 0.011920329989677087\n",
      "Training Opt_acc epoch: 0.9144766817057315\n",
      "Training loss per 100 training steps: 0.01212742332427202\n",
      "Training Opt_acc epoch: 0.9132431520583545\n",
      "Training loss per 100 training steps: 0.012114563388720188\n",
      "Training Opt_acc epoch: 0.9141390419563804\n",
      "Training loss per 100 training steps: 0.01207400702986829\n",
      "Training Opt_acc epoch: 0.9137679492516038\n",
      "Training loss per 100 training steps: 0.0119994885644881\n",
      "Training Opt_acc epoch: 0.9147758789982396\n",
      "Training loss per 100 training steps: 0.01176344272475692\n",
      "Training Opt_acc epoch: 0.9150155257746634\n",
      "Training loss per 100 training steps: 0.011774781874816537\n",
      "Training Opt_acc epoch: 0.9147699875351419\n",
      "Training loss per 100 training steps: 0.011818539701359683\n",
      "Training Opt_acc epoch: 0.9135641156110423\n",
      "Training loss per 100 training steps: 0.01180127067838131\n",
      "Training Opt_acc epoch: 0.9126016083838572\n",
      "Training loss per 100 training steps: 0.011849763549082482\n",
      "Training Opt_acc epoch: 0.9121969999143742\n",
      "Training loss per 100 training steps: 0.011824314493286706\n",
      "Training Opt_acc epoch: 0.9119118721000359\n",
      "Training loss per 100 training steps: 0.011820312509942563\n",
      "Training Opt_acc epoch: 0.9125868924501863\n",
      "Training loss per 100 training steps: 0.011825020158867912\n",
      "Training Opt_acc epoch: 0.9127615307136283\n",
      "Training loss per 100 training steps: 0.011578827935460055\n",
      "Training Opt_acc epoch: 0.9145143300923627\n",
      "Training loss per 100 training steps: 0.011515001278358606\n",
      "Training Opt_acc epoch: 0.914596639980112\n",
      "Training loss per 100 training steps: 0.01137050887101352\n",
      "Training Opt_acc epoch: 0.915531129204219\n",
      "Training loss per 100 training steps: 0.011325582359048915\n",
      "Training Opt_acc epoch: 0.9158697317035206\n",
      "Training loss epoch: 0.011200857095210397\n",
      "Training accuracy epoch: 0.9961858903747981\n",
      "Training Opt_acc epoch: 0.9164738350032319\n",
      "tr_zero_steps: 302\n",
      "tr_opt_steps: 4073\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.002717255847528577\n",
      "Training Opt_acc epoch: 1.0\n",
      "Training loss per 100 training steps: 0.006841758300174886\n",
      "Training Opt_acc epoch: 0.9657032256755248\n",
      "Training loss per 100 training steps: 0.007349692340221058\n",
      "Training Opt_acc epoch: 0.9588667763499187\n",
      "Training loss per 100 training steps: 0.007292746658601098\n",
      "Training Opt_acc epoch: 0.9579567286647294\n",
      "Training loss per 100 training steps: 0.008547945093842439\n",
      "Training Opt_acc epoch: 0.9497077399479638\n",
      "Training loss per 100 training steps: 0.008374427810502096\n",
      "Training Opt_acc epoch: 0.9470072276530133\n",
      "Training loss per 100 training steps: 0.009212605580187255\n",
      "Training Opt_acc epoch: 0.9467632167062264\n",
      "Training loss per 100 training steps: 0.008857164655034808\n",
      "Training Opt_acc epoch: 0.9493987000002705\n",
      "Training loss per 100 training steps: 0.008736562424005018\n",
      "Training Opt_acc epoch: 0.9482608767548799\n",
      "Training loss per 100 training steps: 0.008688969663870498\n",
      "Training Opt_acc epoch: 0.94811318278095\n",
      "Training loss per 100 training steps: 0.008656464550579319\n",
      "Training Opt_acc epoch: 0.9478431529884108\n",
      "Training loss per 100 training steps: 0.00865972802501772\n",
      "Training Opt_acc epoch: 0.9433155557856776\n",
      "Training loss per 100 training steps: 0.008577101003985758\n",
      "Training Opt_acc epoch: 0.9453857124910491\n",
      "Training loss per 100 training steps: 0.008630868562200154\n",
      "Training Opt_acc epoch: 0.9449308341446442\n",
      "Training loss per 100 training steps: 0.008554366837811701\n",
      "Training Opt_acc epoch: 0.9455854431246988\n",
      "Training loss per 100 training steps: 0.0083009829890881\n",
      "Training Opt_acc epoch: 0.947061757206719\n",
      "Training loss per 100 training steps: 0.008173099677791736\n",
      "Training Opt_acc epoch: 0.9476349190131294\n",
      "Training loss per 100 training steps: 0.008153130176718164\n",
      "Training Opt_acc epoch: 0.94725604648643\n",
      "Training loss per 100 training steps: 0.008289206626999885\n",
      "Training Opt_acc epoch: 0.9468678654177217\n",
      "Training loss per 100 training steps: 0.008210258699078083\n",
      "Training Opt_acc epoch: 0.9473538904109883\n",
      "Training loss per 100 training steps: 0.008269384824348013\n",
      "Training Opt_acc epoch: 0.9465511287958258\n",
      "Training loss per 100 training steps: 0.008241592476042033\n",
      "Training Opt_acc epoch: 0.9451433917220712\n",
      "Training loss per 100 training steps: 0.008066092454490848\n",
      "Training Opt_acc epoch: 0.9456761668871341\n",
      "Training loss per 100 training steps: 0.00818943424666993\n",
      "Training Opt_acc epoch: 0.9444035799453937\n",
      "Training loss per 100 training steps: 0.008481007854798696\n",
      "Training Opt_acc epoch: 0.9419555699231389\n",
      "Training loss per 100 training steps: 0.00842113261693809\n",
      "Training Opt_acc epoch: 0.9412669525377987\n",
      "Training loss per 100 training steps: 0.008179506604397892\n",
      "Training Opt_acc epoch: 0.9432155585783667\n",
      "Training loss per 100 training steps: 0.008236657354210409\n",
      "Training Opt_acc epoch: 0.942916032515774\n",
      "Training loss per 100 training steps: 0.00826965203224908\n",
      "Training Opt_acc epoch: 0.9428578809540943\n",
      "Training loss per 100 training steps: 0.008171392712400645\n",
      "Training Opt_acc epoch: 0.9436048935297617\n",
      "Training loss per 100 training steps: 0.008204499923425632\n",
      "Training Opt_acc epoch: 0.9431061969166574\n",
      "Training loss per 100 training steps: 0.008125231790665618\n",
      "Training Opt_acc epoch: 0.9435718436067047\n",
      "Training loss per 100 training steps: 0.008041861993880678\n",
      "Training Opt_acc epoch: 0.9439560801380149\n",
      "Training loss per 100 training steps: 0.007971110107760259\n",
      "Training Opt_acc epoch: 0.9444908785036491\n",
      "Training loss per 100 training steps: 0.007871641283278413\n",
      "Training Opt_acc epoch: 0.9447902132197629\n",
      "Training loss per 100 training steps: 0.007814124465678278\n",
      "Training Opt_acc epoch: 0.9451718257092411\n",
      "Training loss per 100 training steps: 0.007892687135944215\n",
      "Training Opt_acc epoch: 0.9446296931206252\n",
      "Training loss per 100 training steps: 0.007842131895617785\n",
      "Training Opt_acc epoch: 0.9451524536730861\n",
      "Training loss per 100 training steps: 0.007832161917324078\n",
      "Training Opt_acc epoch: 0.9449333083198295\n",
      "Training loss per 100 training steps: 0.007780069404072296\n",
      "Training Opt_acc epoch: 0.9449898466505102\n",
      "Training loss per 100 training steps: 0.007797452023273413\n",
      "Training Opt_acc epoch: 0.9448017564745501\n",
      "Training loss per 100 training steps: 0.0077930720173700506\n",
      "Training Opt_acc epoch: 0.9442099345646404\n",
      "Training loss epoch: 0.007782289743898893\n",
      "Training accuracy epoch: 0.9975541964857914\n",
      "Training Opt_acc epoch: 0.9443651127462416\n",
      "tr_zero_steps: 319\n",
      "tr_opt_steps: 4056\n",
      "Training epoch: 3\n",
      "Training loss per 100 training steps: 0.0006667484994977713\n",
      "Training Opt_acc epoch: 1.0\n",
      "Training loss per 100 training steps: 0.0038996811543387196\n",
      "Training Opt_acc epoch: 0.9642832197353474\n",
      "Training loss per 100 training steps: 0.003676438190885038\n",
      "Training Opt_acc epoch: 0.9675253980632689\n",
      "Training loss per 100 training steps: 0.003719321966219918\n",
      "Training Opt_acc epoch: 0.9701502792012295\n",
      "Training loss per 100 training steps: 0.003488058730042559\n",
      "Training Opt_acc epoch: 0.972776371850609\n",
      "Training loss per 100 training steps: 0.0030877314304040217\n",
      "Training Opt_acc epoch: 0.9740419017717037\n",
      "Training loss per 100 training steps: 0.00315834823372855\n",
      "Training Opt_acc epoch: 0.9711650305679148\n",
      "Training loss per 100 training steps: 0.00301530835226926\n",
      "Training Opt_acc epoch: 0.9727848193793318\n",
      "Training loss per 100 training steps: 0.003289582816189883\n",
      "Training Opt_acc epoch: 0.9698792574247217\n",
      "Training loss per 100 training steps: 0.003619317490180324\n",
      "Training Opt_acc epoch: 0.9690124451480097\n",
      "Training loss per 100 training steps: 0.0037662562474056753\n",
      "Training Opt_acc epoch: 0.9669308575256204\n",
      "Training loss per 100 training steps: 0.004055271465131269\n",
      "Training Opt_acc epoch: 0.9668332386009799\n",
      "Training loss per 100 training steps: 0.004217934548396132\n",
      "Training Opt_acc epoch: 0.9668731925158323\n",
      "Training loss per 100 training steps: 0.004380739548871768\n",
      "Training Opt_acc epoch: 0.9657374006872124\n",
      "Training loss per 100 training steps: 0.004318854580881113\n",
      "Training Opt_acc epoch: 0.9661823374175129\n",
      "Training loss per 100 training steps: 0.0042835219538378145\n",
      "Training Opt_acc epoch: 0.9665541410061634\n",
      "Training loss per 100 training steps: 0.004135174379694544\n",
      "Training Opt_acc epoch: 0.9675608252019353\n",
      "Training loss per 100 training steps: 0.00442912330327809\n",
      "Training Opt_acc epoch: 0.9657239577176991\n",
      "Training loss per 100 training steps: 0.004529829939206262\n",
      "Training Opt_acc epoch: 0.9649411904928804\n",
      "Training loss per 100 training steps: 0.004638374882576056\n",
      "Training Opt_acc epoch: 0.9645398176050339\n",
      "Training loss per 100 training steps: 0.004841455163215727\n",
      "Training Opt_acc epoch: 0.9640786100087464\n",
      "Training loss per 100 training steps: 0.004829335303133717\n",
      "Training Opt_acc epoch: 0.9644721815374475\n",
      "Training loss per 100 training steps: 0.004754498408101034\n",
      "Training Opt_acc epoch: 0.9649841836278165\n",
      "Training loss per 100 training steps: 0.004722303866104352\n",
      "Training Opt_acc epoch: 0.9654820670446104\n",
      "Training loss per 100 training steps: 0.004704081405946532\n",
      "Training Opt_acc epoch: 0.9655701704206786\n",
      "Training loss per 100 training steps: 0.004719583856441415\n",
      "Training Opt_acc epoch: 0.9653237817055669\n",
      "Training loss per 100 training steps: 0.0046929080562092326\n",
      "Training Opt_acc epoch: 0.9654751775909993\n",
      "Training loss per 100 training steps: 0.004896209327258207\n",
      "Training Opt_acc epoch: 0.964758933975879\n",
      "Training loss per 100 training steps: 0.004940793855611725\n",
      "Training Opt_acc epoch: 0.9648625264233699\n",
      "Training loss per 100 training steps: 0.004952315031655584\n",
      "Training Opt_acc epoch: 0.9654554591702472\n",
      "Training loss per 100 training steps: 0.004988633046753462\n",
      "Training Opt_acc epoch: 0.9653239194635027\n",
      "Training loss per 100 training steps: 0.0050133940397982645\n",
      "Training Opt_acc epoch: 0.9649832830655379\n",
      "Training loss per 100 training steps: 0.005081366719152399\n",
      "Training Opt_acc epoch: 0.9645338686641095\n",
      "Training loss per 100 training steps: 0.005117306624620303\n",
      "Training Opt_acc epoch: 0.9644079841504728\n",
      "Training loss per 100 training steps: 0.005163038672987392\n",
      "Training Opt_acc epoch: 0.9639580528017148\n",
      "Training loss per 100 training steps: 0.005094527497955102\n",
      "Training Opt_acc epoch: 0.9641632508394524\n",
      "Training loss per 100 training steps: 0.005134523866764173\n",
      "Training Opt_acc epoch: 0.9639420636144309\n",
      "Training loss per 100 training steps: 0.005096774481379607\n",
      "Training Opt_acc epoch: 0.9638788624968442\n",
      "Training loss per 100 training steps: 0.005233324368336583\n",
      "Training Opt_acc epoch: 0.9640728262977857\n",
      "Training loss per 100 training steps: 0.005344319946833133\n",
      "Training Opt_acc epoch: 0.9626536942134342\n",
      "Training loss per 100 training steps: 0.005313918616377007\n",
      "Training Opt_acc epoch: 0.9627195684240852\n",
      "Training loss per 100 training steps: 0.0052847918141416825\n",
      "Training Opt_acc epoch: 0.9627997736304631\n",
      "Training loss per 100 training steps: 0.005285527561727187\n",
      "Training Opt_acc epoch: 0.9625267510223047\n",
      "Training loss per 100 training steps: 0.0052616034657302965\n",
      "Training Opt_acc epoch: 0.9626857429926917\n",
      "Training loss epoch: 0.005268958349590009\n",
      "Training accuracy epoch: 0.9983502631179964\n",
      "Training Opt_acc epoch: 0.962623129970367\n",
      "tr_zero_steps: 304\n",
      "tr_opt_steps: 4071\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14ebb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy, eval_precision, eval_f1, eval_recall, eval_opt_acc, eval_opt_acc_n = 0, 0, 0, 0, 0, 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    opt_zero_steps, opt_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels) #模型預測+算損失值\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "#             if idx % 100==0:\n",
    "#                 loss_step = eval_loss/nb_eval_steps\n",
    "#                 print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels) 遮罩 不會取無用\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,) 取最大機率的標記 num_label就消失 每批有效字元標記\n",
    "\n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "#             tmp_eval_precision = precision_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "#             tmp_eval_f1 = f1_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "#             tmp_eval_recall = recall_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "#             tn, fp, fn, tp = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1]).ravel()\n",
    "#             tmp_eval_opt_acc = tp/(fn+fp+tp)\n",
    "            tn, fo, fv, fn, to, fv2, fn2, fo2, tv = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1, 2]).ravel()\n",
    "            tmp_eval_opt_acc = (to+tv)/(fo+fv+fn+fv2+fn2+fo2+to+tv)\n",
    "            \n",
    "            if np.isnan(tmp_eval_opt_acc) == True:\n",
    "                tmp_eval_opt_acc = 1\n",
    "                opt_zero_steps += 1\n",
    "#                 print('opt_zero_steps:',opt_zero_steps)\n",
    "            else:\n",
    "                opt_steps += 1\n",
    "                eval_opt_acc_n += tmp_eval_opt_acc\n",
    "                \n",
    "                if idx % 100==0:\n",
    "                    loss_step = eval_loss/nb_eval_steps\n",
    "                    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "                    opt_acc_step = eval_opt_acc_n / opt_steps\n",
    "                    print(f\"Validation Opt_acc epoch: {opt_acc_step}\")\n",
    "                \n",
    "#             print(tn, fp, fn, tp)\n",
    "#             print(tmp_eval_opt_acc)\n",
    "#             print(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "#             print('labels:',labels.cpu().numpy())\n",
    "#             print('prediction:',predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "#             eval_precision += tmp_eval_precision\n",
    "#             eval_f1 += tmp_eval_f1\n",
    "#             eval_recall += tmp_eval_recall\n",
    "#             eval_opt_acc += tmp_eval_opt_acc\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    print(eval_opt_acc_n)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "#     eval_precision = eval_precision / nb_eval_steps\n",
    "#     eval_f1 = eval_f1 / nb_eval_steps\n",
    "#     eval_recall = eval_recall / nb_eval_steps\n",
    "#     eval_opt_acc = eval_opt_acc / nb_eval_steps\n",
    "    eval_opt_acc_n = eval_opt_acc_n / opt_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "#     print(f\"Validation Precision: {eval_precision}\")\n",
    "#     print(f\"Validation F1: {eval_f1}\")\n",
    "#     print(f\"Validation Recall: {eval_recall}\")\n",
    "#     print(f\"Validation Opt_Acc: {eval_opt_acc}\")\n",
    "    print(f\"Validation Opt_Acc_n: {eval_opt_acc_n}\")\n",
    "    print('zero_steps:',opt_zero_steps)\n",
    "    print('opt_steps:',opt_steps)\n",
    "    \n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cd491e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.009141453203261533\n",
      "Validation Opt_acc epoch: 0.9205154877484976\n",
      "Validation loss per 100 evaluation steps: 0.00932039673693832\n",
      "Validation Opt_acc epoch: 0.9261605271039234\n",
      "Validation loss per 100 evaluation steps: 0.009205232507021375\n",
      "Validation Opt_acc epoch: 0.9269257703081232\n",
      "Validation loss per 100 evaluation steps: 0.008141942591260385\n",
      "Validation Opt_acc epoch: 0.9360164835164836\n",
      "Validation loss per 100 evaluation steps: 0.00824483796662346\n",
      "Validation Opt_acc epoch: 0.9361904761904762\n",
      "Validation loss per 100 evaluation steps: 0.009177730164347857\n",
      "Validation Opt_acc epoch: 0.9344241079682013\n",
      "Validation loss per 100 evaluation steps: 0.009143626024518273\n",
      "Validation Opt_acc epoch: 0.936499582289056\n",
      "Validation loss per 100 evaluation steps: 0.009998250525713386\n",
      "Validation Opt_acc epoch: 0.935031293105223\n",
      "Validation loss per 100 evaluation steps: 0.010037156889316\n",
      "Validation Opt_acc epoch: 0.9343228637114834\n",
      "Validation loss per 100 evaluation steps: 0.010137205902612861\n",
      "Validation Opt_acc epoch: 0.9341678868030835\n",
      "Validation loss per 100 evaluation steps: 0.009701469398733452\n",
      "Validation Opt_acc epoch: 0.9352074955501349\n",
      "Validation loss per 100 evaluation steps: 0.009758148463345905\n",
      "Validation Opt_acc epoch: 0.9362770036127698\n",
      "Validation loss per 100 evaluation steps: 0.010120152030206424\n",
      "Validation Opt_acc epoch: 0.9366914995880251\n",
      "Validation loss per 100 evaluation steps: 0.009763180577181507\n",
      "Validation Opt_acc epoch: 0.9367426831541663\n",
      "Validation loss per 100 evaluation steps: 0.009614001234748213\n",
      "Validation Opt_acc epoch: 0.9372827977229035\n",
      "Validation loss per 100 evaluation steps: 0.00977254303448437\n",
      "Validation Opt_acc epoch: 0.9358973580936197\n",
      "Validation loss per 100 evaluation steps: 0.010082523166266602\n",
      "Validation Opt_acc epoch: 0.9352792747394677\n",
      "Validation loss per 100 evaluation steps: 0.009989749392905178\n",
      "Validation Opt_acc epoch: 0.9359805628632248\n",
      "Validation loss per 100 evaluation steps: 0.009979937019471577\n",
      "Validation Opt_acc epoch: 0.9344670968854796\n",
      "Validation loss per 100 evaluation steps: 0.010104338388587446\n",
      "Validation Opt_acc epoch: 0.9355488034111932\n",
      "Validation loss per 100 evaluation steps: 0.010056191626912608\n",
      "Validation Opt_acc epoch: 0.9356904371624333\n",
      "Validation loss per 100 evaluation steps: 0.010460626153343063\n",
      "Validation Opt_acc epoch: 0.9339808958158207\n",
      "Validation loss per 100 evaluation steps: 0.010425319995739982\n",
      "Validation Opt_acc epoch: 0.9328247010169922\n",
      "Validation loss per 100 evaluation steps: 0.010613890230930477\n",
      "Validation Opt_acc epoch: 0.9322283934564273\n",
      "Validation loss per 100 evaluation steps: 0.010490916623893282\n",
      "Validation Opt_acc epoch: 0.9325168518855889\n",
      "Validation loss per 100 evaluation steps: 0.010863767797901908\n",
      "Validation Opt_acc epoch: 0.9317827619150159\n",
      "Validation loss per 100 evaluation steps: 0.010729264500340207\n",
      "Validation Opt_acc epoch: 0.9308115389521582\n",
      "Validation loss per 100 evaluation steps: 0.010688469303554583\n",
      "Validation Opt_acc epoch: 0.9302757264010876\n",
      "Validation loss per 100 evaluation steps: 0.010573869404774894\n",
      "Validation Opt_acc epoch: 0.9308578779517279\n",
      "Validation loss per 100 evaluation steps: 0.010585190338574415\n",
      "Validation Opt_acc epoch: 0.9300519051655416\n",
      "Validation loss per 100 evaluation steps: 0.010532724284322243\n",
      "Validation Opt_acc epoch: 0.9296390963872422\n",
      "Validation loss per 100 evaluation steps: 0.010540822743493173\n",
      "Validation Opt_acc epoch: 0.9308008286699186\n",
      "Validation loss per 100 evaluation steps: 0.010637546239619487\n",
      "Validation Opt_acc epoch: 0.9302325241565751\n",
      "Validation loss per 100 evaluation steps: 0.010644467087387378\n",
      "Validation Opt_acc epoch: 0.9300494768622262\n",
      "Validation loss per 100 evaluation steps: 0.010656394585276095\n",
      "Validation Opt_acc epoch: 0.930060858912823\n",
      "Validation loss per 100 evaluation steps: 0.010640654194922147\n",
      "Validation Opt_acc epoch: 0.9304533501196198\n",
      "Validation loss per 100 evaluation steps: 0.01061439966010598\n",
      "Validation Opt_acc epoch: 0.9315708863179022\n",
      "Validation loss per 100 evaluation steps: 0.01041462979280839\n",
      "Validation Opt_acc epoch: 0.9321479994177367\n",
      "3445.14256854257\n",
      "Validation Loss: 0.010402128657720581\n",
      "Validation Accuracy: 0.9971451111126847\n",
      "Validation Opt_Acc_n: 0.9323795855324953\n",
      "zero_steps: 3805\n",
      "opt_steps: 3695\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a226542a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 5.2315403991087805e-06\n",
      "Validation loss per 100 evaluation steps: 0.0051576542535647805\n",
      "Validation loss per 100 evaluation steps: 0.008994274023082052\n",
      "Validation loss per 100 evaluation steps: 0.009179425629381708\n",
      "Validation loss per 100 evaluation steps: 0.009211852786256344\n",
      "Validation loss per 100 evaluation steps: 0.008786174003366815\n",
      "Validation loss per 100 evaluation steps: 0.007905135659207651\n",
      "Validation loss per 100 evaluation steps: 0.008813488478926745\n",
      "Validation loss per 100 evaluation steps: 0.007814253564141779\n",
      "Validation loss per 100 evaluation steps: 0.00806316829391777\n",
      "Validation loss per 100 evaluation steps: 0.008348489976179912\n",
      "Validation loss per 100 evaluation steps: 0.008557843492716722\n",
      "Validation loss per 100 evaluation steps: 0.008923472931435814\n",
      "Validation loss per 100 evaluation steps: 0.009024817274739074\n",
      "Validation loss per 100 evaluation steps: 0.010040913569460736\n",
      "Validation loss per 100 evaluation steps: 0.009608210017030789\n",
      "Validation loss per 100 evaluation steps: 0.009631962117817373\n",
      "Validation loss per 100 evaluation steps: 0.009820552413061621\n",
      "Validation loss per 100 evaluation steps: 0.009633456519614001\n",
      "Validation loss per 100 evaluation steps: 0.009922352913449872\n",
      "Validation loss per 100 evaluation steps: 0.00990108805985437\n",
      "Validation loss per 100 evaluation steps: 0.009599363010065564\n",
      "Validation loss per 100 evaluation steps: 0.009322808221761104\n",
      "Validation loss per 100 evaluation steps: 0.009125552130950716\n",
      "Validation loss per 100 evaluation steps: 0.009168007517230244\n",
      "Validation loss per 100 evaluation steps: 0.009102877993187172\n",
      "Validation loss per 100 evaluation steps: 0.009253119698042636\n",
      "Validation loss per 100 evaluation steps: 0.009700103979298378\n",
      "Validation loss per 100 evaluation steps: 0.009539237656821896\n",
      "Validation loss per 100 evaluation steps: 0.009465553037099565\n",
      "Validation loss per 100 evaluation steps: 0.0093180507090321\n",
      "Validation loss per 100 evaluation steps: 0.009384694091373623\n",
      "Validation loss per 100 evaluation steps: 0.009526151902339682\n",
      "Validation loss per 100 evaluation steps: 0.009809540437562232\n",
      "Validation loss per 100 evaluation steps: 0.009936360694194098\n",
      "Validation loss per 100 evaluation steps: 0.009970750029470248\n",
      "Validation loss per 100 evaluation steps: 0.010074180239263956\n",
      "Validation loss per 100 evaluation steps: 0.010014589893734212\n",
      "Validation loss per 100 evaluation steps: 0.009912771868029127\n",
      "Validation loss per 100 evaluation steps: 0.009928787403809388\n",
      "Validation loss per 100 evaluation steps: 0.00994476384650698\n",
      "Validation loss per 100 evaluation steps: 0.00989499132549217\n",
      "Validation loss per 100 evaluation steps: 0.009937984218847605\n",
      "Validation loss per 100 evaluation steps: 0.010212910726422438\n",
      "Validation loss per 100 evaluation steps: 0.010352710094574738\n",
      "Validation loss per 100 evaluation steps: 0.010347463929697797\n",
      "Validation loss per 100 evaluation steps: 0.01037639297090493\n",
      "Validation loss per 100 evaluation steps: 0.010201647073863038\n",
      "Validation loss per 100 evaluation steps: 0.010224346887371316\n",
      "Validation loss per 100 evaluation steps: 0.010527547172509295\n",
      "Validation loss per 100 evaluation steps: 0.010481770931930857\n",
      "Validation loss per 100 evaluation steps: 0.010646643880142472\n",
      "Validation loss per 100 evaluation steps: 0.010639205765779057\n",
      "Validation loss per 100 evaluation steps: 0.010512346958093555\n",
      "Validation loss per 100 evaluation steps: 0.01046349635274047\n",
      "Validation loss per 100 evaluation steps: 0.010403243469873117\n",
      "Validation loss per 100 evaluation steps: 0.010417472795058736\n",
      "Validation loss per 100 evaluation steps: 0.010350082151014987\n",
      "Validation loss per 100 evaluation steps: 0.010358281022760709\n",
      "Validation loss per 100 evaluation steps: 0.01024469284280058\n",
      "Validation loss per 100 evaluation steps: 0.010299275756437376\n",
      "Validation loss per 100 evaluation steps: 0.010162605954462852\n",
      "Validation loss per 100 evaluation steps: 0.010280378377716169\n",
      "Validation loss per 100 evaluation steps: 0.010420451043346147\n",
      "Validation loss per 100 evaluation steps: 0.010523561808958205\n",
      "Validation loss per 100 evaluation steps: 0.01052399989584555\n",
      "Validation loss per 100 evaluation steps: 0.010584985171341857\n",
      "Validation loss per 100 evaluation steps: 0.01063308872736568\n",
      "Validation loss per 100 evaluation steps: 0.010639707107873224\n",
      "Validation loss per 100 evaluation steps: 0.01059526294165323\n",
      "Validation loss per 100 evaluation steps: 0.010483938090321283\n",
      "Validation loss per 100 evaluation steps: 0.010579035196243125\n",
      "Validation loss per 100 evaluation steps: 0.010483827152432732\n",
      "Validation loss per 100 evaluation steps: 0.0104111544160183\n",
      "Validation loss per 100 evaluation steps: 0.010276305403170557\n",
      "3425.721717171718\n",
      "Validation Loss: 0.010260844369369245\n",
      "Validation Accuracy: 0.9968075926227843\n",
      "Validation Opt_Acc_n: 0.927123604106013\n",
      "zero_steps: 3805\n",
      "opt_steps: 3695\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6cf4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90effc59",
   "metadata": {},
   "source": [
    "## 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0e89d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "model = BertForTokenClassification.from_pretrained('cl-tohoku/bert-large-japanese', num_labels=len(labels_to_ids))\n",
    "# model.to(device)\n",
    "model1 = model.load_state_dict(torch.load('model_Noni/pytorch_model.bin'))\n",
    "model1 = model.to(device)\n",
    "model1 = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c33a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('cl-tohoku/bert-large-japanese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac4cd7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffe9e4",
   "metadata": {},
   "source": [
    "L８—３ -> 應為Ｖ\n",
    "Ｖ彼がその本を読むのに4日かかるでしょう。\n",
    "Ｖ車を修理するのに１万円かかった。\n",
    "Ｖ名古屋から東京まで行くのに2時間かかった。\n",
    "Ｖ私がそのパズルを解くのに2時間かかりました。\n",
    "Ｖ学校に来るのにどれくらい時間がかかりますか。\n",
    "Ｏ卒論作るのにどれくらいかかったかを教えてください。\n",
    "Ｏ日本に留学するのにいくらかかりましたか？\n",
    ".Ｖお互いを理解するのに時間が必要です。\n",
    " \n",
    "L２１—２ -> 應為Ｏ\n",
    "Ｏバスを1時間も待っているのに、まだ来ません。\n",
    "Ｏ両親がとても心配しているのに、うちに電話をしませんでした。\n",
    "Ｏ毎日薬を飲んでいるのに、なかなかよくなりません。\n",
    "Ｏ毎日勉強しているのに、なかなか覚えることができません。\n",
    "Ｏワンさんは約束したのに、来ませんでした。\n",
    "Ｏお金が入れたのに、ジュースが出ません。\n",
    "Ｏタクシーで来たのに、間に合いませんでした。\n",
    "Ｏこの料理は、砂糖を入いれていないのに甘い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c27b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['日', '本', 'に', '留', '学', 'す', 'る', 'の', 'に', 'い', 'く', 'ら', 'か', 'か', 'り', 'ま', 'し', 'た', 'か', '？']\n",
      "['Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"日本に留学するのにいくらかかりましたか？\"\n",
    "\n",
    "inputs = tokenizer([word for word in sentence],\n",
    "                    is_pretokenized=True, \n",
    "                    return_offsets_mapping=True, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=MAX_LEN,\n",
    "                    return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "    #only predictions on first word pieces are important\n",
    "    if mapping[0] == 0 and mapping[1] != 0:\n",
    "        prediction.append(token_pred[1])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(sentence.split())\n",
    "print([word for word in sentence])\n",
    "# print(inputs['input_ids'])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "284b5fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>日</td>\n",
       "      <td>本</td>\n",
       "      <td>に</td>\n",
       "      <td>留</td>\n",
       "      <td>学</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>の</td>\n",
       "      <td>に</td>\n",
       "      <td>い</td>\n",
       "      <td>く</td>\n",
       "      <td>ら</td>\n",
       "      <td>か</td>\n",
       "      <td>か</td>\n",
       "      <td>り</td>\n",
       "      <td>ま</td>\n",
       "      <td>し</td>\n",
       "      <td>た</td>\n",
       "      <td>か</td>\n",
       "      <td>？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19\n",
       "Tokens  日  本  に  留  学  す  る  の  に  い  く  ら  か  か  り  ま  し  た  か  ？\n",
       "Tags    Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame([[word for word in sentence], prediction], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "200a64c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 2719, 2828,  893, 3705, 1967,  875,  925,  896,  893,  854,  865,\n",
       "          923,  861,  861,  924,  912,  873,  881,  861,    1,    3,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'offset_mapping': tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4b9ec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n",
      "This tutorial is completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"./model_Noni\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# save vocabulary of the tokenizer\n",
    "tokenizer.save_vocabulary(directory)\n",
    "# save the model weights and its configuration file\n",
    "model.save_pretrained(directory)\n",
    "print('All files saved')\n",
    "print('This tutorial is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12825ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812289c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
