{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43cc344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34caca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4860e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_labels = {0: 'Ｘ', 1: 'Ｏ', -100:'IGN'}\n",
    "labels_to_ids = {'Ｘ': 0, 'Ｏ': 1, 'IGN':-100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb6bc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ｘ', 1: 'Ｏ', -100: 'IGN'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1787914b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ｘ': 0, 'Ｏ': 1, 'IGN': -100}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a1ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizerFast.from_pretrained('cl-tohoku/bert-large-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ff3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "re_vocab = {vocab[word]:word for word in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53930828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_Tari_tag_all.txt', 'r') as filein:\n",
    "    wiki_tag = filein.readlines()\n",
    "    filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03fac487",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tag_all = [''.join(wiki_tag[i].strip()) for i in range(len(wiki_tag))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b804bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tag_all[40000:40005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962da953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "jp_train_tag, jp_test_tag = train_test_split(wiki_tag_all, random_state=55, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db66e3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jp_train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e211d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24489\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_train_tag:\n",
    "    if 'Ｏ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45466d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jp_test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f8bfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10511\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_test_tag:\n",
    "    if 'Ｏ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a7b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_Tari_pattern_all.txt', 'r') as filein:\n",
    "    wiki_sent = filein.readlines()\n",
    "    filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563df8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sent_all = [wiki_sent[i].strip() for i in range(len(wiki_sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0721637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_train_texts, jp_test_texts = train_test_split(wiki_sent_all, random_state=55, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33284c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['近年では2006年トリノオリンピックのジャンプ競技で金メダルを獲得している。',\n",
       " 'モータースポーツは、オーストリアで3番目に人気のあるスポーツである（スキーとサッカーに次ぐ）。',\n",
       " '数人のオーストリアのドライバーがF1で成功を収めている。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sent_all[40017:40020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6454a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sent =[wiki_sent_all[40019][word] for word in range(len(wiki_sent_all[40019]))]\n",
    "data_label =[wiki_tag_all[40019][word] for word in range(len(wiki_tag_all[40019]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf0c17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>数</td>\n",
       "      <td>人</td>\n",
       "      <td>の</td>\n",
       "      <td>オ</td>\n",
       "      <td>ー</td>\n",
       "      <td>ス</td>\n",
       "      <td>ト</td>\n",
       "      <td>リ</td>\n",
       "      <td>ア</td>\n",
       "      <td>の</td>\n",
       "      <td>ド</td>\n",
       "      <td>ラ</td>\n",
       "      <td>イ</td>\n",
       "      <td>バ</td>\n",
       "      <td>ー</td>\n",
       "      <td>が</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>で</td>\n",
       "      <td>成</td>\n",
       "      <td>功</td>\n",
       "      <td>を</td>\n",
       "      <td>収</td>\n",
       "      <td>め</td>\n",
       "      <td>て</td>\n",
       "      <td>い</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23  \\\n",
       "sent   数  人  の  オ  ー  ス  ト  リ  ア  の  ド  ラ  イ  バ  ー  が  F  1  で  成  功  を  収  め   \n",
       "label  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ   \n",
       "\n",
       "      24 25 26 27  \n",
       "sent   て  い  る  。  \n",
       "label  Ｘ  Ｘ  Ｘ  Ｘ  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame([data_sent, data_label], index=[\"sent\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0f8d912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for num in range(len(wiki_sent_all)):\n",
    "    text.append(len(wiki_sent_all[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1ff01595",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74,\n",
       " 88,\n",
       " 52,\n",
       " 110,\n",
       " 77,\n",
       " 85,\n",
       " 82,\n",
       " 106,\n",
       " 46,\n",
       " 192,\n",
       " 62,\n",
       " 67,\n",
       " 77,\n",
       " 115,\n",
       " 118,\n",
       " 89,\n",
       " 35,\n",
       " 84,\n",
       " 69,\n",
       " 52,\n",
       " 91,\n",
       " 98,\n",
       " 60,\n",
       " 98,\n",
       " 79,\n",
       " 130,\n",
       " 110,\n",
       " 61,\n",
       " 93,\n",
       " 39,\n",
       " 90,\n",
       " 153,\n",
       " 59,\n",
       " 75,\n",
       " 36,\n",
       " 82,\n",
       " 82,\n",
       " 49,\n",
       " 50,\n",
       " 73,\n",
       " 77,\n",
       " 112,\n",
       " 66,\n",
       " 96,\n",
       " 30,\n",
       " 55,\n",
       " 65,\n",
       " 86,\n",
       " 68,\n",
       " 75,\n",
       " 75,\n",
       " 59,\n",
       " 135,\n",
       " 51,\n",
       " 72,\n",
       " 68,\n",
       " 94,\n",
       " 61,\n",
       " 79,\n",
       " 79,\n",
       " 68,\n",
       " 87,\n",
       " 101,\n",
       " 133,\n",
       " 98,\n",
       " 116,\n",
       " 62,\n",
       " 48,\n",
       " 44,\n",
       " 50,\n",
       " 49,\n",
       " 98,\n",
       " 64,\n",
       " 154,\n",
       " 46,\n",
       " 138,\n",
       " 82,\n",
       " 57,\n",
       " 95,\n",
       " 53,\n",
       " 48,\n",
       " 44,\n",
       " 57,\n",
       " 80,\n",
       " 41,\n",
       " 61,\n",
       " 35,\n",
       " 25,\n",
       " 115,\n",
       " 102,\n",
       " 68,\n",
       " 79,\n",
       " 53,\n",
       " 98,\n",
       " 51,\n",
       " 23,\n",
       " 114,\n",
       " 97,\n",
       " 140,\n",
       " 97,\n",
       " 40,\n",
       " 98,\n",
       " 68,\n",
       " 73,\n",
       " 41,\n",
       " 57,\n",
       " 25,\n",
       " 66,\n",
       " 50,\n",
       " 61,\n",
       " 154,\n",
       " 99,\n",
       " 89,\n",
       " 151,\n",
       " 57,\n",
       " 96,\n",
       " 113,\n",
       " 60,\n",
       " 96,\n",
       " 86,\n",
       " 77,\n",
       " 94,\n",
       " 41,\n",
       " 76,\n",
       " 44,\n",
       " 97,\n",
       " 86,\n",
       " 81,\n",
       " 101,\n",
       " 46,\n",
       " 93,\n",
       " 112,\n",
       " 68,\n",
       " 72,\n",
       " 55,\n",
       " 50,\n",
       " 133,\n",
       " 95,\n",
       " 50,\n",
       " 70,\n",
       " 37,\n",
       " 72,\n",
       " 50,\n",
       " 81,\n",
       " 112,\n",
       " 134,\n",
       " 40,\n",
       " 44,\n",
       " 84,\n",
       " 63,\n",
       " 50,\n",
       " 102,\n",
       " 93,\n",
       " 33,\n",
       " 73,\n",
       " 36,\n",
       " 106,\n",
       " 113,\n",
       " 43,\n",
       " 54,\n",
       " 75,\n",
       " 40,\n",
       " 147,\n",
       " 60,\n",
       " 75,\n",
       " 94,\n",
       " 41,\n",
       " 90,\n",
       " 58,\n",
       " 54,\n",
       " 34,\n",
       " 117,\n",
       " 142,\n",
       " 76,\n",
       " 88,\n",
       " 65,\n",
       " 50,\n",
       " 69,\n",
       " 70,\n",
       " 120,\n",
       " 106,\n",
       " 50,\n",
       " 135,\n",
       " 139,\n",
       " 97,\n",
       " 65,\n",
       " 84,\n",
       " 97,\n",
       " 75,\n",
       " 72,\n",
       " 55,\n",
       " 108,\n",
       " 62,\n",
       " 55,\n",
       " 274,\n",
       " 86,\n",
       " 68,\n",
       " 41,\n",
       " 56,\n",
       " 69,\n",
       " 45,\n",
       " 85,\n",
       " 143,\n",
       " 117,\n",
       " 57,\n",
       " 65,\n",
       " 133,\n",
       " 67,\n",
       " 173,\n",
       " 68,\n",
       " 116,\n",
       " 109,\n",
       " 103,\n",
       " 95,\n",
       " 117,\n",
       " 73,\n",
       " 168,\n",
       " 85,\n",
       " 48,\n",
       " 73,\n",
       " 111,\n",
       " 81,\n",
       " 159,\n",
       " 105,\n",
       " 96,\n",
       " 97,\n",
       " 117,\n",
       " 41,\n",
       " 42,\n",
       " 211,\n",
       " 83,\n",
       " 48,\n",
       " 57,\n",
       " 49,\n",
       " 52,\n",
       " 51,\n",
       " 93,\n",
       " 36,\n",
       " 30,\n",
       " 78,\n",
       " 97,\n",
       " 61,\n",
       " 56,\n",
       " 90,\n",
       " 65,\n",
       " 31,\n",
       " 55,\n",
       " 49,\n",
       " 113,\n",
       " 45,\n",
       " 67,\n",
       " 72,\n",
       " 101,\n",
       " 74,\n",
       " 85,\n",
       " 87,\n",
       " 54,\n",
       " 38,\n",
       " 29,\n",
       " 66,\n",
       " 71,\n",
       " 87,\n",
       " 115,\n",
       " 55,\n",
       " 114,\n",
       " 46,\n",
       " 32,\n",
       " 87,\n",
       " 68,\n",
       " 66,\n",
       " 44,\n",
       " 63,\n",
       " 53,\n",
       " 49,\n",
       " 178,\n",
       " 55,\n",
       " 85,\n",
       " 81,\n",
       " 73,\n",
       " 144,\n",
       " 51,\n",
       " 96,\n",
       " 107,\n",
       " 117,\n",
       " 148,\n",
       " 104,\n",
       " 54,\n",
       " 90,\n",
       " 67,\n",
       " 82,\n",
       " 75,\n",
       " 53,\n",
       " 48,\n",
       " 72,\n",
       " 120,\n",
       " 124,\n",
       " 111,\n",
       " 77,\n",
       " 88,\n",
       " 137,\n",
       " 84,\n",
       " 171,\n",
       " 91,\n",
       " 106,\n",
       " 54,\n",
       " 91,\n",
       " 119,\n",
       " 78,\n",
       " 148,\n",
       " 173,\n",
       " 257,\n",
       " 46,\n",
       " 160,\n",
       " 62,\n",
       " 36,\n",
       " 54,\n",
       " 79,\n",
       " 84,\n",
       " 185,\n",
       " 44,\n",
       " 62,\n",
       " 88,\n",
       " 100,\n",
       " 58,\n",
       " 59,\n",
       " 69,\n",
       " 59,\n",
       " 45,\n",
       " 51,\n",
       " 57,\n",
       " 59,\n",
       " 32,\n",
       " 80,\n",
       " 76,\n",
       " 56,\n",
       " 103,\n",
       " 146,\n",
       " 59,\n",
       " 44,\n",
       " 52,\n",
       " 44,\n",
       " 77,\n",
       " 86,\n",
       " 88,\n",
       " 80,\n",
       " 36,\n",
       " 27,\n",
       " 155,\n",
       " 68,\n",
       " 56,\n",
       " 49,\n",
       " 163,\n",
       " 65,\n",
       " 76,\n",
       " 63,\n",
       " 179,\n",
       " 197,\n",
       " 95,\n",
       " 39,\n",
       " 86,\n",
       " 81,\n",
       " 45,\n",
       " 113,\n",
       " 43,\n",
       " 34,\n",
       " 107,\n",
       " 62,\n",
       " 58,\n",
       " 123,\n",
       " 70,\n",
       " 43,\n",
       " 89,\n",
       " 69,\n",
       " 44,\n",
       " 76,\n",
       " 110,\n",
       " 110,\n",
       " 58,\n",
       " 145,\n",
       " 50,\n",
       " 154,\n",
       " 106,\n",
       " 73,\n",
       " 57,\n",
       " 49,\n",
       " 70,\n",
       " 65,\n",
       " 85,\n",
       " 21,\n",
       " 54,\n",
       " 50,\n",
       " 81,\n",
       " 139,\n",
       " 128,\n",
       " 39,\n",
       " 231,\n",
       " 55,\n",
       " 66,\n",
       " 155,\n",
       " 49,\n",
       " 87,\n",
       " 136,\n",
       " 43,\n",
       " 179,\n",
       " 219,\n",
       " 93,\n",
       " 66,\n",
       " 244,\n",
       " 51,\n",
       " 54,\n",
       " 96,\n",
       " 75,\n",
       " 62,\n",
       " 97,\n",
       " 30,\n",
       " 39,\n",
       " 42,\n",
       " 89,\n",
       " 35,\n",
       " 44,\n",
       " 53,\n",
       " 105,\n",
       " 127,\n",
       " 90,\n",
       " 110,\n",
       " 124,\n",
       " 82,\n",
       " 86,\n",
       " 130,\n",
       " 72,\n",
       " 83,\n",
       " 49,\n",
       " 188,\n",
       " 145,\n",
       " 50,\n",
       " 49,\n",
       " 50,\n",
       " 119,\n",
       " 30,\n",
       " 85,\n",
       " 42,\n",
       " 62,\n",
       " 84,\n",
       " 54,\n",
       " 109,\n",
       " 78,\n",
       " 65,\n",
       " 55,\n",
       " 100,\n",
       " 106,\n",
       " 55,\n",
       " 58,\n",
       " 15,\n",
       " 77,\n",
       " 60,\n",
       " 68,\n",
       " 95,\n",
       " 64,\n",
       " 68,\n",
       " 53,\n",
       " 146,\n",
       " 46,\n",
       " 58,\n",
       " 60,\n",
       " 62,\n",
       " 88,\n",
       " 32,\n",
       " 45,\n",
       " 80,\n",
       " 311,\n",
       " 48,\n",
       " 57,\n",
       " 122,\n",
       " 181,\n",
       " 96,\n",
       " 23,\n",
       " 50,\n",
       " 115,\n",
       " 45,\n",
       " 55,\n",
       " 150,\n",
       " 58,\n",
       " 53,\n",
       " 48,\n",
       " 36,\n",
       " 35,\n",
       " 62,\n",
       " 63,\n",
       " 133,\n",
       " 135,\n",
       " 83,\n",
       " 114,\n",
       " 52,\n",
       " 97,\n",
       " 89,\n",
       " 133,\n",
       " 52,\n",
       " 71,\n",
       " 89,\n",
       " 122,\n",
       " 57,\n",
       " 42,\n",
       " 69,\n",
       " 49,\n",
       " 117,\n",
       " 66,\n",
       " 70,\n",
       " 42,\n",
       " 70,\n",
       " 131,\n",
       " 57,\n",
       " 41,\n",
       " 72,\n",
       " 74,\n",
       " 82,\n",
       " 144,\n",
       " 115,\n",
       " 73,\n",
       " 51,\n",
       " 40,\n",
       " 117,\n",
       " 80,\n",
       " 147,\n",
       " 142,\n",
       " 94,\n",
       " 81,\n",
       " 65,\n",
       " 37,\n",
       " 65,\n",
       " 82,\n",
       " 62,\n",
       " 69,\n",
       " 43,\n",
       " 64,\n",
       " 54,\n",
       " 83,\n",
       " 120,\n",
       " 85,\n",
       " 40,\n",
       " 49,\n",
       " 66,\n",
       " 193,\n",
       " 51,\n",
       " 47,\n",
       " 100,\n",
       " 34,\n",
       " 133,\n",
       " 74,\n",
       " 125,\n",
       " 110,\n",
       " 50,\n",
       " 38,\n",
       " 82,\n",
       " 134,\n",
       " 98,\n",
       " 28,\n",
       " 119,\n",
       " 43,\n",
       " 77,\n",
       " 51,\n",
       " 84,\n",
       " 57,\n",
       " 150,\n",
       " 70,\n",
       " 70,\n",
       " 76,\n",
       " 49,\n",
       " 63,\n",
       " 106,\n",
       " 33,\n",
       " 50,\n",
       " 121,\n",
       " 113,\n",
       " 78,\n",
       " 87,\n",
       " 82,\n",
       " 76,\n",
       " 103,\n",
       " 102,\n",
       " 94,\n",
       " 117,\n",
       " 62,\n",
       " 44,\n",
       " 77,\n",
       " 34,\n",
       " 41,\n",
       " 46,\n",
       " 67,\n",
       " 50,\n",
       " 122,\n",
       " 96,\n",
       " 95,\n",
       " 82,\n",
       " 54,\n",
       " 120,\n",
       " 232,\n",
       " 66,\n",
       " 77,\n",
       " 83,\n",
       " 47,\n",
       " 91,\n",
       " 89,\n",
       " 106,\n",
       " 90,\n",
       " 59,\n",
       " 119,\n",
       " 60,\n",
       " 74,\n",
       " 173,\n",
       " 96,\n",
       " 120,\n",
       " 29,\n",
       " 33,\n",
       " 114,\n",
       " 44,\n",
       " 80,\n",
       " 41,\n",
       " 70,\n",
       " 58,\n",
       " 57,\n",
       " 65,\n",
       " 196,\n",
       " 75,\n",
       " 49,\n",
       " 83,\n",
       " 133,\n",
       " 81,\n",
       " 47,\n",
       " 81,\n",
       " 55,\n",
       " 58,\n",
       " 50,\n",
       " 54,\n",
       " 58,\n",
       " 57,\n",
       " 54,\n",
       " 73,\n",
       " 66,\n",
       " 47,\n",
       " 78,\n",
       " 53,\n",
       " 44,\n",
       " 61,\n",
       " 72,\n",
       " 132,\n",
       " 140,\n",
       " 66,\n",
       " 64,\n",
       " 85,\n",
       " 52,\n",
       " 76,\n",
       " 58,\n",
       " 66,\n",
       " 65,\n",
       " 114,\n",
       " 48,\n",
       " 170,\n",
       " 166,\n",
       " 158,\n",
       " 47,\n",
       " 59,\n",
       " 65,\n",
       " 71,\n",
       " 60,\n",
       " 47,\n",
       " 51,\n",
       " 56,\n",
       " 54,\n",
       " 141,\n",
       " 20,\n",
       " 41,\n",
       " 89,\n",
       " 44,\n",
       " 20,\n",
       " 91,\n",
       " 53,\n",
       " 63,\n",
       " 66,\n",
       " 89,\n",
       " 31,\n",
       " 41,\n",
       " 108,\n",
       " 84,\n",
       " 31,\n",
       " 59,\n",
       " 64,\n",
       " 61,\n",
       " 29,\n",
       " 46,\n",
       " 124,\n",
       " 87,\n",
       " 57,\n",
       " 138,\n",
       " 166,\n",
       " 264,\n",
       " 21,\n",
       " 38,\n",
       " 55,\n",
       " 76,\n",
       " 43,\n",
       " 41,\n",
       " 32,\n",
       " 56,\n",
       " 97,\n",
       " 52,\n",
       " 79,\n",
       " 75,\n",
       " 133,\n",
       " 105,\n",
       " 64,\n",
       " 193,\n",
       " 73,\n",
       " 64,\n",
       " 70,\n",
       " 167,\n",
       " 76,\n",
       " 83,\n",
       " 88,\n",
       " 75,\n",
       " 84,\n",
       " 107,\n",
       " 35,\n",
       " 125,\n",
       " 42,\n",
       " 69,\n",
       " 100,\n",
       " 42,\n",
       " 97,\n",
       " 48,\n",
       " 134,\n",
       " 51,\n",
       " 46,\n",
       " 91,\n",
       " 59,\n",
       " 47,\n",
       " 84,\n",
       " 72,\n",
       " 92,\n",
       " 163,\n",
       " 146,\n",
       " 73,\n",
       " 139,\n",
       " 82,\n",
       " 121,\n",
       " 163,\n",
       " 39,\n",
       " 34,\n",
       " 40,\n",
       " 154,\n",
       " 45,\n",
       " 76,\n",
       " 195,\n",
       " 99,\n",
       " 67,\n",
       " 66,\n",
       " 62,\n",
       " 39,\n",
       " 63,\n",
       " 69,\n",
       " 39,\n",
       " 54,\n",
       " 100,\n",
       " 92,\n",
       " 33,\n",
       " 114,\n",
       " 130,\n",
       " 185,\n",
       " 67,\n",
       " 102,\n",
       " 55,\n",
       " 77,\n",
       " 45,\n",
       " 37,\n",
       " 60,\n",
       " 76,\n",
       " 67,\n",
       " 49,\n",
       " 61,\n",
       " 37,\n",
       " 69,\n",
       " 180,\n",
       " 57,\n",
       " 70,\n",
       " 39,\n",
       " 76,\n",
       " 51,\n",
       " 113,\n",
       " 62,\n",
       " 72,\n",
       " 99,\n",
       " 200,\n",
       " 73,\n",
       " 102,\n",
       " 77,\n",
       " 52,\n",
       " 124,\n",
       " 69,\n",
       " 78,\n",
       " 67,\n",
       " 64,\n",
       " 37,\n",
       " 29,\n",
       " 34,\n",
       " 37,\n",
       " 57,\n",
       " 92,\n",
       " 46,\n",
       " 43,\n",
       " 73,\n",
       " 49,\n",
       " 82,\n",
       " 129,\n",
       " 70,\n",
       " 46,\n",
       " 126,\n",
       " 47,\n",
       " 123,\n",
       " 79,\n",
       " 79,\n",
       " 113,\n",
       " 69,\n",
       " 46,\n",
       " 77,\n",
       " 53,\n",
       " 79,\n",
       " 45,\n",
       " 88,\n",
       " 113,\n",
       " 30,\n",
       " 55,\n",
       " 105,\n",
       " 43,\n",
       " 183,\n",
       " 50,\n",
       " 68,\n",
       " 86,\n",
       " 83,\n",
       " 103,\n",
       " 42,\n",
       " 58,\n",
       " 112,\n",
       " 57,\n",
       " 50,\n",
       " 129,\n",
       " 67,\n",
       " 73,\n",
       " 56,\n",
       " 46,\n",
       " 141,\n",
       " 38,\n",
       " 57,\n",
       " 96,\n",
       " 129,\n",
       " 24,\n",
       " 74,\n",
       " 127,\n",
       " 113,\n",
       " 84,\n",
       " 84,\n",
       " 133,\n",
       " 112,\n",
       " 37,\n",
       " 52,\n",
       " 116,\n",
       " 79,\n",
       " 113,\n",
       " 70,\n",
       " 62,\n",
       " 130,\n",
       " 95,\n",
       " 61,\n",
       " 118,\n",
       " 55,\n",
       " 87,\n",
       " 261,\n",
       " 163,\n",
       " 230,\n",
       " 85,\n",
       " 27,\n",
       " 40,\n",
       " 47,\n",
       " 88,\n",
       " 147,\n",
       " 55,\n",
       " 65,\n",
       " 71,\n",
       " 54,\n",
       " 76,\n",
       " 42,\n",
       " 60,\n",
       " 55,\n",
       " 41,\n",
       " 42,\n",
       " 61,\n",
       " 68,\n",
       " 69,\n",
       " 64,\n",
       " 60,\n",
       " 52,\n",
       " 137,\n",
       " 45,\n",
       " 38,\n",
       " 103,\n",
       " 234,\n",
       " 164,\n",
       " 108,\n",
       " 110,\n",
       " 111,\n",
       " 121,\n",
       " 37,\n",
       " 48,\n",
       " 50,\n",
       " 56,\n",
       " 76,\n",
       " 76,\n",
       " 95,\n",
       " 59,\n",
       " 95,\n",
       " 73,\n",
       " 50,\n",
       " 79,\n",
       " 122,\n",
       " 96,\n",
       " 67,\n",
       " 112,\n",
       " 124,\n",
       " 57,\n",
       " 79,\n",
       " 74,\n",
       " 160,\n",
       " 92,\n",
       " 46,\n",
       " 50,\n",
       " 102,\n",
       " 48,\n",
       " 60,\n",
       " 112,\n",
       " 111,\n",
       " 95,\n",
       " 68,\n",
       " 41,\n",
       " 70,\n",
       " 110,\n",
       " 66,\n",
       " 91,\n",
       " 97,\n",
       " 45,\n",
       " 141,\n",
       " 59,\n",
       " 74,\n",
       " 59,\n",
       " 51,\n",
       " 75,\n",
       " 62,\n",
       " 65,\n",
       " 49,\n",
       " 150,\n",
       " 84,\n",
       " 53,\n",
       " 62,\n",
       " 67,\n",
       " 107,\n",
       " 54,\n",
       " 83,\n",
       " 86,\n",
       " 129,\n",
       " 38,\n",
       " 38,\n",
       " 50,\n",
       " 81,\n",
       " 140,\n",
       " 71,\n",
       " 84,\n",
       " 31,\n",
       " 144,\n",
       " 72,\n",
       " 30,\n",
       " 75,\n",
       " 57,\n",
       " 40,\n",
       " 94,\n",
       " 57,\n",
       " 149,\n",
       " 92,\n",
       " 102,\n",
       " 75,\n",
       " 148,\n",
       " 107,\n",
       " 143,\n",
       " 85,\n",
       " 58,\n",
       " 146,\n",
       " 125,\n",
       " 54,\n",
       " 98,\n",
       " 66,\n",
       " 69,\n",
       " 361,\n",
       " 122,\n",
       " 113,\n",
       " ...]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0500b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "count  70000.000000\n",
      "mean      64.957729\n",
      "std       38.637470\n",
      "min        2.000000\n",
      "25%       39.000000\n",
      "50%       57.000000\n",
      "75%       82.000000\n",
      "max      603.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "df = pd.DataFrame(text)\n",
    "print(df.describe())\n",
    "df.plot.box(title=\"Box Chart\")\n",
    "plt.grid(linestyle=\"--\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e772ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ところが、2003年時点では輸出入とも相手国が分散する。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18462836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 11668, 6307, 6244, 1785, 1559, 828, 4865, 4923, 1725, 1385, 893, 1846, 863, 6335, 4440, 3186, 3039, 932, 1040, 858, 12259, 828, 1719, 2000, 896, 1314, 3690, 4923, 932, 4823, 2678, 1390, 1985, 15554, 6157, 11229, 20529, 6335, 6461, 6416, 6456, 21606, 2044, 2678, 3228, 4865, 4923, 4908, 4323, 896, 1039, 3323, 932, 14675, 11263, 2664, 4049, 897, 6456, 6156, 3690, 24523, 11304, 829, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(wiki_sent_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36832c99",
   "metadata": {},
   "source": [
    "## 檢查是否有None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4f7851f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(wiki_sent_all)):\n",
    "    data_sent =[wiki_sent_all[i][word] for word in range(len(wiki_sent_all[i]))]\n",
    "    data_label =[wiki_tag_all[i][word] for word in range(len(wiki_tag_all[i]))]\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    if len(wiki_sent_all[i]) == len(wiki_tag_all[i]):\n",
    "#         print(i,'OK')\n",
    "        continue\n",
    "    else:\n",
    "        df = pd.DataFrame([data_sent, data_label], index=[\"sent\", \"label\"])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "122f7fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['鑑賞とは音楽を聴いてそれを味わったり、価値を見極めたりすることである。',\n",
       " 'リラックスしたり、エネルギーを増やしたり、思考を改善したり、その日のやる気を引き出したりする必要がある場合でも、明るい音楽は最も必要なときに追加のサポートを提供できる。',\n",
       " '縁故主義により経済的に成功したり、海外への留学を楽しんだりする高位聖職者や政府・軍高官の一族は「アガザデ」（高貴な生まれ）と呼ばれている。',\n",
       " '読んでいる途中で、難しい言葉の解説を行ったり、分かりにくい表現について内容をわかりやすく説明したりする。']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sent_all[16:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9494d0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>鑑</td>\n",
       "      <td>賞</td>\n",
       "      <td>と</td>\n",
       "      <td>は</td>\n",
       "      <td>音</td>\n",
       "      <td>楽</td>\n",
       "      <td>を</td>\n",
       "      <td>聴</td>\n",
       "      <td>い</td>\n",
       "      <td>て</td>\n",
       "      <td>そ</td>\n",
       "      <td>れ</td>\n",
       "      <td>を</td>\n",
       "      <td>味</td>\n",
       "      <td>わ</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>、</td>\n",
       "      <td>価</td>\n",
       "      <td>値</td>\n",
       "      <td>を</td>\n",
       "      <td>見</td>\n",
       "      <td>極</td>\n",
       "      <td>め</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>こ</td>\n",
       "      <td>と</td>\n",
       "      <td>で</td>\n",
       "      <td>あ</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23  \\\n",
       "sent   鑑  賞  と  は  音  楽  を  聴  い  て  そ  れ  を  味  わ  っ  た  り  、  価  値  を  見  極   \n",
       "label  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ   \n",
       "\n",
       "      24 25 26 27 28 29 30 31 32 33 34  \n",
       "sent   め  た  り  す  る  こ  と  で  あ  る  。  \n",
       "label  Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_try = [wiki_sent_all[16][word] for word in range(len(wiki_sent_all[16]))]\n",
    "label_try = [wiki_tag_all[16][word] for word in range(len(wiki_tag_all[16]))]\n",
    "df = pd.DataFrame([sent_try, label_try], index=[\"sent\", \"label\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "221cddae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>こ</td>\n",
       "      <td>の</td>\n",
       "      <td>結</td>\n",
       "      <td>果</td>\n",
       "      <td>、</td>\n",
       "      <td>例</td>\n",
       "      <td>え</td>\n",
       "      <td>ば</td>\n",
       "      <td>旧</td>\n",
       "      <td>暦</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>「</td>\n",
       "      <td>秋</td>\n",
       "      <td>」</td>\n",
       "      <td>で</td>\n",
       "      <td>あ</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>「</td>\n",
       "      <td>文</td>\n",
       "      <td>月</td>\n",
       "      <td>（</td>\n",
       "      <td>7</td>\n",
       "      <td>月</td>\n",
       "      <td>）</td>\n",
       "      <td>」</td>\n",
       "      <td>が</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>「</td>\n",
       "      <td>夏</td>\n",
       "      <td>」</td>\n",
       "      <td>に</td>\n",
       "      <td>な</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>、</td>\n",
       "      <td>7</td>\n",
       "      <td>月</td>\n",
       "      <td>9</td>\n",
       "      <td>日</td>\n",
       "      <td>頃</td>\n",
       "      <td>か</td>\n",
       "      <td>ら</td>\n",
       "      <td>8</td>\n",
       "      <td>月</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>日</td>\n",
       "      <td>頃</td>\n",
       "      <td>ま</td>\n",
       "      <td>で</td>\n",
       "      <td>で</td>\n",
       "      <td>あ</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>二</td>\n",
       "      <td>百</td>\n",
       "      <td>十</td>\n",
       "      <td>日</td>\n",
       "      <td>が</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>9</td>\n",
       "      <td>月</td>\n",
       "      <td>1</td>\n",
       "      <td>日</td>\n",
       "      <td>に</td>\n",
       "      <td>な</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>、</td>\n",
       "      <td>盆</td>\n",
       "      <td>の</td>\n",
       "      <td>節</td>\n",
       "      <td>会</td>\n",
       "      <td>を</td>\n",
       "      <td>行</td>\n",
       "      <td>う</td>\n",
       "      <td>時</td>\n",
       "      <td>期</td>\n",
       "      <td>が</td>\n",
       "      <td>地</td>\n",
       "      <td>域</td>\n",
       "      <td>に</td>\n",
       "      <td>よ</td>\n",
       "      <td>っ</td>\n",
       "      <td>て</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>7</td>\n",
       "      <td>月</td>\n",
       "      <td>と</td>\n",
       "      <td>新</td>\n",
       "      <td>暦</td>\n",
       "      <td>8</td>\n",
       "      <td>月</td>\n",
       "      <td>に</td>\n",
       "      <td>別</td>\n",
       "      <td>れ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>な</td>\n",
       "      <td>ど</td>\n",
       "      <td>、</td>\n",
       "      <td>月</td>\n",
       "      <td>遅</td>\n",
       "      <td>れ</td>\n",
       "      <td>に</td>\n",
       "      <td>よ</td>\n",
       "      <td>る</td>\n",
       "      <td>そ</td>\n",
       "      <td>れ</td>\n",
       "      <td>ま</td>\n",
       "      <td>で</td>\n",
       "      <td>の</td>\n",
       "      <td>慣</td>\n",
       "      <td>習</td>\n",
       "      <td>と</td>\n",
       "      <td>の</td>\n",
       "      <td>相</td>\n",
       "      <td>違</td>\n",
       "      <td>が</td>\n",
       "      <td>発</td>\n",
       "      <td>生</td>\n",
       "      <td>し</td>\n",
       "      <td>て</td>\n",
       "      <td>い</td>\n",
       "      <td>る</td>\n",
       "      <td>ほ</td>\n",
       "      <td>か</td>\n",
       "      <td>、</td>\n",
       "      <td>前</td>\n",
       "      <td>記</td>\n",
       "      <td>の</td>\n",
       "      <td>よ</td>\n",
       "      <td>う</td>\n",
       "      <td>な</td>\n",
       "      <td>元</td>\n",
       "      <td>々</td>\n",
       "      <td>の</td>\n",
       "      <td>中</td>\n",
       "      <td>国</td>\n",
       "      <td>風</td>\n",
       "      <td>の</td>\n",
       "      <td>定</td>\n",
       "      <td>義</td>\n",
       "      <td>も</td>\n",
       "      <td>絡</td>\n",
       "      <td>み</td>\n",
       "      <td>、</td>\n",
       "      <td>現</td>\n",
       "      <td>在</td>\n",
       "      <td>で</td>\n",
       "      <td>も</td>\n",
       "      <td>若</td>\n",
       "      <td>干</td>\n",
       "      <td>の</td>\n",
       "      <td>違</td>\n",
       "      <td>和</td>\n",
       "      <td>感</td>\n",
       "      <td>が</td>\n",
       "      <td>存</td>\n",
       "      <td>在</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>こ</td>\n",
       "      <td>と</td>\n",
       "      <td>か</td>\n",
       "      <td>ら</td>\n",
       "      <td>、</td>\n",
       "      <td>日</td>\n",
       "      <td>本</td>\n",
       "      <td>の</td>\n",
       "      <td>メ</td>\n",
       "      <td>デ</td>\n",
       "      <td>ィ</td>\n",
       "      <td>ア</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>「</td>\n",
       "      <td>暦</td>\n",
       "      <td>の</td>\n",
       "      <td>上</td>\n",
       "      <td>で</td>\n",
       "      <td>は</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>」</td>\n",
       "      <td>\\t</td>\n",
       "      <td>と</td>\n",
       "      <td>前</td>\n",
       "      <td>置</td>\n",
       "      <td>き</td>\n",
       "      <td>し</td>\n",
       "      <td>て</td>\n",
       "      <td>説</td>\n",
       "      <td>明</td>\n",
       "      <td>さ</td>\n",
       "      <td>れ</td>\n",
       "      <td>る</td>\n",
       "      <td>こ</td>\n",
       "      <td>と</td>\n",
       "      <td>が</td>\n",
       "      <td>あ</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17   \\\n",
       "sent    こ   の   結   果   、   例   え   ば   旧   暦   で   は   「   秋   」   で   あ   っ   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35   \\\n",
       "sent    た   「   文   月   （   7   月   ）   」   が   新   暦   で   は   「   夏   」   に   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53   \\\n",
       "sent    な   っ   た   り   、   7   月   9   日   頃   か   ら   8   月   1   1   日   頃   \n",
       "label   Ｏ   Ｏ   Ｏ   Ｏ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71   \\\n",
       "sent    ま   で   で   あ   っ   た   二   百   十   日   が   新   暦   9   月   1   日   に   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89   \\\n",
       "sent    な   っ   た   り   、   盆   の   節   会   を   行   う   時   期   が   地   域   に   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      90  91  92  93  94  95  96  97  98  99  100 101 102 103 104 105 106 107  \\\n",
       "sent    よ   っ   て   新   暦   7   月   と   新   暦   8   月   に   別   れ   た   り   す   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｏ   Ｏ   Ｏ   Ｏ   Ｏ   \n",
       "\n",
       "      108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125  \\\n",
       "sent    る   な   ど   、   月   遅   れ   に   よ   る   そ   れ   ま   で   の   慣   習   と   \n",
       "label   Ｏ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143  \\\n",
       "sent    の   相   違   が   発   生   し   て   い   る   ほ   か   、   前   記   の   よ   う   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161  \\\n",
       "sent    な   元   々   の   中   国   風   の   定   義   も   絡   み   、   現   在   で   も   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179  \\\n",
       "sent    若   干   の   違   和   感   が   存   在   す   る   こ   と   か   ら   、   日   本   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197  \\\n",
       "sent    の   メ   デ   ィ   ア   で   は   「   暦   の   上   で   は   …   …   」  \\t   と   \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   \n",
       "\n",
       "      198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213  \n",
       "sent    前   置   き   し   て   説   明   さ   れ   る   こ   と   が   あ   る   。  \n",
       "label   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ   Ｘ  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('Ｘ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c2e68d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-b738ffe15d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ｘ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfileout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   4325\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m             \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4328\u001b[0m         )\n\u001b[1;32m   4329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6081\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6082\u001b[0m                 new_data = self._mgr.fillna(\n\u001b[0;32m-> 6083\u001b[0;31m                     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6084\u001b[0m                 )\n\u001b[1;32m   6085\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"BlockManager\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         return self.apply(\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0;34m\"fillna\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         )\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# equivalent: _try_coerce_args(value) would not raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_downcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# we can't process the value, but nothing to do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_maybe_downcast\u001b[0;34m(self, blocks, downcast)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0;31m# split and convert the blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_hold_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0;31m# split and convert the blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_hold_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, copy, datetime, numeric, timedelta, coerce)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m             \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_and_operate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2444\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36msplit_and_operate\u001b[0;34m(self, mask, f, inplace)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# need a new block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0mnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(mask, val, idx)\u001b[0m\n\u001b[1;32m   2431\u001b[0m                 \u001b[0mtimedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m                 \u001b[0mcoerce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m             )\n\u001b[1;32m   2435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36msoft_convert_objects\u001b[0;34m(values, datetime, numeric, timedelta, coerce, copy)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# bound of nanosecond-resolution 64-bit integers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_convert_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_datetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jpt/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('check_Tari.txt', 'w') as fileout:\n",
    "    for i in range(len(wiki_sent_all)):\n",
    "        data_sent =[wiki_sent_all[i][word] for word in range(len(wiki_sent_all[i]))]\n",
    "        data_label =[wiki_tag_all[i][word] for word in range(len(wiki_tag_all[i]))]\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        dd = pd.DataFrame([data_sent, data_label], index=[\"sent\", \"label\"])\n",
    "        df = dd.fillna('Ｘ')\n",
    "        answer = ''.join(df.loc['label'])\n",
    "        fileout.write(answer+'\\n')\n",
    "fileout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0b500",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d40935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "#         sentence = self.data.sentence[index].strip().split()\n",
    "        sentence = [i for i in self.data.sentence[index]]\n",
    "#         word_tag_n = [''.join(label) for label in word_labels[index]]\n",
    "#         word_labels = self.data.word_labels[index].split() \n",
    "    \n",
    "        word_labels = [j for j in self.data.word_labels[index]]    \n",
    "\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_pretokenized=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "\n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ba734a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame([jp_train_texts, jp_train_tag], index=[\"sentence\", \"word_labels\"]).T\n",
    "test_dataset = pd.DataFrame([jp_test_texts, jp_test_tag], index=[\"sentence\", \"word_labels\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0044bb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "たり型 TRAIN Dataset: (49000, 2)\n",
      "たり型 TEST Dataset: (21000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"たり型 TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"たり型 TEST Dataset: {}\".format(test_dataset.shape))\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af2796cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   2,  890,  869,  927,  861,  828,   32,   30,   30,   33, 2181, 2754,\n",
       "         3452,  888,  897, 5163, 1370, 1308,  890,  916, 3806, 2485, 1719,  861,\n",
       "         1376, 2675,  875,  925,  829,    3,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'offset_mapping': tensor([[0, 0],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]),\n",
       " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9c0e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "と           0\n",
      "こ           0\n",
      "ろ           0\n",
      "か           0\n",
      "、           0\n",
      "2           0\n",
      "0           0\n",
      "0           0\n",
      "3           0\n",
      "年           0\n",
      "時           0\n",
      "点           0\n",
      "て           0\n",
      "は           0\n",
      "輸           0\n",
      "出           0\n",
      "入           0\n",
      "と           0\n",
      "も           0\n",
      "相           0\n",
      "手           0\n",
      "国           0\n",
      "か           0\n",
      "分           0\n",
      "散           0\n",
      "す           0\n",
      "る           0\n",
      "。           0\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
    "    print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4ac2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params) ## **表示字典 一批取4\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0280c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32768, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('cl-tohoku/bert-large-japanese', num_labels=len(labels_to_ids))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a26a6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3112, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = training_set[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98d1247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53518d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c00bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model,\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy, tr_opt_acc = 0, 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_opt_zero_steps, tr_opt_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader): # 一次取一批\n",
    "#         print(idx)\n",
    "#         if idx == 193:\n",
    "#             print(batch['input_ids'])\n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels) ## 輸入 算出損失值\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,) ## 算每一批預測值\n",
    "\n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1]).ravel()\n",
    "        tmp_tr_opt_acc = tp/(fn+fp+tp)\n",
    "        if np.isnan(tmp_tr_opt_acc) == True:\n",
    "            tmp_tr_opt_acc = 1\n",
    "            tr_opt_zero_steps += 1\n",
    "        else:\n",
    "            tr_opt_steps += 1\n",
    "            tr_opt_acc += tmp_tr_opt_acc            \n",
    "            \n",
    "            if idx % 100==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "                opt_acc_step = tr_opt_acc / tr_opt_steps\n",
    "                print(f\"Training Opt_acc epoch: {opt_acc_step}\")\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() ## T度值 清空\n",
    "        loss.backward() ## 反向回饋\"修正權重\"\n",
    "        optimizer.step() ## 透過優化器(先算損失值得反向回饋)執行修正\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    tr_opt_acc = tr_opt_acc / tr_opt_steps\n",
    "    \n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training Opt_acc epoch: {tr_opt_acc}\")\n",
    "    print('tr_zero_steps:',tr_opt_zero_steps)\n",
    "    print('tr_opt_steps:',tr_opt_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "835ffabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 1.419046401977539\n",
      "Training Opt_acc epoch: 0.038461538461538464\n",
      "Training loss per 100 training steps: 0.13490166590378463\n",
      "Training Opt_acc epoch: 0.5546958837696521\n",
      "Training loss per 100 training steps: 0.08476757900459247\n",
      "Training Opt_acc epoch: 0.7150622410074727\n",
      "Training loss per 100 training steps: 0.0647275033694084\n",
      "Training Opt_acc epoch: 0.7771185800171692\n",
      "Training loss per 100 training steps: 0.05537666761854308\n",
      "Training Opt_acc epoch: 0.8130118672134662\n",
      "Training loss per 100 training steps: 0.04896588730356845\n",
      "Training Opt_acc epoch: 0.8343752005468914\n",
      "Training loss per 100 training steps: 0.04405566850887688\n",
      "Training Opt_acc epoch: 0.8484817971392932\n",
      "Training loss per 100 training steps: 0.039971953419857716\n",
      "Training Opt_acc epoch: 0.8605450136152235\n",
      "Training loss per 100 training steps: 0.03474579202604125\n",
      "Training Opt_acc epoch: 0.8795761331246403\n",
      "Training loss per 100 training steps: 0.032541070489017374\n",
      "Training Opt_acc epoch: 0.8862142658038316\n",
      "Training loss per 100 training steps: 0.030934319060430527\n",
      "Training Opt_acc epoch: 0.8914273602582019\n",
      "Training loss per 100 training steps: 0.02965927376301062\n",
      "Training Opt_acc epoch: 0.8966704272734084\n",
      "Training loss per 100 training steps: 0.028044488057366763\n",
      "Training Opt_acc epoch: 0.9028398725269738\n",
      "Training loss per 100 training steps: 0.026837019864495836\n",
      "Training Opt_acc epoch: 0.9074158701520253\n",
      "Training loss per 100 training steps: 0.02602652972804775\n",
      "Training Opt_acc epoch: 0.9104206823545683\n",
      "Training loss per 100 training steps: 0.02515046722624717\n",
      "Training Opt_acc epoch: 0.9137376213928509\n",
      "Training loss per 100 training steps: 0.02443197179364602\n",
      "Training Opt_acc epoch: 0.9152213699149687\n",
      "Training loss per 100 training steps: 0.023700955614302213\n",
      "Training Opt_acc epoch: 0.9176801073281191\n",
      "Training loss per 100 training steps: 0.022902395852771908\n",
      "Training Opt_acc epoch: 0.9206710647687979\n",
      "Training loss per 100 training steps: 0.022405672372282668\n",
      "Training Opt_acc epoch: 0.9225545256065629\n",
      "Training loss per 100 training steps: 0.021063103956990344\n",
      "Training Opt_acc epoch: 0.9264776852561424\n",
      "Training loss per 100 training steps: 0.02057549239810296\n",
      "Training Opt_acc epoch: 0.9281261361824588\n",
      "Training loss per 100 training steps: 0.02019512257229087\n",
      "Training Opt_acc epoch: 0.929284620500557\n",
      "Training loss per 100 training steps: 0.01961175088467235\n",
      "Training Opt_acc epoch: 0.9313479888709073\n",
      "Training loss per 100 training steps: 0.019185402168694986\n",
      "Training Opt_acc epoch: 0.9327734185924061\n",
      "Training loss per 100 training steps: 0.018970327658452336\n",
      "Training Opt_acc epoch: 0.9334902415387534\n",
      "Training loss per 100 training steps: 0.018517325840200518\n",
      "Training Opt_acc epoch: 0.9350933520158824\n",
      "Training loss per 100 training steps: 0.018239260526657745\n",
      "Training Opt_acc epoch: 0.9360978584162716\n",
      "Training loss per 100 training steps: 0.01816979659952085\n",
      "Training Opt_acc epoch: 0.9365566050152576\n",
      "Training loss per 100 training steps: 0.017967644622660355\n",
      "Training Opt_acc epoch: 0.9369843869922352\n",
      "Training loss per 100 training steps: 0.017728061427874486\n",
      "Training Opt_acc epoch: 0.9379581944805806\n",
      "Training loss per 100 training steps: 0.01745765039843684\n",
      "Training Opt_acc epoch: 0.938933610754419\n",
      "Training loss per 100 training steps: 0.017229218778490377\n",
      "Training Opt_acc epoch: 0.9394957110728316\n",
      "Training loss per 100 training steps: 0.017104609890695975\n",
      "Training Opt_acc epoch: 0.9400022078531233\n",
      "Training loss per 100 training steps: 0.016797343591378352\n",
      "Training Opt_acc epoch: 0.9411004179240327\n",
      "Training loss per 100 training steps: 0.01657904821838986\n",
      "Training Opt_acc epoch: 0.9419934603764634\n",
      "Training loss per 100 training steps: 0.01628680328329\n",
      "Training Opt_acc epoch: 0.9430337501163999\n",
      "Training loss per 100 training steps: 0.016098816496269003\n",
      "Training Opt_acc epoch: 0.9436013501515779\n",
      "Training loss per 100 training steps: 0.015910867468604185\n",
      "Training Opt_acc epoch: 0.9442234340498333\n",
      "Training loss per 100 training steps: 0.01570676480169541\n",
      "Training Opt_acc epoch: 0.944650309103877\n",
      "Training loss per 100 training steps: 0.01549398265448316\n",
      "Training Opt_acc epoch: 0.9453730937209603\n",
      "Training loss per 100 training steps: 0.015304803864373497\n",
      "Training Opt_acc epoch: 0.9461248514252976\n",
      "Training loss per 100 training steps: 0.015092264740681137\n",
      "Training Opt_acc epoch: 0.9468575055955868\n",
      "Training loss per 100 training steps: 0.014915131997192329\n",
      "Training Opt_acc epoch: 0.9474994146849167\n",
      "Training loss per 100 training steps: 0.014565810334581474\n",
      "Training Opt_acc epoch: 0.9488002657392571\n",
      "Training loss per 100 training steps: 0.014383300803479106\n",
      "Training Opt_acc epoch: 0.9494376666212916\n",
      "Training loss per 100 training steps: 0.014211883602968692\n",
      "Training Opt_acc epoch: 0.9500000794125169\n",
      "Training loss per 100 training steps: 0.014086451546152675\n",
      "Training Opt_acc epoch: 0.9502259601046946\n",
      "Training loss per 100 training steps: 0.013932133443767172\n",
      "Training Opt_acc epoch: 0.9508482213523805\n",
      "Training loss per 100 training steps: 0.013797002487049568\n",
      "Training Opt_acc epoch: 0.9512484711736533\n",
      "Training loss per 100 training steps: 0.013709139903657405\n",
      "Training Opt_acc epoch: 0.9514496904461679\n",
      "Training loss per 100 training steps: 0.013597359630440372\n",
      "Training Opt_acc epoch: 0.9519233291554456\n",
      "Training loss per 100 training steps: 0.01343409822192326\n",
      "Training Opt_acc epoch: 0.9523801934465582\n",
      "Training loss per 100 training steps: 0.013313922788009136\n",
      "Training Opt_acc epoch: 0.9527842988939508\n",
      "Training loss per 100 training steps: 0.013219395740370806\n",
      "Training Opt_acc epoch: 0.953025452845092\n",
      "Training loss per 100 training steps: 0.013146114779258467\n",
      "Training Opt_acc epoch: 0.9530466184758378\n",
      "Training loss per 100 training steps: 0.013055402614732791\n",
      "Training Opt_acc epoch: 0.9532401435390272\n",
      "Training loss per 100 training steps: 0.012903324383886146\n",
      "Training Opt_acc epoch: 0.9537302753219048\n",
      "Training loss per 100 training steps: 0.012799493348667191\n",
      "Training Opt_acc epoch: 0.9541298452164901\n",
      "Training loss per 100 training steps: 0.012592342749178562\n",
      "Training Opt_acc epoch: 0.954891334087657\n",
      "Training loss per 100 training steps: 0.012529122850779293\n",
      "Training Opt_acc epoch: 0.9551604460058047\n",
      "Training loss per 100 training steps: 0.012448436262139994\n",
      "Training Opt_acc epoch: 0.9553830845003891\n",
      "Training loss per 100 training steps: 0.012370042679801272\n",
      "Training Opt_acc epoch: 0.9553956639292853\n",
      "Training loss per 100 training steps: 0.012266982405373121\n",
      "Training Opt_acc epoch: 0.9557568054282214\n",
      "Training loss per 100 training steps: 0.012163262019559452\n",
      "Training Opt_acc epoch: 0.9561451480682308\n",
      "Training loss per 100 training steps: 0.01209279016737605\n",
      "Training Opt_acc epoch: 0.9563281389307049\n",
      "Training loss per 100 training steps: 0.012017405818761298\n",
      "Training Opt_acc epoch: 0.9565817308496096\n",
      "Training loss per 100 training steps: 0.011918031161965754\n",
      "Training Opt_acc epoch: 0.9567955595100494\n",
      "Training loss per 100 training steps: 0.011833168973910643\n",
      "Training Opt_acc epoch: 0.9570380080205437\n",
      "Training loss per 100 training steps: 0.011758355779231738\n",
      "Training Opt_acc epoch: 0.9573132490773442\n",
      "Training loss per 100 training steps: 0.011656772070411959\n",
      "Training Opt_acc epoch: 0.9577266194237127\n",
      "Training loss per 100 training steps: 0.01156041176381863\n",
      "Training Opt_acc epoch: 0.9579877804608189\n",
      "Training loss per 100 training steps: 0.011504563917057028\n",
      "Training Opt_acc epoch: 0.9582280949791906\n",
      "Training loss per 100 training steps: 0.011493630581174747\n",
      "Training Opt_acc epoch: 0.9584392967950193\n",
      "Training loss per 100 training steps: 0.011435509073141935\n",
      "Training Opt_acc epoch: 0.9587221644428324\n",
      "Training loss per 100 training steps: 0.011270236348313147\n",
      "Training Opt_acc epoch: 0.9591926630329338\n",
      "Training loss per 100 training steps: 0.01120158375325603\n",
      "Training Opt_acc epoch: 0.959470640461113\n",
      "Training loss per 100 training steps: 0.011084850865407185\n",
      "Training Opt_acc epoch: 0.9598263236527773\n",
      "Training loss per 100 training steps: 0.01102840052961827\n",
      "Training Opt_acc epoch: 0.9598605119996878\n",
      "Training loss per 100 training steps: 0.010999168331671876\n",
      "Training Opt_acc epoch: 0.9599560530676087\n",
      "Training loss per 100 training steps: 0.0109922313935421\n",
      "Training Opt_acc epoch: 0.9600780969232234\n",
      "Training loss per 100 training steps: 0.01092515333780248\n",
      "Training Opt_acc epoch: 0.9602092986204808\n",
      "Training loss per 100 training steps: 0.01085979401611991\n",
      "Training Opt_acc epoch: 0.9605186855978437\n",
      "Training loss per 100 training steps: 0.010754030587898719\n",
      "Training Opt_acc epoch: 0.9609223305770704\n",
      "Training loss per 100 training steps: 0.010684524013586882\n",
      "Training Opt_acc epoch: 0.9611722895276478\n",
      "Training loss per 100 training steps: 0.010631628021883593\n",
      "Training Opt_acc epoch: 0.961407196028933\n",
      "Training loss per 100 training steps: 0.01056481702527854\n",
      "Training Opt_acc epoch: 0.9617205156997243\n",
      "Training loss per 100 training steps: 0.01052397287037308\n",
      "Training Opt_acc epoch: 0.9617575918961798\n",
      "Training loss per 100 training steps: 0.010477561300811462\n",
      "Training Opt_acc epoch: 0.9618319999728879\n",
      "Training loss per 100 training steps: 0.010442472851763843\n",
      "Training Opt_acc epoch: 0.9619199500276587\n",
      "Training loss per 100 training steps: 0.010378389133754416\n",
      "Training Opt_acc epoch: 0.9621778375505105\n",
      "Training loss per 100 training steps: 0.010324042913209514\n",
      "Training Opt_acc epoch: 0.9623503282778082\n",
      "Training loss per 100 training steps: 0.010312992283141365\n",
      "Training Opt_acc epoch: 0.962478454972252\n",
      "Training loss per 100 training steps: 0.010300915727171978\n",
      "Training Opt_acc epoch: 0.9625635752635706\n",
      "Training loss per 100 training steps: 0.010224840238971305\n",
      "Training Opt_acc epoch: 0.962827625525679\n",
      "Training loss per 100 training steps: 0.010165306610948078\n",
      "Training Opt_acc epoch: 0.9630841833498459\n",
      "Training loss per 100 training steps: 0.01013844478964395\n",
      "Training Opt_acc epoch: 0.9632599151284954\n",
      "Training loss per 100 training steps: 0.010075121669916504\n",
      "Training Opt_acc epoch: 0.9635328429159914\n",
      "Training loss per 100 training steps: 0.01005330889473796\n",
      "Training Opt_acc epoch: 0.9635522156445332\n",
      "Training loss per 100 training steps: 0.00993546154089032\n",
      "Training Opt_acc epoch: 0.963963854424209\n",
      "Training loss per 100 training steps: 0.009870745389347525\n",
      "Training Opt_acc epoch: 0.9642275348988921\n",
      "Training loss per 100 training steps: 0.009821878158259818\n",
      "Training Opt_acc epoch: 0.9644479351512928\n",
      "Training loss per 100 training steps: 0.00974569993621574\n",
      "Training Opt_acc epoch: 0.9647280838836235\n",
      "Training loss per 100 training steps: 0.00968990804115082\n",
      "Training Opt_acc epoch: 0.9649112798776769\n",
      "Training loss per 100 training steps: 0.009635353923908926\n",
      "Training Opt_acc epoch: 0.9651275400612436\n",
      "Training loss per 100 training steps: 0.009615589917444491\n",
      "Training Opt_acc epoch: 0.9650834267078603\n",
      "Training loss per 100 training steps: 0.009588865943504787\n",
      "Training Opt_acc epoch: 0.9651294209300043\n",
      "Training loss per 100 training steps: 0.009534299850662854\n",
      "Training Opt_acc epoch: 0.9651404579665389\n",
      "Training loss per 100 training steps: 0.009519077167114897\n",
      "Training Opt_acc epoch: 0.9652233363012506\n",
      "Training loss per 100 training steps: 0.009490072608610166\n",
      "Training Opt_acc epoch: 0.9653257415586287\n",
      "Training loss per 100 training steps: 0.009461690571416104\n",
      "Training Opt_acc epoch: 0.9654203611332837\n",
      "Training loss per 100 training steps: 0.009409612122721187\n",
      "Training Opt_acc epoch: 0.9656578666667314\n",
      "Training loss per 100 training steps: 0.009371566064128482\n",
      "Training Opt_acc epoch: 0.9658468134379014\n",
      "Training loss per 100 training steps: 0.009321248705438835\n",
      "Training Opt_acc epoch: 0.9660148018932475\n",
      "Training loss epoch: 0.00930768192037919\n",
      "Training accuracy epoch: 0.9971716359830524\n",
      "Training Opt_acc epoch: 0.966029096508025\n",
      "tr_zero_steps: 801\n",
      "tr_opt_steps: 11449\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 0.0038817615699252347\n",
      "Training Opt_acc epoch: 0.9847676100219511\n",
      "Training loss per 100 training steps: 0.003566803641833939\n",
      "Training Opt_acc epoch: 0.9867087964484353\n",
      "Training loss per 100 training steps: 0.0034402632226042135\n",
      "Training Opt_acc epoch: 0.9880585937280403\n",
      "Training loss per 100 training steps: 0.0036659306858328926\n",
      "Training Opt_acc epoch: 0.9884241433177938\n",
      "Training loss per 100 training steps: 0.0036516096423643246\n",
      "Training Opt_acc epoch: 0.987548804153136\n",
      "Training loss per 100 training steps: 0.0036576841696040357\n",
      "Training Opt_acc epoch: 0.986845207229426\n",
      "Training loss per 100 training steps: 0.003636761691114906\n",
      "Training Opt_acc epoch: 0.9868837398983248\n",
      "Training loss per 100 training steps: 0.0037424880944360406\n",
      "Training Opt_acc epoch: 0.9865363447090281\n",
      "Training loss per 100 training steps: 0.0035829359130514363\n",
      "Training Opt_acc epoch: 0.9870258098164797\n",
      "Training loss per 100 training steps: 0.003722012711291803\n",
      "Training Opt_acc epoch: 0.9863958740392227\n",
      "Training loss per 100 training steps: 0.003765792245948994\n",
      "Training Opt_acc epoch: 0.9863400261559478\n",
      "Training loss per 100 training steps: 0.003848032757546742\n",
      "Training Opt_acc epoch: 0.9864609270277654\n",
      "Training loss per 100 training steps: 0.0037407121170790294\n",
      "Training Opt_acc epoch: 0.986586338088045\n",
      "Training loss per 100 training steps: 0.003786655051819494\n",
      "Training Opt_acc epoch: 0.9866683799777378\n",
      "Training loss per 100 training steps: 0.0037148363327844827\n",
      "Training Opt_acc epoch: 0.9868817111741867\n",
      "Training loss per 100 training steps: 0.004142994633719991\n",
      "Training Opt_acc epoch: 0.9853373383711695\n",
      "Training loss per 100 training steps: 0.004235224994558841\n",
      "Training Opt_acc epoch: 0.9847771020604946\n",
      "Training loss per 100 training steps: 0.0042985419517748555\n",
      "Training Opt_acc epoch: 0.9847200272543946\n",
      "Training loss per 100 training steps: 0.0043335357652808815\n",
      "Training Opt_acc epoch: 0.9846278309272292\n",
      "Training loss per 100 training steps: 0.00429250812061099\n",
      "Training Opt_acc epoch: 0.9842607379720689\n",
      "Training loss per 100 training steps: 0.004242214871564395\n",
      "Training Opt_acc epoch: 0.9844865997098202\n",
      "Training loss per 100 training steps: 0.004159626450425474\n",
      "Training Opt_acc epoch: 0.9847335745014574\n",
      "Training loss per 100 training steps: 0.00431710778275492\n",
      "Training Opt_acc epoch: 0.9841066598937903\n",
      "Training loss per 100 training steps: 0.004408644688809465\n",
      "Training Opt_acc epoch: 0.9836439397361377\n",
      "Training loss per 100 training steps: 0.004367976689807738\n",
      "Training Opt_acc epoch: 0.9838981016991412\n",
      "Training loss per 100 training steps: 0.004421642619672655\n",
      "Training Opt_acc epoch: 0.9837017988714696\n",
      "Training loss per 100 training steps: 0.004475628847672922\n",
      "Training Opt_acc epoch: 0.9833715130593466\n",
      "Training loss per 100 training steps: 0.004495861919057573\n",
      "Training Opt_acc epoch: 0.983508965276152\n",
      "Training loss per 100 training steps: 0.00449624407749722\n",
      "Training Opt_acc epoch: 0.9830703714954147\n",
      "Training loss per 100 training steps: 0.004550808223174724\n",
      "Training Opt_acc epoch: 0.9824615651834372\n",
      "Training loss per 100 training steps: 0.0045025239141389855\n",
      "Training Opt_acc epoch: 0.9826189114500111\n",
      "Training loss per 100 training steps: 0.004462934661272765\n",
      "Training Opt_acc epoch: 0.9829186394659318\n",
      "Training loss per 100 training steps: 0.004526150465480023\n",
      "Training Opt_acc epoch: 0.9825978786640943\n",
      "Training loss per 100 training steps: 0.004518063624088195\n",
      "Training Opt_acc epoch: 0.9826437072366856\n",
      "Training loss per 100 training steps: 0.004488055993132337\n",
      "Training Opt_acc epoch: 0.9828527571584221\n",
      "Training loss per 100 training steps: 0.004505551380652696\n",
      "Training Opt_acc epoch: 0.9824893509195807\n",
      "Training loss per 100 training steps: 0.004489044675181114\n",
      "Training Opt_acc epoch: 0.982317890005999\n",
      "Training loss per 100 training steps: 0.004492910599107853\n",
      "Training Opt_acc epoch: 0.982325454121267\n",
      "Training loss per 100 training steps: 0.004528661334960772\n",
      "Training Opt_acc epoch: 0.9822676091967952\n",
      "Training loss per 100 training steps: 0.004515441494973268\n",
      "Training Opt_acc epoch: 0.9823444501238722\n",
      "Training loss per 100 training steps: 0.004506075008251416\n",
      "Training Opt_acc epoch: 0.9824224093133923\n",
      "Training loss per 100 training steps: 0.004474830394186662\n",
      "Training Opt_acc epoch: 0.9826017503914197\n",
      "Training loss per 100 training steps: 0.004394338657277116\n",
      "Training Opt_acc epoch: 0.9829273536779026\n",
      "Training loss per 100 training steps: 0.004397249429058996\n",
      "Training Opt_acc epoch: 0.9830480243500054\n",
      "Training loss per 100 training steps: 0.004367344702706656\n",
      "Training Opt_acc epoch: 0.9831404718614859\n",
      "Training loss per 100 training steps: 0.004331492948072028\n",
      "Training Opt_acc epoch: 0.9832066860167127\n",
      "Training loss per 100 training steps: 0.004346049952003513\n",
      "Training Opt_acc epoch: 0.9829999703452307\n",
      "Training loss per 100 training steps: 0.004295227254803813\n",
      "Training Opt_acc epoch: 0.9831691601837175\n",
      "Training loss per 100 training steps: 0.00426081623853057\n",
      "Training Opt_acc epoch: 0.9833204906282412\n",
      "Training loss per 100 training steps: 0.004233187118064114\n",
      "Training Opt_acc epoch: 0.9834637840826139\n",
      "Training loss per 100 training steps: 0.004202700616015852\n",
      "Training Opt_acc epoch: 0.9835598633017557\n",
      "Training loss per 100 training steps: 0.004188200781733427\n",
      "Training Opt_acc epoch: 0.9835839904210847\n",
      "Training loss per 100 training steps: 0.004174570734153787\n",
      "Training Opt_acc epoch: 0.9837274465206253\n",
      "Training loss per 100 training steps: 0.004182882674663067\n",
      "Training Opt_acc epoch: 0.9838209373778714\n",
      "Training loss per 100 training steps: 0.0041924425247216604\n",
      "Training Opt_acc epoch: 0.9838298361685084\n",
      "Training loss per 100 training steps: 0.004182184684551785\n",
      "Training Opt_acc epoch: 0.983839398540356\n",
      "Training loss per 100 training steps: 0.004173706580146106\n",
      "Training Opt_acc epoch: 0.9838300102340788\n",
      "Training loss per 100 training steps: 0.0041484006092535755\n",
      "Training Opt_acc epoch: 0.9839318998991912\n",
      "Training loss per 100 training steps: 0.004118205190344263\n",
      "Training Opt_acc epoch: 0.9839893755888146\n",
      "Training loss per 100 training steps: 0.004095928487691736\n",
      "Training Opt_acc epoch: 0.98397853480765\n",
      "Training loss per 100 training steps: 0.004078190774883123\n",
      "Training Opt_acc epoch: 0.9840203709825086\n",
      "Training loss per 100 training steps: 0.004058153148143841\n",
      "Training Opt_acc epoch: 0.9841434116945466\n",
      "Training loss per 100 training steps: 0.004059807861139164\n",
      "Training Opt_acc epoch: 0.9841412347232374\n",
      "Training loss per 100 training steps: 0.0040438391159507495\n",
      "Training Opt_acc epoch: 0.9842962740847913\n",
      "Training loss per 100 training steps: 0.004034688508372087\n",
      "Training Opt_acc epoch: 0.9841343962662461\n",
      "Training loss per 100 training steps: 0.004033286664723616\n",
      "Training Opt_acc epoch: 0.9841995825342331\n",
      "Training loss per 100 training steps: 0.004028770431962087\n",
      "Training Opt_acc epoch: 0.9842527400845654\n",
      "Training loss per 100 training steps: 0.004076874005550075\n",
      "Training Opt_acc epoch: 0.984076958600753\n",
      "Training loss per 100 training steps: 0.004066260926988304\n",
      "Training Opt_acc epoch: 0.9841915385050882\n",
      "Training loss per 100 training steps: 0.004063894158342857\n",
      "Training Opt_acc epoch: 0.9842343827577545\n",
      "Training loss per 100 training steps: 0.004050276873375698\n",
      "Training Opt_acc epoch: 0.9842590131812946\n",
      "Training loss per 100 training steps: 0.0040288029947688625\n",
      "Training Opt_acc epoch: 0.9842789506082424\n",
      "Training loss per 100 training steps: 0.004031862093866631\n",
      "Training Opt_acc epoch: 0.9843082115109121\n",
      "Training loss per 100 training steps: 0.004055473526829097\n",
      "Training Opt_acc epoch: 0.9842494093604747\n",
      "Training loss per 100 training steps: 0.004065644423670151\n",
      "Training Opt_acc epoch: 0.9842986585447905\n",
      "Training loss per 100 training steps: 0.0040768685141761764\n",
      "Training Opt_acc epoch: 0.984302655474493\n",
      "Training loss per 100 training steps: 0.004119683872369448\n",
      "Training Opt_acc epoch: 0.984102895217004\n",
      "Training loss per 100 training steps: 0.00412188670375903\n",
      "Training Opt_acc epoch: 0.9841162102577345\n",
      "Training loss per 100 training steps: 0.004133219756513237\n",
      "Training Opt_acc epoch: 0.984049149770962\n",
      "Training loss per 100 training steps: 0.004149742538052369\n",
      "Training Opt_acc epoch: 0.9841069046246284\n",
      "Training loss per 100 training steps: 0.004134951818915689\n",
      "Training Opt_acc epoch: 0.9841561988547922\n",
      "Training loss per 100 training steps: 0.004162533815321294\n",
      "Training Opt_acc epoch: 0.9841712482522271\n",
      "Training loss per 100 training steps: 0.004135754505052774\n",
      "Training Opt_acc epoch: 0.9842646416168817\n",
      "Training loss per 100 training steps: 0.004128367140127089\n",
      "Training Opt_acc epoch: 0.9842687328646124\n",
      "Training loss per 100 training steps: 0.0040988963749246\n",
      "Training Opt_acc epoch: 0.9843944567144262\n",
      "Training loss per 100 training steps: 0.004074682844052481\n",
      "Training Opt_acc epoch: 0.9845052134341212\n",
      "Training loss per 100 training steps: 0.0040624339800387\n",
      "Training Opt_acc epoch: 0.9845882400894862\n",
      "Training loss per 100 training steps: 0.00407943695645076\n",
      "Training Opt_acc epoch: 0.9845290653978889\n",
      "Training loss per 100 training steps: 0.0040493596290684985\n",
      "Training Opt_acc epoch: 0.9846647716263826\n",
      "Training loss per 100 training steps: 0.004036774457933949\n",
      "Training Opt_acc epoch: 0.9846959092473174\n",
      "Training loss per 100 training steps: 0.004040804945310606\n",
      "Training Opt_acc epoch: 0.984711189174167\n",
      "Training loss per 100 training steps: 0.004060154040511338\n",
      "Training Opt_acc epoch: 0.984653976137219\n",
      "Training loss per 100 training steps: 0.004050126531686657\n",
      "Training Opt_acc epoch: 0.9846802000532968\n",
      "Training loss per 100 training steps: 0.004089831564732548\n",
      "Training Opt_acc epoch: 0.9846429524649013\n",
      "Training loss per 100 training steps: 0.0041020341296009975\n",
      "Training Opt_acc epoch: 0.9846108727365162\n",
      "Training loss per 100 training steps: 0.004090511957664938\n",
      "Training Opt_acc epoch: 0.984569922862148\n",
      "Training loss per 100 training steps: 0.0040828923787748946\n",
      "Training Opt_acc epoch: 0.9845044934985065\n",
      "Training loss per 100 training steps: 0.004068674187325508\n",
      "Training Opt_acc epoch: 0.984529849849113\n",
      "Training loss per 100 training steps: 0.0040518789338427254\n",
      "Training Opt_acc epoch: 0.9845746540042547\n",
      "Training loss per 100 training steps: 0.004064715679045968\n",
      "Training Opt_acc epoch: 0.9845720295677183\n",
      "Training loss per 100 training steps: 0.004058238727595078\n",
      "Training Opt_acc epoch: 0.9845545973016926\n",
      "Training loss per 100 training steps: 0.004059070360795742\n",
      "Training Opt_acc epoch: 0.9845831128625214\n",
      "Training loss per 100 training steps: 0.004032195082559376\n",
      "Training Opt_acc epoch: 0.9846978789352977\n",
      "Training loss per 100 training steps: 0.00401413507619354\n",
      "Training Opt_acc epoch: 0.9847328159092164\n",
      "Training loss per 100 training steps: 0.003998064224946025\n",
      "Training Opt_acc epoch: 0.9848049853134084\n",
      "Training loss per 100 training steps: 0.003970846285455704\n",
      "Training Opt_acc epoch: 0.9849143556070685\n",
      "Training loss per 100 training steps: 0.0039680539365464776\n",
      "Training Opt_acc epoch: 0.9849071996470934\n",
      "Training loss per 100 training steps: 0.003968655883030536\n",
      "Training Opt_acc epoch: 0.9849168259642753\n",
      "Training loss per 100 training steps: 0.003953974363064959\n",
      "Training Opt_acc epoch: 0.9849602283236486\n",
      "Training loss per 100 training steps: 0.003948776944544445\n",
      "Training Opt_acc epoch: 0.9849435390857034\n",
      "Training loss per 100 training steps: 0.003958875581987527\n",
      "Training Opt_acc epoch: 0.9849109747094302\n",
      "Training loss per 100 training steps: 0.003942480396216399\n",
      "Training Opt_acc epoch: 0.9849576945610191\n",
      "Training loss per 100 training steps: 0.003952460242136298\n",
      "Training Opt_acc epoch: 0.9849176869701538\n",
      "Training loss per 100 training steps: 0.0039618911672233545\n",
      "Training Opt_acc epoch: 0.9849029228849947\n",
      "Training loss per 100 training steps: 0.003952484918630311\n",
      "Training Opt_acc epoch: 0.9849549166191338\n",
      "Training loss per 100 training steps: 0.003964970866891678\n",
      "Training Opt_acc epoch: 0.9849474135102666\n",
      "Training loss epoch: 0.00396324084719418\n",
      "Training accuracy epoch: 0.9988030750789346\n",
      "Training Opt_acc epoch: 0.9849503090864429\n",
      "tr_zero_steps: 802\n",
      "tr_opt_steps: 11448\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268f64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0aeaf796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy, eval_precision, eval_f1, eval_recall, eval_opt_acc, eval_opt_acc_n = 0, 0, 0, 0, 0, 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    opt_zero_steps, opt_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels) 遮罩 不會取無用\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,) 取最大機率的標記 num_label就消失 每批有效字元標記\n",
    "\n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1]).ravel()\n",
    "            tmp_eval_opt_acc = tp/(fn+fp+tp)\n",
    "            if fp !=0 or fn !=0:\n",
    "                for batch_id in range(len(ids.cpu().numpy())):\n",
    "                    print('\\n')\n",
    "                    sent = []\n",
    "                    for word_id in ids.cpu().numpy()[batch_id]:\n",
    "                        sent.append(re_vocab[word_id])\n",
    "                    print(''.join(sent))\n",
    "            \n",
    "            if np.isnan(tmp_eval_opt_acc) == True:\n",
    "                tmp_eval_opt_acc = 1\n",
    "                opt_zero_steps += 1\n",
    "#                 print('opt_zero_steps:',opt_zero_steps)\n",
    "            else:\n",
    "                opt_steps += 1\n",
    "                eval_opt_acc_n += tmp_eval_opt_acc\n",
    "                \n",
    "                if idx % 100==0:\n",
    "                    loss_step = eval_loss/nb_eval_steps\n",
    "                    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "                    opt_acc_step = eval_opt_acc_n / opt_steps\n",
    "                    print(f\"Validation Opt_acc epoch: {opt_acc_step}\")\n",
    "                \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    print(eval_opt_acc_n)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    eval_opt_acc_n = eval_opt_acc_n / opt_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation Opt_Acc_n: {eval_opt_acc_n}\")\n",
    "    print('zero_steps:',opt_zero_steps)\n",
    "    print('opt_steps:',opt_steps)\n",
    "    \n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27aefe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0014195917906276085\n",
      "Validation Opt_acc epoch: 0.9934183463595228\n",
      "Validation loss per 100 evaluation steps: 0.0016285872610224395\n",
      "Validation Opt_acc epoch: 0.9935136292279149\n",
      "Validation loss per 100 evaluation steps: 0.0027470051384489076\n",
      "Validation Opt_acc epoch: 0.9894670681875763\n",
      "Validation loss per 100 evaluation steps: 0.002515969748668695\n",
      "Validation Opt_acc epoch: 0.9905349088174573\n",
      "Validation loss per 100 evaluation steps: 0.0026360191157017107\n",
      "Validation Opt_acc epoch: 0.988259368327214\n",
      "Validation loss per 100 evaluation steps: 0.0026338864796612003\n",
      "Validation Opt_acc epoch: 0.986960677648937\n",
      "Validation loss per 100 evaluation steps: 0.002429869389852381\n",
      "Validation Opt_acc epoch: 0.9872077944615569\n",
      "Validation loss per 100 evaluation steps: 0.0023760501440838124\n",
      "Validation Opt_acc epoch: 0.9873075052711299\n",
      "Validation loss per 100 evaluation steps: 0.002270761346975466\n",
      "Validation Opt_acc epoch: 0.9881409112803554\n",
      "Validation loss per 100 evaluation steps: 0.0023948294806235442\n",
      "Validation Opt_acc epoch: 0.9882081625429514\n",
      "Validation loss per 100 evaluation steps: 0.0024044471571183644\n",
      "Validation Opt_acc epoch: 0.9883919116992702\n",
      "Validation loss per 100 evaluation steps: 0.002352703018194742\n",
      "Validation Opt_acc epoch: 0.988607362962726\n",
      "Validation loss per 100 evaluation steps: 0.002371371752660689\n",
      "Validation Opt_acc epoch: 0.9888855593950456\n",
      "Validation loss per 100 evaluation steps: 0.00239781297277028\n",
      "Validation Opt_acc epoch: 0.9888378893313106\n",
      "Validation loss per 100 evaluation steps: 0.0023735894550053734\n",
      "Validation Opt_acc epoch: 0.9884596862467677\n",
      "Validation loss per 100 evaluation steps: 0.002637689046876822\n",
      "Validation Opt_acc epoch: 0.9881556503276258\n",
      "Validation loss per 100 evaluation steps: 0.0025308130928206095\n",
      "Validation Opt_acc epoch: 0.9874846181681441\n",
      "Validation loss per 100 evaluation steps: 0.0024578486229402193\n",
      "Validation Opt_acc epoch: 0.9878320531974061\n",
      "Validation loss per 100 evaluation steps: 0.0023379806532227774\n",
      "Validation Opt_acc epoch: 0.9876497246520541\n",
      "Validation loss per 100 evaluation steps: 0.0026751622370660903\n",
      "Validation Opt_acc epoch: 0.986273764873389\n",
      "Validation loss per 100 evaluation steps: 0.0026227515822711\n",
      "Validation Opt_acc epoch: 0.98672189997132\n",
      "Validation loss per 100 evaluation steps: 0.002656766140738716\n",
      "Validation Opt_acc epoch: 0.9862672832924678\n",
      "Validation loss per 100 evaluation steps: 0.002601087584823763\n",
      "Validation Opt_acc epoch: 0.9865148828461217\n",
      "Validation loss per 100 evaluation steps: 0.002504701018431105\n",
      "Validation Opt_acc epoch: 0.9862711814392029\n",
      "Validation loss per 100 evaluation steps: 0.0024944778922369334\n",
      "Validation Opt_acc epoch: 0.9864047023413707\n",
      "Validation loss per 100 evaluation steps: 0.0025300915166608464\n",
      "Validation Opt_acc epoch: 0.9866049835375685\n",
      "Validation loss per 100 evaluation steps: 0.002516321849397758\n",
      "Validation Opt_acc epoch: 0.9864236568154406\n",
      "Validation loss per 100 evaluation steps: 0.002395613561021953\n",
      "Validation Opt_acc epoch: 0.9871564211673284\n",
      "Validation loss per 100 evaluation steps: 0.002410503493460914\n",
      "Validation Opt_acc epoch: 0.987140991865318\n",
      "Validation loss per 100 evaluation steps: 0.0023653185089126233\n",
      "Validation Opt_acc epoch: 0.9875641575955448\n",
      "Validation loss per 100 evaluation steps: 0.002333974917621443\n",
      "Validation Opt_acc epoch: 0.9877304968888528\n",
      "Validation loss per 100 evaluation steps: 0.002329150739669542\n",
      "Validation Opt_acc epoch: 0.9877923814180198\n",
      "Validation loss per 100 evaluation steps: 0.0022989670064842383\n",
      "Validation Opt_acc epoch: 0.987984827517022\n",
      "Validation loss per 100 evaluation steps: 0.0024043172829192254\n",
      "Validation Opt_acc epoch: 0.9879745344504233\n",
      "Validation loss per 100 evaluation steps: 0.0024643243268294385\n",
      "Validation Opt_acc epoch: 0.9873827155502294\n",
      "Validation loss per 100 evaluation steps: 0.0024318415252157417\n",
      "Validation Opt_acc epoch: 0.9875272276710066\n",
      "Validation loss per 100 evaluation steps: 0.0023988137946592126\n",
      "Validation Opt_acc epoch: 0.9878127348902516\n",
      "Validation loss per 100 evaluation steps: 0.002394405693012318\n",
      "Validation Opt_acc epoch: 0.9879419401888961\n",
      "Validation loss per 100 evaluation steps: 0.0023941058931450155\n",
      "Validation Opt_acc epoch: 0.9877111777369824\n",
      "Validation loss per 100 evaluation steps: 0.0023838912720609553\n",
      "Validation Opt_acc epoch: 0.987783979024248\n",
      "Validation loss per 100 evaluation steps: 0.0024787757800250236\n",
      "Validation Opt_acc epoch: 0.9869915413190301\n",
      "Validation loss per 100 evaluation steps: 0.00275154181808805\n",
      "Validation Opt_acc epoch: 0.9867536665179572\n",
      "Validation loss per 100 evaluation steps: 0.0027651858193912375\n",
      "Validation Opt_acc epoch: 0.9866158261536433\n",
      "Validation loss per 100 evaluation steps: 0.002755022653575432\n",
      "Validation Opt_acc epoch: 0.9866518667522998\n",
      "Validation loss per 100 evaluation steps: 0.002931075842218439\n",
      "Validation Opt_acc epoch: 0.9865472553998139\n",
      "Validation loss per 100 evaluation steps: 0.0029166602834700367\n",
      "Validation Opt_acc epoch: 0.986625236426802\n",
      "Validation loss per 100 evaluation steps: 0.0029450062698732557\n",
      "Validation Opt_acc epoch: 0.9863814955052577\n",
      "Validation loss per 100 evaluation steps: 0.0029411919652739017\n",
      "Validation Opt_acc epoch: 0.98629042532718\n",
      "Validation loss per 100 evaluation steps: 0.0029361170816505563\n",
      "Validation Opt_acc epoch: 0.9863301287755565\n",
      "Validation loss per 100 evaluation steps: 0.002932908666800856\n",
      "Validation Opt_acc epoch: 0.9861897233850064\n",
      "Validation loss per 100 evaluation steps: 0.0029247112339308174\n",
      "Validation Opt_acc epoch: 0.98623795187807\n",
      "Validation loss per 100 evaluation steps: 0.0029422431084539834\n",
      "Validation Opt_acc epoch: 0.9860808519095382\n",
      "Validation loss per 100 evaluation steps: 0.002931184327167225\n",
      "Validation Opt_acc epoch: 0.985802306485719\n",
      "Validation loss per 100 evaluation steps: 0.0029686583796943032\n",
      "Validation Opt_acc epoch: 0.9857291783130276\n",
      "Validation loss per 100 evaluation steps: 0.0029593007782124712\n",
      "Validation Opt_acc epoch: 0.9858220349304515\n",
      "Validation loss per 100 evaluation steps: 0.0029397142499099694\n",
      "Validation Opt_acc epoch: 0.9857663026072377\n",
      "Validation loss per 100 evaluation steps: 0.0030396615867861235\n",
      "Validation Opt_acc epoch: 0.985767986219922\n",
      "Validation loss per 100 evaluation steps: 0.0030219563506701064\n",
      "Validation Opt_acc epoch: 0.9858161141036101\n",
      "Validation loss per 100 evaluation steps: 0.003023640108639559\n",
      "Validation Opt_acc epoch: 0.9858659786333793\n",
      "Validation loss per 100 evaluation steps: 0.0030063717462894654\n",
      "Validation Opt_acc epoch: 0.9859211597729232\n",
      "Validation loss per 100 evaluation steps: 0.003007206982453258\n",
      "Validation Opt_acc epoch: 0.9859609605485974\n",
      "Validation loss per 100 evaluation steps: 0.003010430302459006\n",
      "Validation Opt_acc epoch: 0.9860022375909352\n",
      "Validation loss per 100 evaluation steps: 0.0029965667448859775\n",
      "Validation Opt_acc epoch: 0.9861413420447648\n",
      "Validation loss per 100 evaluation steps: 0.002977807073387143\n",
      "Validation Opt_acc epoch: 0.9863017049542453\n",
      "Validation loss per 100 evaluation steps: 0.0029704029708248554\n",
      "Validation Opt_acc epoch: 0.9863688618197789\n",
      "Validation loss per 100 evaluation steps: 0.002988624777217225\n",
      "Validation Opt_acc epoch: 0.9862773299484247\n",
      "Validation loss per 100 evaluation steps: 0.0030802809606619666\n",
      "Validation Opt_acc epoch: 0.9861229485821003\n",
      "Validation loss per 100 evaluation steps: 0.0031688968229975855\n",
      "Validation Opt_acc epoch: 0.9857363186268768\n",
      "Validation loss per 100 evaluation steps: 0.003186805711904627\n",
      "Validation Opt_acc epoch: 0.9857552093168663\n",
      "Validation loss per 100 evaluation steps: 0.003148120947546259\n",
      "Validation Opt_acc epoch: 0.9859381844107422\n",
      "Validation loss per 100 evaluation steps: 0.0031620524826290537\n",
      "Validation Opt_acc epoch: 0.9859711100227246\n",
      "Validation loss per 100 evaluation steps: 0.0031754660104117136\n",
      "Validation Opt_acc epoch: 0.9860241415766798\n",
      "Validation loss per 100 evaluation steps: 0.003160750616367806\n",
      "Validation Opt_acc epoch: 0.9859853808899323\n",
      "Validation loss per 100 evaluation steps: 0.003157542925808963\n",
      "Validation Opt_acc epoch: 0.9859196899886574\n",
      "Validation loss per 100 evaluation steps: 0.003182389697337381\n",
      "Validation Opt_acc epoch: 0.9855244524615803\n",
      "Validation loss per 100 evaluation steps: 0.003178689434581931\n",
      "Validation Opt_acc epoch: 0.9855067910262919\n",
      "Validation loss per 100 evaluation steps: 0.0031643019549381897\n",
      "Validation Opt_acc epoch: 0.9855931524773895\n",
      "Validation loss per 100 evaluation steps: 0.0031599877211340565\n",
      "Validation Opt_acc epoch: 0.9854845661950423\n",
      "Validation loss per 100 evaluation steps: 0.0031289902795694938\n",
      "Validation Opt_acc epoch: 0.9853571792996227\n",
      "Validation loss per 100 evaluation steps: 0.0031547822036662376\n",
      "Validation Opt_acc epoch: 0.985335942008072\n",
      "Validation loss per 100 evaluation steps: 0.0031902098449953075\n",
      "Validation Opt_acc epoch: 0.9852418109561444\n",
      "Validation loss per 100 evaluation steps: 0.003193060683202171\n",
      "Validation Opt_acc epoch: 0.985280227420286\n",
      "Validation loss per 100 evaluation steps: 0.0031757755900680296\n",
      "Validation Opt_acc epoch: 0.9853615228385435\n",
      "Validation loss per 100 evaluation steps: 0.0031488226811438465\n",
      "Validation Opt_acc epoch: 0.9853933139973374\n",
      "Validation loss per 100 evaluation steps: 0.003115954297295258\n",
      "Validation Opt_acc epoch: 0.9854779544285492\n",
      "Validation loss per 100 evaluation steps: 0.0031191171249129795\n",
      "Validation Opt_acc epoch: 0.9853049046652688\n",
      "Validation loss per 100 evaluation steps: 0.0031040871436131077\n",
      "Validation Opt_acc epoch: 0.9853661892717204\n",
      "Validation loss per 100 evaluation steps: 0.0031219611151497998\n",
      "Validation Opt_acc epoch: 0.985408505345455\n",
      "Validation loss per 100 evaluation steps: 0.0032271206503196505\n",
      "Validation Opt_acc epoch: 0.9852397282271007\n",
      "Validation loss per 100 evaluation steps: 0.0032174145290553833\n",
      "Validation Opt_acc epoch: 0.9852754597290938\n",
      "Validation loss per 100 evaluation steps: 0.0032013876540001275\n",
      "Validation Opt_acc epoch: 0.9853497852917473\n",
      "Validation loss per 100 evaluation steps: 0.0031847049156891937\n",
      "Validation Opt_acc epoch: 0.9853319917118329\n",
      "Validation loss per 100 evaluation steps: 0.0031819982387417275\n",
      "Validation Opt_acc epoch: 0.9853158500957269\n",
      "Validation loss per 100 evaluation steps: 0.0031705821134134712\n",
      "Validation Opt_acc epoch: 0.985284532007221\n",
      "Validation loss per 100 evaluation steps: 0.00317449320398057\n",
      "Validation Opt_acc epoch: 0.98533352849562\n",
      "Validation loss per 100 evaluation steps: 0.0031630632116438967\n",
      "Validation Opt_acc epoch: 0.9852477071541869\n",
      "Validation loss per 100 evaluation steps: 0.0031743977752051286\n",
      "Validation Opt_acc epoch: 0.9851958152286209\n",
      "Validation loss per 100 evaluation steps: 0.0032174566891632695\n",
      "Validation Opt_acc epoch: 0.9851487166990724\n",
      "Validation loss per 100 evaluation steps: 0.0032431633100095942\n",
      "Validation Opt_acc epoch: 0.9851514609669749\n",
      "Validation loss per 100 evaluation steps: 0.003248774125048461\n",
      "Validation Opt_acc epoch: 0.9851604681829905\n",
      "Validation loss per 100 evaluation steps: 0.0032898358245859556\n",
      "Validation Opt_acc epoch: 0.9851854263275402\n",
      "Validation loss per 100 evaluation steps: 0.0032872539598585186\n",
      "Validation Opt_acc epoch: 0.9851982371060958\n",
      "Validation loss per 100 evaluation steps: 0.0032668908239293615\n",
      "Validation Opt_acc epoch: 0.9852286357548372\n",
      "Validation loss per 100 evaluation steps: 0.003255731356960205\n",
      "Validation Opt_acc epoch: 0.9852358649934306\n",
      "Validation loss per 100 evaluation steps: 0.003238456229708286\n",
      "Validation Opt_acc epoch: 0.9852404561311866\n",
      "Validation loss per 100 evaluation steps: 0.0032465042465397273\n",
      "Validation Opt_acc epoch: 0.985179238554492\n",
      "10324.124099795845\n",
      "Validation Loss: 0.0032561921405702384\n",
      "Validation Accuracy: 0.999110709810463\n",
      "Validation Opt_Acc_n: 0.9851263454003669\n",
      "zero_steps: 10520\n",
      "opt_steps: 10480\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30826d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dfd9ca1",
   "metadata": {},
   "source": [
    "## 讀檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a8d1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32768, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "model = BertForTokenClassification.from_pretrained('cl-tohoku/bert-large-japanese', num_labels=len(labels_to_ids))\n",
    "# model.to(device)\n",
    "model.load_state_dict(torch.load('model_Tari/pytorch_model.bin'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c52bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4535dd72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf7434",
   "metadata": {},
   "source": [
    "* 召使い役て現れ、物語を引っかき回したり、インナモラーティの恋の仲立ちをしたりする。(標記對，但沒有符合到正規式所要求的字 應該只要'回したり')\n",
    "* かく乱したり凝縮核を投入したりすると、急に滴を生して飽和蒸気となる。(mecab斷錯，正規式判別不出來，但神經網路還可以 '乱したり')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c27b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['鑑', '賞', 'と', 'は', '音', '楽', 'を', '聴', 'い', 'て', 'そ', 'れ', 'を', '味', 'わ', 'っ', 'た', 'り', '、', '価', '値', 'を', '見', '極', 'め', 'た', 'り', 'す', 'る', 'こ', 'と', 'で', 'あ', 'る']\n",
      "['Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"鑑賞とは音楽を聴いてそれを味わったり、価値を見極めたりすることである\"\n",
    "\n",
    "inputs = tokenizer([word for word in sentence],\n",
    "                    is_pretokenized=True, \n",
    "                    return_offsets_mapping=True, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=MAX_LEN,\n",
    "                    return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "    #only predictions on first word pieces are important\n",
    "    if mapping[0] == 0 and mapping[1] != 0:\n",
    "        prediction.append(token_pred[1])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(sentence.split())\n",
    "print([word for word in sentence])\n",
    "# print(inputs['input_ids'])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "284b5fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>鑑</td>\n",
       "      <td>賞</td>\n",
       "      <td>と</td>\n",
       "      <td>は</td>\n",
       "      <td>音</td>\n",
       "      <td>楽</td>\n",
       "      <td>を</td>\n",
       "      <td>聴</td>\n",
       "      <td>い</td>\n",
       "      <td>て</td>\n",
       "      <td>そ</td>\n",
       "      <td>れ</td>\n",
       "      <td>を</td>\n",
       "      <td>味</td>\n",
       "      <td>わ</td>\n",
       "      <td>っ</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>、</td>\n",
       "      <td>価</td>\n",
       "      <td>値</td>\n",
       "      <td>を</td>\n",
       "      <td>見</td>\n",
       "      <td>極</td>\n",
       "      <td>め</td>\n",
       "      <td>た</td>\n",
       "      <td>り</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>こ</td>\n",
       "      <td>と</td>\n",
       "      <td>で</td>\n",
       "      <td>あ</td>\n",
       "      <td>る</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22  \\\n",
       "Tokens  鑑  賞  と  は  音  楽  を  聴  い  て  そ  れ  を  味  わ  っ  た  り  、  価  値  を  見   \n",
       "Tags    Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ   \n",
       "\n",
       "       23 24 25 26 27 28 29 30 31 32 33  \n",
       "Tokens  極  め  た  り  す  る  こ  と  で  あ  る  \n",
       "Tags    Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame([[word for word in sentence], prediction], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "200a64c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 5467, 5055,  890,  897, 5657, 3001,  932, 4345,  854,  888,  879,\n",
       "          926,  932, 1595,  929,  885,  881,  924,  828, 1183, 1225,  932, 4847,\n",
       "         2997,  915,  881,  924,  875,  925,  869,  890,  888,  852,  925,    3,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'offset_mapping': tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4b9ec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n",
      "This tutorial is completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"./model_Tari\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# save vocabulary of the tokenizer\n",
    "tokenizer.save_vocabulary(directory)\n",
    "# save the model weights and its configuration file\n",
    "model.save_pretrained(directory)\n",
    "print('All files saved')\n",
    "print('This tutorial is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12825ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62431818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa9812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c14f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
