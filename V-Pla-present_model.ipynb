{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d716a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fbe8acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc1b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_labels = {0: 'Ｘ', 1: 'Ｏ', -100:'IGN'}\n",
    "labels_to_ids = {'Ｘ': 0, 'Ｏ': 1, 'IGN':-100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98a7dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ｘ', 1: 'Ｏ', -100: 'IGN'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802b6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ｘ': 0, 'Ｏ': 1, 'IGN': -100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c998a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizerFast.from_pretrained('cl-tohoku/bert-large-japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55036e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "re_vocab = {vocab[word]:word for word in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96dca5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_V_tag_all_ori.txt', 'r') as filein:\n",
    "    wiki_tag = filein.readlines()\n",
    "filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a8345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tag_all = [''.join(wiki_tag[i].strip()) for i in range(len(wiki_tag))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee97a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸＸＸＯＯＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸＸＸＸＸＸＯＯＯＸＸＸＸＸＸＸＯＯＸ',\n",
       " 'ＸＸＸＸＸＸＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ',\n",
       " 'ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＸＸＸＸＸＸＸＸＸＸＸＸ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tag_all[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70a212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "jp_train_tag, jp_test_tag = train_test_split(wiki_tag_all, random_state=55, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5a4dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jp_train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187c8e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35015\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_train_tag:\n",
    "    if 'Ｏ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dd55de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jp_test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7e927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14999\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in jp_test_tag:\n",
    "    if 'Ｏ' in word:\n",
    "        cnt = cnt+1\n",
    "print(cnt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34194e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_V_pattern_10w_all.txt', 'r') as filein:\n",
    "    wiki_sent = filein.readlines()\n",
    "filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f68026",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sent_all = [wiki_sent[i].strip() for i in range(len(wiki_sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbaf470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_train_texts, jp_test_texts = train_test_split(wiki_sent_all, random_state=55, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac73369c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['アンパサンド（&,ampersand）は、並立助詞「…と…」を意味する記号である。',\n",
       " 'ラテン語で「…と…」を表す接続詞”et”の合字を起源とする。',\n",
       " '現代のフォントでも、TrebuchetMSなど一部のフォントでは、”et”の合字であることが容易にわかる字形を使用している。',\n",
       " '英語で教育を行う学校でアルファベットを復唱する場合、その文字自体が単語となる文字（”A”,”I”,かつては”O”も）については、伝統的にラテン語のperse（それ自体）を用いて”AperseA”のように唱えられていた。',\n",
       " 'また、アルファベットの最後に、27番目の文字のように”&”を加えることも広く行われていた。']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sent_all[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54045742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "また、長時間の入浴は体力を消耗するので、短時間にとどめるべきである。\n"
     ]
    }
   ],
   "source": [
    "for sent in jp_test_texts:\n",
    "    if '長時間の入浴' in sent:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c896892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for num in range(len(wiki_sent_all)):\n",
    "    text.append(len(wiki_sent_all[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc81eb54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sent =[wiki_sent_all[1][word] for word in range(len(wiki_sent_all[1]))]\n",
    "data_label =[wiki_tag_all[1][word] for word in range(len(wiki_tag_all[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f4ca6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>ラ</td>\n",
       "      <td>テ</td>\n",
       "      <td>ン</td>\n",
       "      <td>語</td>\n",
       "      <td>で</td>\n",
       "      <td>「</td>\n",
       "      <td>…</td>\n",
       "      <td>と</td>\n",
       "      <td>…</td>\n",
       "      <td>」</td>\n",
       "      <td>を</td>\n",
       "      <td>表</td>\n",
       "      <td>す</td>\n",
       "      <td>接</td>\n",
       "      <td>続</td>\n",
       "      <td>詞</td>\n",
       "      <td>”</td>\n",
       "      <td>e</td>\n",
       "      <td>t</td>\n",
       "      <td>”</td>\n",
       "      <td>の</td>\n",
       "      <td>合</td>\n",
       "      <td>字</td>\n",
       "      <td>を</td>\n",
       "      <td>起</td>\n",
       "      <td>源</td>\n",
       "      <td>と</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23  \\\n",
       "sent   ラ  テ  ン  語  で  「  …  と  …  」  を  表  す  接  続  詞  ”  e  t  ”  の  合  字  を   \n",
       "label  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ   \n",
       "\n",
       "      24 25 26 27 28 29  \n",
       "sent   起  源  と  す  る  。  \n",
       "label  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｘ  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame([data_sent, data_label], index=[\"sent\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60febc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "#         sentence = self.data.sentence[index].strip().split()\n",
    "        sentence = [i for i in self.data.sentence[index]]\n",
    "#         word_tag_n = [''.join(label) for label in word_labels[index]]\n",
    "#         word_labels = self.data.word_labels[index].split() \n",
    "    \n",
    "        word_labels = [j for j in self.data.word_labels[index]]    \n",
    "\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_pretokenized=True, \n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "\n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83187d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame([jp_train_texts, jp_train_tag], index=[\"sentence\", \"word_labels\"]).T\n",
    "test_dataset = pd.DataFrame([jp_test_texts, jp_test_tag], index=[\"sentence\", \"word_labels\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e2fb861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*キャッシールース・ボール当時の大阪アメリカンセンター館長の娘。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>**インドのアッサム地方でイギリス人・ロバート・ブルースが野生茶樹を発見。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>また永野も湖川らの設定画に直接解説文や注釈を書き込んだりしている。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>**8月1日7月1日にイー・アクセスがワイモバイル株式会社に社名を変更した上で、同日ウィルコ...</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:透視能力。</td>\n",
       "      <td>ＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>本人曰く「あまり萌えないので男の子を描くのは比較的苦手」とのこと。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＯＯＯＯＸＸＸＸＸＸＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>また、阿蘇の五岳を見ながらの野外結婚式も行われている。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>かつてシャープがユニフォームスポンサーを務めた。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>柔道漫画『帯をギュッとね!』（週刊少年サンデー）が初の連載作品となる。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>音階も西洋音楽のものとは異なる特徴的な音を含む。</td>\n",
       "      <td>ＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＸＸＸＸＸＸＯＯＸ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0                       *キャッシールース・ボール当時の大阪アメリカンセンター館長の娘。   \n",
       "1                  **インドのアッサム地方でイギリス人・ロバート・ブルースが野生茶樹を発見。   \n",
       "2                      また永野も湖川らの設定画に直接解説文や注釈を書き込んだりしている。   \n",
       "3      **8月1日7月1日にイー・アクセスがワイモバイル株式会社に社名を変更した上で、同日ウィルコ...   \n",
       "4                                                 :透視能力。   \n",
       "...                                                  ...   \n",
       "69995                  本人曰く「あまり萌えないので男の子を描くのは比較的苦手」とのこと。   \n",
       "69996                        また、阿蘇の五岳を見ながらの野外結婚式も行われている。   \n",
       "69997                           かつてシャープがユニフォームスポンサーを務めた。   \n",
       "69998                柔道漫画『帯をギュッとね!』（週刊少年サンデー）が初の連載作品となる。   \n",
       "69999                           音階も西洋音楽のものとは異なる特徴的な音を含む。   \n",
       "\n",
       "                                             word_labels  \n",
       "0                       ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "1                  ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "2                      ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ  \n",
       "3      ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ...  \n",
       "4                                                 ＸＸＸＸＸＸ  \n",
       "...                                                  ...  \n",
       "69995                  ＸＸＸＸＸＸＸＸＯＯＯＯＸＸＸＸＸＸＯＯＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "69996                        ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ  \n",
       "69997                           ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸ  \n",
       "69998                ＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＸＯＯＸ  \n",
       "69999                           ＸＸＸＸＸＸＸＸＸＸＸＸＯＯＯＸＸＸＸＸＸＯＯＸ  \n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d907224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "動詞普通型 TRAIN Dataset: (70000, 2)\n",
      "動詞普通型 TEST Dataset: (30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"動詞普通型 TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"動詞普通型 TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90299177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   2,   24,  951, 1005,  973,  961, 1026, 1013, 1026,  963, 1025,  997,\n",
       "         1026, 1013, 2266, 2754,  896, 1846, 5518,  940, 1003, 1012,  949, 1021,\n",
       "          965, 1021,  969, 1026, 5729, 5479,  896, 1916,  829,    3,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'offset_mapping': tensor([[0, 0],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]]),\n",
       " 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33f4986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "*           0\n",
      "キ           0\n",
      "ャ           0\n",
      "ッ           0\n",
      "シ           0\n",
      "ー           0\n",
      "ル           0\n",
      "ー           0\n",
      "ス           0\n",
      "・           0\n",
      "ホ           0\n",
      "ー           0\n",
      "ル           0\n",
      "当           0\n",
      "時           0\n",
      "の           0\n",
      "大           0\n",
      "阪           0\n",
      "ア           0\n",
      "メ           0\n",
      "リ           0\n",
      "カ           0\n",
      "ン           0\n",
      "セ           0\n",
      "ン           0\n",
      "タ           0\n",
      "ー           0\n",
      "館           0\n",
      "長           0\n",
      "の           0\n",
      "娘           0\n",
      "。           0\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
    "    print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "210df19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "345d074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32768, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('cl-tohoku/bert-large-japanese', num_labels=len(labels_to_ids))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88166004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9495, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = training_set[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46422f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e36a05f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1e-05\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "896f8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model,\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy, tr_opt_acc = 0, 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_opt_zero_steps, tr_opt_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader): # 一次取一批\n",
    "#         print(idx)\n",
    "#         if idx == 193:\n",
    "#             print(batch['input_ids'])\n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels) ## 輸入 算出損失值\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,) ## 算每一批預測值\n",
    "\n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1]).ravel()\n",
    "        tmp_tr_opt_acc = tp/(fn+fp+tp)\n",
    "        if np.isnan(tmp_tr_opt_acc) == True:\n",
    "            tmp_tr_opt_acc = 1\n",
    "            tr_opt_zero_steps += 1\n",
    "        else:\n",
    "            tr_opt_steps += 1\n",
    "            tr_opt_acc += tmp_tr_opt_acc            \n",
    "            \n",
    "            if idx % 100==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "                opt_acc_step = tr_opt_acc / tr_opt_steps\n",
    "                print(f\"Training Opt_acc epoch: {opt_acc_step}\")\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() ## T度值 清空\n",
    "        loss.backward() ## 反向回饋\"修正權重\"\n",
    "        optimizer.step() ## 透過優化器(先算損失值得反向回饋)執行修正\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    tr_opt_acc = tr_opt_acc / tr_opt_steps\n",
    "    \n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training Opt_acc epoch: {tr_opt_acc}\")\n",
    "    print('tr_zero_steps:',tr_opt_zero_steps)\n",
    "    print('tr_opt_steps:',tr_opt_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb056c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 1.0382858514785767\n",
      "Training Opt_acc epoch: 0.07017543859649122\n",
      "Training loss per 100 training steps: 0.1037967376327972\n",
      "Training Opt_acc epoch: 0.5154820762854329\n",
      "Training loss per 100 training steps: 0.06781205858001996\n",
      "Training Opt_acc epoch: 0.6683819950198618\n",
      "Training loss per 100 training steps: 0.052124230458944154\n",
      "Training Opt_acc epoch: 0.73559314910154\n",
      "Training loss per 100 training steps: 0.0445660822355639\n",
      "Training Opt_acc epoch: 0.7736348543654243\n",
      "Training loss per 100 training steps: 0.03920186405454575\n",
      "Training Opt_acc epoch: 0.7989836531615304\n",
      "Training loss per 100 training steps: 0.034711065170523275\n",
      "Training Opt_acc epoch: 0.8199304308140777\n",
      "Training loss per 100 training steps: 0.031232379639071856\n",
      "Training Opt_acc epoch: 0.838697732711748\n",
      "Training loss per 100 training steps: 0.028861406971755717\n",
      "Training Opt_acc epoch: 0.851253734920692\n",
      "Training loss per 100 training steps: 0.02642658868006625\n",
      "Training Opt_acc epoch: 0.8635625483372521\n",
      "Training loss per 100 training steps: 0.024688814406301635\n",
      "Training Opt_acc epoch: 0.8727689532047269\n",
      "Training loss per 100 training steps: 0.023403734447183473\n",
      "Training Opt_acc epoch: 0.878267911141484\n",
      "Training loss per 100 training steps: 0.022106663187319263\n",
      "Training Opt_acc epoch: 0.8838191740340575\n",
      "Training loss per 100 training steps: 0.021261537382932617\n",
      "Training Opt_acc epoch: 0.8881916837832521\n",
      "Training loss per 100 training steps: 0.020095178340847532\n",
      "Training Opt_acc epoch: 0.893698840619127\n",
      "Training loss per 100 training steps: 0.019160569378459558\n",
      "Training Opt_acc epoch: 0.8983058958437141\n",
      "Training loss per 100 training steps: 0.018448576270299607\n",
      "Training Opt_acc epoch: 0.9019440319846134\n",
      "Training loss per 100 training steps: 0.017760251704955553\n",
      "Training Opt_acc epoch: 0.9054934470956391\n",
      "Training loss per 100 training steps: 0.017185277824586644\n",
      "Training Opt_acc epoch: 0.9086652370240206\n",
      "Training loss per 100 training steps: 0.016678331411724784\n",
      "Training Opt_acc epoch: 0.9110762541104733\n",
      "Training loss per 100 training steps: 0.016268794262606374\n",
      "Training Opt_acc epoch: 0.9133754428399707\n",
      "Training loss per 100 training steps: 0.015700392870647742\n",
      "Training Opt_acc epoch: 0.916334418875616\n",
      "Training loss per 100 training steps: 0.015402969758383475\n",
      "Training Opt_acc epoch: 0.9171320174481634\n",
      "Training loss per 100 training steps: 0.015224635848422307\n",
      "Training Opt_acc epoch: 0.9186027487396208\n",
      "Training loss per 100 training steps: 0.01484211663623366\n",
      "Training Opt_acc epoch: 0.9202632447494776\n",
      "Training loss per 100 training steps: 0.014589499309686496\n",
      "Training Opt_acc epoch: 0.9214227390875666\n",
      "Training loss per 100 training steps: 0.01429977790879466\n",
      "Training Opt_acc epoch: 0.9230111940284293\n",
      "Training loss per 100 training steps: 0.01404811007615333\n",
      "Training Opt_acc epoch: 0.9243887645517239\n",
      "Training loss per 100 training steps: 0.013907422475722846\n",
      "Training Opt_acc epoch: 0.9250702785098127\n",
      "Training loss per 100 training steps: 0.013720234404875657\n",
      "Training Opt_acc epoch: 0.9262254709558198\n",
      "Training loss per 100 training steps: 0.013356558952880565\n",
      "Training Opt_acc epoch: 0.9282610091822233\n",
      "Training loss per 100 training steps: 0.013231642850474635\n",
      "Training Opt_acc epoch: 0.929141504534825\n",
      "Training loss per 100 training steps: 0.013120726990755371\n",
      "Training Opt_acc epoch: 0.9299272760128836\n",
      "Training loss per 100 training steps: 0.013010345388463655\n",
      "Training Opt_acc epoch: 0.9306985656508562\n",
      "Training loss per 100 training steps: 0.012877876484005426\n",
      "Training Opt_acc epoch: 0.9317569131075614\n",
      "Training loss per 100 training steps: 0.012748577829616081\n",
      "Training Opt_acc epoch: 0.932761006237071\n",
      "Training loss per 100 training steps: 0.0126265797556761\n",
      "Training Opt_acc epoch: 0.9333773200487201\n",
      "Training loss per 100 training steps: 0.012451118214182337\n",
      "Training Opt_acc epoch: 0.9343502427819054\n",
      "Training loss per 100 training steps: 0.012309578902468907\n",
      "Training Opt_acc epoch: 0.9352501947804421\n",
      "Training loss per 100 training steps: 0.012041537030208705\n",
      "Training Opt_acc epoch: 0.9366705382629784\n",
      "Training loss per 100 training steps: 0.01192160669804095\n",
      "Training Opt_acc epoch: 0.9371397926949833\n",
      "Training loss per 100 training steps: 0.011666752726210842\n",
      "Training Opt_acc epoch: 0.9384872012153477\n",
      "Training loss per 100 training steps: 0.011553709565298253\n",
      "Training Opt_acc epoch: 0.9390132640988965\n",
      "Training loss per 100 training steps: 0.011442599660591756\n",
      "Training Opt_acc epoch: 0.9394033421558102\n",
      "Training loss per 100 training steps: 0.011305425995947768\n",
      "Training Opt_acc epoch: 0.9402694162032698\n",
      "Training loss per 100 training steps: 0.011162222361903723\n",
      "Training Opt_acc epoch: 0.9409110418930224\n",
      "Training loss per 100 training steps: 0.011072454503997029\n",
      "Training Opt_acc epoch: 0.941387280435918\n",
      "Training loss per 100 training steps: 0.010933011589099035\n",
      "Training Opt_acc epoch: 0.9425152156655074\n",
      "Training loss per 100 training steps: 0.010879522351731797\n",
      "Training Opt_acc epoch: 0.9433094648743038\n",
      "Training loss per 100 training steps: 0.010794686377140152\n",
      "Training Opt_acc epoch: 0.9436285371778768\n",
      "Training loss per 100 training steps: 0.010903494859118703\n",
      "Training Opt_acc epoch: 0.9426517174735564\n",
      "Training loss per 100 training steps: 0.010786687978655504\n",
      "Training Opt_acc epoch: 0.9430913241230765\n",
      "Training loss per 100 training steps: 0.010708477492920851\n",
      "Training Opt_acc epoch: 0.9433148144642054\n",
      "Training loss per 100 training steps: 0.010636778418006834\n",
      "Training Opt_acc epoch: 0.9439466981205973\n",
      "Training loss per 100 training steps: 0.0104629805618808\n",
      "Training Opt_acc epoch: 0.9449351488707228\n",
      "Training loss per 100 training steps: 0.010203837134097194\n",
      "Training Opt_acc epoch: 0.9465329531648885\n",
      "Training loss per 100 training steps: 0.010150894151528443\n",
      "Training Opt_acc epoch: 0.9469062998249661\n",
      "Training loss per 100 training steps: 0.010112809193498731\n",
      "Training Opt_acc epoch: 0.9468995014261229\n",
      "Training loss per 100 training steps: 0.01000364858852556\n",
      "Training Opt_acc epoch: 0.9474224475807201\n",
      "Training loss per 100 training steps: 0.009946409662028963\n",
      "Training Opt_acc epoch: 0.9478401052482105\n",
      "Training loss per 100 training steps: 0.00986103931480863\n",
      "Training Opt_acc epoch: 0.9481429854647301\n",
      "Training loss per 100 training steps: 0.009772759923097047\n",
      "Training Opt_acc epoch: 0.9485404361631594\n",
      "Training loss per 100 training steps: 0.009702633898640785\n",
      "Training Opt_acc epoch: 0.9489514650473808\n",
      "Training loss per 100 training steps: 0.009662771828769292\n",
      "Training Opt_acc epoch: 0.949081893140677\n",
      "Training loss per 100 training steps: 0.00959121995030596\n",
      "Training Opt_acc epoch: 0.9493792723290321\n",
      "Training loss per 100 training steps: 0.009557027884158778\n",
      "Training Opt_acc epoch: 0.94964667018271\n",
      "Training loss per 100 training steps: 0.009499402726642152\n",
      "Training Opt_acc epoch: 0.9499906863283805\n",
      "Training loss per 100 training steps: 0.009417550134193611\n",
      "Training Opt_acc epoch: 0.9502951245844315\n",
      "Training loss per 100 training steps: 0.00933082630652964\n",
      "Training Opt_acc epoch: 0.9507720061002238\n",
      "Training loss per 100 training steps: 0.009312441105510268\n",
      "Training Opt_acc epoch: 0.9507956473864374\n",
      "Training loss per 100 training steps: 0.009329226856957298\n",
      "Training Opt_acc epoch: 0.9507538956432443\n",
      "Training loss per 100 training steps: 0.009299636133818089\n",
      "Training Opt_acc epoch: 0.9509260255847137\n",
      "Training loss per 100 training steps: 0.00923643153108883\n",
      "Training Opt_acc epoch: 0.9512195665245984\n",
      "Training loss per 100 training steps: 0.00918313561577151\n",
      "Training Opt_acc epoch: 0.95155256301512\n",
      "Training loss per 100 training steps: 0.00912460084706729\n",
      "Training Opt_acc epoch: 0.9518813933098529\n",
      "Training loss per 100 training steps: 0.009075647355948992\n",
      "Training Opt_acc epoch: 0.9520547722690245\n",
      "Training loss per 100 training steps: 0.009020683689759296\n",
      "Training Opt_acc epoch: 0.9522573213827972\n",
      "Training loss per 100 training steps: 0.008967382015638962\n",
      "Training Opt_acc epoch: 0.9525322894715943\n",
      "Training loss per 100 training steps: 0.008918793641752553\n",
      "Training Opt_acc epoch: 0.9528722947954243\n",
      "Training loss per 100 training steps: 0.008858725578439959\n",
      "Training Opt_acc epoch: 0.9531690400528136\n",
      "Training loss per 100 training steps: 0.008844916048339712\n",
      "Training Opt_acc epoch: 0.953210133193938\n",
      "Training loss per 100 training steps: 0.008820755655209286\n",
      "Training Opt_acc epoch: 0.9532883315786261\n",
      "Training loss per 100 training steps: 0.008778906229300899\n",
      "Training Opt_acc epoch: 0.9535018000175018\n",
      "Training loss per 100 training steps: 0.008790735493880828\n",
      "Training Opt_acc epoch: 0.9532547496993802\n",
      "Training loss per 100 training steps: 0.008756350119957488\n",
      "Training Opt_acc epoch: 0.9534841313178329\n",
      "Training loss per 100 training steps: 0.008711188119733726\n",
      "Training Opt_acc epoch: 0.9537728964528268\n",
      "Training loss per 100 training steps: 0.008693183716766636\n",
      "Training Opt_acc epoch: 0.9539155761574731\n",
      "Training loss per 100 training steps: 0.00865505355357298\n",
      "Training Opt_acc epoch: 0.9541493230365193\n",
      "Training loss per 100 training steps: 0.008606558393054319\n",
      "Training Opt_acc epoch: 0.9544266588981684\n",
      "Training loss per 100 training steps: 0.008563393766301101\n",
      "Training Opt_acc epoch: 0.9547106167065819\n",
      "Training loss per 100 training steps: 0.008519274141128639\n",
      "Training Opt_acc epoch: 0.9549176709127588\n",
      "Training loss per 100 training steps: 0.008522075649332583\n",
      "Training Opt_acc epoch: 0.9549226716626896\n",
      "Training loss per 100 training steps: 0.00851509324657973\n",
      "Training Opt_acc epoch: 0.9549815213491202\n",
      "Training loss per 100 training steps: 0.008485781844888109\n",
      "Training Opt_acc epoch: 0.9551860770447497\n",
      "Training loss per 100 training steps: 0.008460509313106115\n",
      "Training Opt_acc epoch: 0.9552351743304186\n",
      "Training loss per 100 training steps: 0.008422704571305974\n",
      "Training Opt_acc epoch: 0.9553896656234068\n",
      "Training loss per 100 training steps: 0.008409428949599174\n",
      "Training Opt_acc epoch: 0.9556041298275318\n",
      "Training loss per 100 training steps: 0.008372515691876702\n",
      "Training Opt_acc epoch: 0.9558234221540197\n",
      "Training loss per 100 training steps: 0.008278327178253882\n",
      "Training Opt_acc epoch: 0.9562055815350529\n",
      "Training loss per 100 training steps: 0.008236205440999105\n",
      "Training Opt_acc epoch: 0.9564339900365704\n",
      "Training loss per 100 training steps: 0.008183005689037967\n",
      "Training Opt_acc epoch: 0.9565809733655499\n",
      "Training loss per 100 training steps: 0.00814979761809463\n",
      "Training Opt_acc epoch: 0.9565614080584193\n",
      "Training loss per 100 training steps: 0.008122017867395406\n",
      "Training Opt_acc epoch: 0.9566669235629375\n",
      "Training loss per 100 training steps: 0.008088527299610725\n",
      "Training Opt_acc epoch: 0.9568625718758068\n",
      "Training loss per 100 training steps: 0.008072152141168954\n",
      "Training Opt_acc epoch: 0.9568983514452705\n",
      "Training loss per 100 training steps: 0.008044384115687515\n",
      "Training Opt_acc epoch: 0.9570892874611057\n",
      "Training loss per 100 training steps: 0.008047988595041423\n",
      "Training Opt_acc epoch: 0.9570426285670525\n",
      "Training loss per 100 training steps: 0.008073510023193788\n",
      "Training Opt_acc epoch: 0.957009710708411\n",
      "Training loss per 100 training steps: 0.0080374244666803\n",
      "Training Opt_acc epoch: 0.957178749549918\n",
      "Training loss per 100 training steps: 0.007994211480956318\n",
      "Training Opt_acc epoch: 0.9573229633271502\n",
      "Training loss per 100 training steps: 0.007957771494357153\n",
      "Training Opt_acc epoch: 0.9574777109522307\n",
      "Training loss per 100 training steps: 0.007913843088070699\n",
      "Training Opt_acc epoch: 0.957750204963092\n",
      "Training loss per 100 training steps: 0.007874018596508638\n",
      "Training Opt_acc epoch: 0.9579380758099958\n",
      "Training loss per 100 training steps: 0.007844548603574976\n",
      "Training Opt_acc epoch: 0.9581034935386801\n",
      "Training loss per 100 training steps: 0.007810520622699557\n",
      "Training Opt_acc epoch: 0.958287454540939\n",
      "Training loss per 100 training steps: 0.007786145025649544\n",
      "Training Opt_acc epoch: 0.9584201379174243\n",
      "Training loss per 100 training steps: 0.007775288788508548\n",
      "Training Opt_acc epoch: 0.9585005928038696\n",
      "Training loss per 100 training steps: 0.007740358976916249\n",
      "Training Opt_acc epoch: 0.9586048295185351\n",
      "Training loss per 100 training steps: 0.007703690910928472\n",
      "Training Opt_acc epoch: 0.9588587525976731\n",
      "Training loss per 100 training steps: 0.007697733543699096\n",
      "Training Opt_acc epoch: 0.9588407839519619\n",
      "Training loss per 100 training steps: 0.007664027073899586\n",
      "Training Opt_acc epoch: 0.9590597317056953\n",
      "Training loss per 100 training steps: 0.007623666004647186\n",
      "Training Opt_acc epoch: 0.9592775536921465\n",
      "Training loss per 100 training steps: 0.007644591636476805\n",
      "Training Opt_acc epoch: 0.9593102474692604\n",
      "Training loss per 100 training steps: 0.007625900568699446\n",
      "Training Opt_acc epoch: 0.9592589537111973\n",
      "Training loss per 100 training steps: 0.007584255516991824\n",
      "Training Opt_acc epoch: 0.9594715208570639\n",
      "Training loss per 100 training steps: 0.007598341873945984\n",
      "Training Opt_acc epoch: 0.9595533445908387\n",
      "Training loss per 100 training steps: 0.0076006257119549505\n",
      "Training Opt_acc epoch: 0.9594984443630001\n",
      "Training loss per 100 training steps: 0.007582081001920685\n",
      "Training Opt_acc epoch: 0.9595800426403064\n",
      "Training loss per 100 training steps: 0.007599317754133534\n",
      "Training Opt_acc epoch: 0.9594699401727587\n",
      "Training loss per 100 training steps: 0.007613255571147455\n",
      "Training Opt_acc epoch: 0.9595481504113234\n",
      "Training loss per 100 training steps: 0.0076034268926610735\n",
      "Training Opt_acc epoch: 0.9596137342930374\n",
      "Training loss per 100 training steps: 0.00759848927447957\n",
      "Training Opt_acc epoch: 0.9597652120142995\n",
      "Training loss per 100 training steps: 0.007590551744194347\n",
      "Training Opt_acc epoch: 0.959742640269096\n",
      "Training loss per 100 training steps: 0.007576370155708832\n",
      "Training Opt_acc epoch: 0.9598914401295271\n",
      "Training loss per 100 training steps: 0.007551622715717101\n",
      "Training Opt_acc epoch: 0.9600104279284736\n",
      "Training loss per 100 training steps: 0.007538488095173578\n",
      "Training Opt_acc epoch: 0.9601504367124676\n",
      "Training loss per 100 training steps: 0.0075217027927169345\n",
      "Training Opt_acc epoch: 0.9602505796920302\n",
      "Training loss per 100 training steps: 0.007495310175861419\n",
      "Training Opt_acc epoch: 0.9603786136015838\n",
      "Training loss per 100 training steps: 0.0074798395291913004\n",
      "Training Opt_acc epoch: 0.9604898372863241\n",
      "Training loss per 100 training steps: 0.007454262016526451\n",
      "Training Opt_acc epoch: 0.9606610201609427\n",
      "Training loss per 100 training steps: 0.007454309226645729\n",
      "Training Opt_acc epoch: 0.9607014673437032\n",
      "Training loss per 100 training steps: 0.0074276055571974595\n",
      "Training Opt_acc epoch: 0.9608382354596857\n",
      "Training loss per 100 training steps: 0.0073995155761604975\n",
      "Training Opt_acc epoch: 0.9609981836085135\n",
      "Training loss per 100 training steps: 0.007397130454625526\n",
      "Training Opt_acc epoch: 0.9610265113686929\n",
      "Training loss per 100 training steps: 0.007386987109560776\n",
      "Training Opt_acc epoch: 0.9611208731213088\n",
      "Training loss per 100 training steps: 0.007364729681916852\n",
      "Training Opt_acc epoch: 0.9612079306525094\n",
      "Training loss per 100 training steps: 0.0073558287645460445\n",
      "Training Opt_acc epoch: 0.961295483222025\n",
      "Training loss per 100 training steps: 0.007294398536215428\n",
      "Training Opt_acc epoch: 0.9616559875886651\n",
      "Training loss per 100 training steps: 0.007287027699839395\n",
      "Training Opt_acc epoch: 0.9617263172097592\n",
      "Training loss per 100 training steps: 0.007238883142787093\n",
      "Training Opt_acc epoch: 0.9619840783373237\n",
      "Training loss per 100 training steps: 0.007240101357983047\n",
      "Training Opt_acc epoch: 0.9619237867574753\n",
      "Training loss per 100 training steps: 0.007222141476261898\n",
      "Training Opt_acc epoch: 0.9620432090798494\n",
      "Training loss per 100 training steps: 0.0072058823260783035\n",
      "Training Opt_acc epoch: 0.9621607723263488\n",
      "Training loss per 100 training steps: 0.0071986563897934\n",
      "Training Opt_acc epoch: 0.9621611621747116\n",
      "Training loss per 100 training steps: 0.0071751848599831605\n",
      "Training Opt_acc epoch: 0.9623162956731681\n",
      "Training loss per 100 training steps: 0.007158091637223837\n",
      "Training Opt_acc epoch: 0.9623405262144772\n",
      "Training loss per 100 training steps: 0.007142384961220766\n",
      "Training Opt_acc epoch: 0.962371950559592\n",
      "Training loss per 100 training steps: 0.0071095713137453356\n",
      "Training Opt_acc epoch: 0.9624652254464375\n",
      "Training loss per 100 training steps: 0.007094710275301009\n",
      "Training Opt_acc epoch: 0.9625143092141779\n",
      "Training loss per 100 training steps: 0.007069466065555282\n",
      "Training Opt_acc epoch: 0.9626584933458934\n",
      "Training loss per 100 training steps: 0.007051929706464222\n",
      "Training Opt_acc epoch: 0.962711284835279\n",
      "Training loss epoch: 0.007057716166165509\n",
      "Training accuracy epoch: 0.997950184150255\n",
      "Training Opt_acc epoch: 0.9627125199880006\n",
      "tr_zero_steps: 1068\n",
      "tr_opt_steps: 16432\n",
      "Training epoch: 2\n",
      "Training loss per 100 training steps: 8.191131200874224e-05\n",
      "Training Opt_acc epoch: 1.0\n",
      "Training loss per 100 training steps: 0.001970302454330774\n",
      "Training Opt_acc epoch: 0.9944120115995116\n",
      "Training loss per 100 training steps: 0.004038802227173325\n",
      "Training Opt_acc epoch: 0.9832594270373985\n",
      "Training loss per 100 training steps: 0.004709249621691471\n",
      "Training Opt_acc epoch: 0.9790701411461922\n",
      "Training loss per 100 training steps: 0.005193233487899077\n",
      "Training Opt_acc epoch: 0.9740348063152314\n",
      "Training loss per 100 training steps: 0.004879246978016236\n",
      "Training Opt_acc epoch: 0.9762076911547005\n",
      "Training loss per 100 training steps: 0.00446242639025713\n",
      "Training Opt_acc epoch: 0.9781304132918847\n",
      "Training loss per 100 training steps: 0.0038618108312894334\n",
      "Training Opt_acc epoch: 0.979883819606979\n",
      "Training loss per 100 training steps: 0.004239510872752204\n",
      "Training Opt_acc epoch: 0.9799056160115613\n",
      "Training loss per 100 training steps: 0.004070898143248258\n",
      "Training Opt_acc epoch: 0.9803149368092471\n",
      "Training loss per 100 training steps: 0.003943928883225962\n",
      "Training Opt_acc epoch: 0.9805805763816857\n",
      "Training loss per 100 training steps: 0.003919347781995068\n",
      "Training Opt_acc epoch: 0.9811732441830205\n",
      "Training loss per 100 training steps: 0.004000709817314766\n",
      "Training Opt_acc epoch: 0.9806657031237385\n",
      "Training loss per 100 training steps: 0.004045549319653855\n",
      "Training Opt_acc epoch: 0.9805633066945375\n",
      "Training loss per 100 training steps: 0.0038904181297909498\n",
      "Training Opt_acc epoch: 0.9814921023008898\n",
      "Training loss per 100 training steps: 0.0037730452422778454\n",
      "Training Opt_acc epoch: 0.9821070487092445\n",
      "Training loss per 100 training steps: 0.0037770169398695574\n",
      "Training Opt_acc epoch: 0.9825432093359586\n",
      "Training loss per 100 training steps: 0.003676698063196595\n",
      "Training Opt_acc epoch: 0.9831694734167469\n",
      "Training loss per 100 training steps: 0.0038967802045336737\n",
      "Training Opt_acc epoch: 0.9830477418001559\n",
      "Training loss per 100 training steps: 0.004221820634529195\n",
      "Training Opt_acc epoch: 0.9824371066926946\n",
      "Training loss per 100 training steps: 0.004168036351758039\n",
      "Training Opt_acc epoch: 0.9821876568953991\n",
      "Training loss per 100 training steps: 0.0040909628401587786\n",
      "Training Opt_acc epoch: 0.982818858914981\n",
      "Training loss per 100 training steps: 0.003998869296354694\n",
      "Training Opt_acc epoch: 0.9833383569951002\n",
      "Training loss per 100 training steps: 0.0038750025129309037\n",
      "Training Opt_acc epoch: 0.9839426805133642\n",
      "Training loss per 100 training steps: 0.0037686414323279457\n",
      "Training Opt_acc epoch: 0.9843810658062245\n",
      "Training loss per 100 training steps: 0.0038310599915987007\n",
      "Training Opt_acc epoch: 0.9839601139890558\n",
      "Training loss per 100 training steps: 0.0038248051369573306\n",
      "Training Opt_acc epoch: 0.9834636672809756\n",
      "Training loss per 100 training steps: 0.003997645339634792\n",
      "Training Opt_acc epoch: 0.9829107891614487\n",
      "Training loss per 100 training steps: 0.004115249171650453\n",
      "Training Opt_acc epoch: 0.9820950922209095\n",
      "Training loss per 100 training steps: 0.004146658859684459\n",
      "Training Opt_acc epoch: 0.9815034118429936\n",
      "Training loss per 100 training steps: 0.004093523291524468\n",
      "Training Opt_acc epoch: 0.9814292700079292\n",
      "Training loss per 100 training steps: 0.004079002106908709\n",
      "Training Opt_acc epoch: 0.9814123793819567\n",
      "Training loss per 100 training steps: 0.003978412202783298\n",
      "Training Opt_acc epoch: 0.9818313851438281\n",
      "Training loss per 100 training steps: 0.0039570302171354045\n",
      "Training Opt_acc epoch: 0.9818786705535429\n",
      "Training loss per 100 training steps: 0.0039024514360698555\n",
      "Training Opt_acc epoch: 0.9822018279008231\n",
      "Training loss per 100 training steps: 0.003837093721379987\n",
      "Training Opt_acc epoch: 0.9823618744346025\n",
      "Training loss per 100 training steps: 0.003769719634935156\n",
      "Training Opt_acc epoch: 0.9825711994399725\n",
      "Training loss per 100 training steps: 0.003760130390519528\n",
      "Training Opt_acc epoch: 0.9823973876732509\n",
      "Training loss per 100 training steps: 0.003846900100932199\n",
      "Training Opt_acc epoch: 0.9818324683778163\n",
      "Training loss per 100 training steps: 0.0038387648003925714\n",
      "Training Opt_acc epoch: 0.9816978465800343\n",
      "Training loss per 100 training steps: 0.0038844818325981727\n",
      "Training Opt_acc epoch: 0.9814619441657636\n",
      "Training loss per 100 training steps: 0.003884406268135106\n",
      "Training Opt_acc epoch: 0.9814046749729604\n",
      "Training loss per 100 training steps: 0.0038319890942478904\n",
      "Training Opt_acc epoch: 0.981653743418013\n",
      "Training loss per 100 training steps: 0.00378583752396309\n",
      "Training Opt_acc epoch: 0.9818077087019064\n",
      "Training loss per 100 training steps: 0.0037566833315956115\n",
      "Training Opt_acc epoch: 0.9820008303136705\n",
      "Training loss per 100 training steps: 0.0037442901462999075\n",
      "Training Opt_acc epoch: 0.9820871447979922\n",
      "Training loss per 100 training steps: 0.0037095240539246274\n",
      "Training Opt_acc epoch: 0.9822611165647894\n",
      "Training loss per 100 training steps: 0.003684756053943554\n",
      "Training Opt_acc epoch: 0.9822960169144125\n",
      "Training loss per 100 training steps: 0.003648991440378436\n",
      "Training Opt_acc epoch: 0.982368264743376\n",
      "Training loss per 100 training steps: 0.003619193541301835\n",
      "Training Opt_acc epoch: 0.9825071954498245\n",
      "Training loss per 100 training steps: 0.0036202620391586606\n",
      "Training Opt_acc epoch: 0.9823116873789134\n",
      "Training loss per 100 training steps: 0.0036028742695120145\n",
      "Training Opt_acc epoch: 0.9822667299410139\n",
      "Training loss per 100 training steps: 0.0035896775507167416\n",
      "Training Opt_acc epoch: 0.9821733177483314\n",
      "Training loss per 100 training steps: 0.003543591991737987\n",
      "Training Opt_acc epoch: 0.9820988044582493\n",
      "Training loss per 100 training steps: 0.0035588515141658623\n",
      "Training Opt_acc epoch: 0.9821934570774664\n",
      "Training loss per 100 training steps: 0.00354151742272675\n",
      "Training Opt_acc epoch: 0.9822017529731101\n",
      "Training loss per 100 training steps: 0.003587862032860136\n",
      "Training Opt_acc epoch: 0.9820118553683634\n",
      "Training loss per 100 training steps: 0.0035967458912302686\n",
      "Training Opt_acc epoch: 0.9820492178724838\n",
      "Training loss per 100 training steps: 0.0035889027148587474\n",
      "Training Opt_acc epoch: 0.9821890670402298\n",
      "Training loss per 100 training steps: 0.0035519159982627126\n",
      "Training Opt_acc epoch: 0.9824101748341926\n",
      "Training loss per 100 training steps: 0.0035109296266726585\n",
      "Training Opt_acc epoch: 0.9826247423295391\n",
      "Training loss per 100 training steps: 0.0035005996026727004\n",
      "Training Opt_acc epoch: 0.9827237657107402\n",
      "Training loss per 100 training steps: 0.0034859951593451985\n",
      "Training Opt_acc epoch: 0.9828001415771214\n",
      "Training loss per 100 training steps: 0.003450484136178026\n",
      "Training Opt_acc epoch: 0.982965182721774\n",
      "Training loss per 100 training steps: 0.003415344268697527\n",
      "Training Opt_acc epoch: 0.9831603340195355\n",
      "Training loss per 100 training steps: 0.0034099317420421093\n",
      "Training Opt_acc epoch: 0.9829830993381088\n",
      "Training loss per 100 training steps: 0.003443811088548454\n",
      "Training Opt_acc epoch: 0.9829333525952688\n",
      "Training loss per 100 training steps: 0.003419094915839296\n",
      "Training Opt_acc epoch: 0.9830448997318989\n",
      "Training loss per 100 training steps: 0.003389114045848275\n",
      "Training Opt_acc epoch: 0.9830886052724974\n",
      "Training loss per 100 training steps: 0.003402513819922085\n",
      "Training Opt_acc epoch: 0.9830981874723801\n",
      "Training loss per 100 training steps: 0.003413257485387869\n",
      "Training Opt_acc epoch: 0.9830976215652216\n",
      "Training loss per 100 training steps: 0.003419168357060645\n",
      "Training Opt_acc epoch: 0.9831419581928201\n",
      "Training loss per 100 training steps: 0.003400641315732083\n",
      "Training Opt_acc epoch: 0.983211786701424\n",
      "Training loss per 100 training steps: 0.003378931098407469\n",
      "Training Opt_acc epoch: 0.9833205609805087\n",
      "Training loss per 100 training steps: 0.0034639244804897853\n",
      "Training Opt_acc epoch: 0.9831752363833681\n",
      "Training loss per 100 training steps: 0.003468067103795126\n",
      "Training Opt_acc epoch: 0.9829171020470661\n",
      "Training loss per 100 training steps: 0.0034703936746069055\n",
      "Training Opt_acc epoch: 0.9829339610590301\n",
      "Training loss per 100 training steps: 0.003453403676304217\n",
      "Training Opt_acc epoch: 0.9830463057394252\n",
      "Training loss per 100 training steps: 0.003457107075527053\n",
      "Training Opt_acc epoch: 0.9828915957417598\n",
      "Training loss per 100 training steps: 0.003406467169235582\n",
      "Training Opt_acc epoch: 0.9831392896203964\n",
      "Training loss per 100 training steps: 0.003394847114660217\n",
      "Training Opt_acc epoch: 0.9832337466031156\n",
      "Training loss per 100 training steps: 0.003403981010712943\n",
      "Training Opt_acc epoch: 0.9831936680698307\n",
      "Training loss per 100 training steps: 0.0034056760300800594\n",
      "Training Opt_acc epoch: 0.983118734949628\n",
      "Training loss per 100 training steps: 0.0034211563800851026\n",
      "Training Opt_acc epoch: 0.9829668464734964\n",
      "Training loss per 100 training steps: 0.00344360885892554\n",
      "Training Opt_acc epoch: 0.9828941639155249\n",
      "Training loss per 100 training steps: 0.0034445251249421945\n",
      "Training Opt_acc epoch: 0.9829091810843784\n",
      "Training loss per 100 training steps: 0.0034337112883820946\n",
      "Training Opt_acc epoch: 0.9828492875648641\n",
      "Training loss per 100 training steps: 0.0034063003596202556\n",
      "Training Opt_acc epoch: 0.9830148330581565\n",
      "Training loss per 100 training steps: 0.0033891412982799277\n",
      "Training Opt_acc epoch: 0.9831038169977019\n",
      "Training loss per 100 training steps: 0.003394775734937714\n",
      "Training Opt_acc epoch: 0.9830470321266909\n",
      "Training loss per 100 training steps: 0.0033888107793007137\n",
      "Training Opt_acc epoch: 0.9830785702277071\n",
      "Training loss per 100 training steps: 0.003389986304106603\n",
      "Training Opt_acc epoch: 0.9831010013520006\n",
      "Training loss per 100 training steps: 0.0033758408455668715\n",
      "Training Opt_acc epoch: 0.9831322183320758\n",
      "Training loss per 100 training steps: 0.003361950607861266\n",
      "Training Opt_acc epoch: 0.9832257446864638\n",
      "Training loss per 100 training steps: 0.0033698304364715905\n",
      "Training Opt_acc epoch: 0.9831520497619631\n",
      "Training loss per 100 training steps: 0.003361872079138213\n",
      "Training Opt_acc epoch: 0.9831457827391032\n",
      "Training loss per 100 training steps: 0.003341076456810761\n",
      "Training Opt_acc epoch: 0.9832468715443878\n",
      "Training loss per 100 training steps: 0.003333945834458169\n",
      "Training Opt_acc epoch: 0.9833225569713634\n",
      "Training loss per 100 training steps: 0.003366162509846739\n",
      "Training Opt_acc epoch: 0.983203741379473\n",
      "Training loss per 100 training steps: 0.0033504713311127633\n",
      "Training Opt_acc epoch: 0.9833084657880778\n",
      "Training loss per 100 training steps: 0.0033335375446147545\n",
      "Training Opt_acc epoch: 0.9834186582332428\n",
      "Training loss per 100 training steps: 0.0033274764988376476\n",
      "Training Opt_acc epoch: 0.98347077174515\n",
      "Training loss per 100 training steps: 0.0033457765162077627\n",
      "Training Opt_acc epoch: 0.9834668411748065\n",
      "Training loss per 100 training steps: 0.003332069850138276\n",
      "Training Opt_acc epoch: 0.9835452996810863\n",
      "Training loss per 100 training steps: 0.003326125631263676\n",
      "Training Opt_acc epoch: 0.983579846646989\n",
      "Training loss per 100 training steps: 0.0033153631450262484\n",
      "Training Opt_acc epoch: 0.9836196389050417\n",
      "Training loss per 100 training steps: 0.0033321555392774767\n",
      "Training Opt_acc epoch: 0.9834425386637246\n",
      "Training loss per 100 training steps: 0.003314545286314088\n",
      "Training Opt_acc epoch: 0.9835354422284638\n",
      "Training loss per 100 training steps: 0.003330114393681342\n",
      "Training Opt_acc epoch: 0.9835677327919016\n",
      "Training loss per 100 training steps: 0.003325404292169269\n",
      "Training Opt_acc epoch: 0.983531288812354\n",
      "Training loss per 100 training steps: 0.0033240002022943884\n",
      "Training Opt_acc epoch: 0.9835544314213908\n",
      "Training loss per 100 training steps: 0.003314088478585602\n",
      "Training Opt_acc epoch: 0.9835993540085438\n",
      "Training loss per 100 training steps: 0.003299472645432656\n",
      "Training Opt_acc epoch: 0.9836462151890524\n",
      "Training loss per 100 training steps: 0.00328542647047757\n",
      "Training Opt_acc epoch: 0.9837376836994707\n",
      "Training loss per 100 training steps: 0.003361692267027523\n",
      "Training Opt_acc epoch: 0.9833394421115386\n",
      "Training loss per 100 training steps: 0.0033743004716334747\n",
      "Training Opt_acc epoch: 0.9833192309329284\n",
      "Training loss per 100 training steps: 0.0033595247261134728\n",
      "Training Opt_acc epoch: 0.9833581614298368\n",
      "Training loss per 100 training steps: 0.0033719401805800023\n",
      "Training Opt_acc epoch: 0.9832597502067376\n",
      "Training loss per 100 training steps: 0.003368708173405129\n",
      "Training Opt_acc epoch: 0.9832466114941691\n",
      "Training loss per 100 training steps: 0.0033882186646905224\n",
      "Training Opt_acc epoch: 0.9831585865683412\n",
      "Training loss per 100 training steps: 0.0033826327734662776\n",
      "Training Opt_acc epoch: 0.9830951424496287\n",
      "Training loss per 100 training steps: 0.0033976907454129307\n",
      "Training Opt_acc epoch: 0.9830387588563415\n",
      "Training loss per 100 training steps: 0.00338404690031545\n",
      "Training Opt_acc epoch: 0.9831269279167661\n",
      "Training loss per 100 training steps: 0.003368923871121711\n",
      "Training Opt_acc epoch: 0.9832274433663756\n",
      "Training loss per 100 training steps: 0.0033631405355019134\n",
      "Training Opt_acc epoch: 0.9832276298895497\n",
      "Training loss per 100 training steps: 0.0033672162095092625\n",
      "Training Opt_acc epoch: 0.9832250340588023\n",
      "Training loss per 100 training steps: 0.003361797380619327\n",
      "Training Opt_acc epoch: 0.9832816287352756\n",
      "Training loss per 100 training steps: 0.0033763990320906977\n",
      "Training Opt_acc epoch: 0.9831463572007066\n",
      "Training loss per 100 training steps: 0.003369606942801711\n",
      "Training Opt_acc epoch: 0.983142221874754\n",
      "Training loss per 100 training steps: 0.003405262671625029\n",
      "Training Opt_acc epoch: 0.9829410321864511\n",
      "Training loss per 100 training steps: 0.003402638892500568\n",
      "Training Opt_acc epoch: 0.9829438110878361\n",
      "Training loss per 100 training steps: 0.0034010201098799648\n",
      "Training Opt_acc epoch: 0.982847589273942\n",
      "Training loss per 100 training steps: 0.003403264607563953\n",
      "Training Opt_acc epoch: 0.9827801888938796\n",
      "Training loss per 100 training steps: 0.0033879602342909146\n",
      "Training Opt_acc epoch: 0.9827806924857887\n",
      "Training loss per 100 training steps: 0.003394698229393536\n",
      "Training Opt_acc epoch: 0.9825911677657224\n",
      "Training loss per 100 training steps: 0.003395934405209162\n",
      "Training Opt_acc epoch: 0.9825936870422144\n",
      "Training loss per 100 training steps: 0.003386899261658018\n",
      "Training Opt_acc epoch: 0.9825522663455506\n",
      "Training loss per 100 training steps: 0.003380606953945362\n",
      "Training Opt_acc epoch: 0.9825451987839052\n",
      "Training loss per 100 training steps: 0.003368069448404415\n",
      "Training Opt_acc epoch: 0.9825549508817268\n",
      "Training loss per 100 training steps: 0.0033727613237332817\n",
      "Training Opt_acc epoch: 0.9825465242504846\n",
      "Training loss per 100 training steps: 0.0033761292261773944\n",
      "Training Opt_acc epoch: 0.9825909935291938\n",
      "Training loss per 100 training steps: 0.0033629805908903564\n",
      "Training Opt_acc epoch: 0.9826720312806028\n",
      "Training loss per 100 training steps: 0.0033578817860779385\n",
      "Training Opt_acc epoch: 0.9827333415146012\n",
      "Training loss per 100 training steps: 0.003365635081297147\n",
      "Training Opt_acc epoch: 0.9826874056649865\n",
      "Training loss per 100 training steps: 0.003358170313672247\n",
      "Training Opt_acc epoch: 0.9827265872306048\n",
      "Training loss per 100 training steps: 0.0033622055431610258\n",
      "Training Opt_acc epoch: 0.9827211371281791\n",
      "Training loss per 100 training steps: 0.0033547984290858797\n",
      "Training Opt_acc epoch: 0.9827487717787387\n",
      "Training loss per 100 training steps: 0.0033479605542116706\n",
      "Training Opt_acc epoch: 0.982825021607284\n",
      "Training loss per 100 training steps: 0.003374445178094173\n",
      "Training Opt_acc epoch: 0.9828145951927652\n",
      "Training loss per 100 training steps: 0.0033682530529045734\n",
      "Training Opt_acc epoch: 0.982859753192258\n",
      "Training loss per 100 training steps: 0.0033655864382672197\n",
      "Training Opt_acc epoch: 0.9829175901022605\n",
      "Training loss per 100 training steps: 0.003360098085643872\n",
      "Training Opt_acc epoch: 0.982937515833009\n",
      "Training loss per 100 training steps: 0.0033480573524960174\n",
      "Training Opt_acc epoch: 0.9830024962614551\n",
      "Training loss per 100 training steps: 0.0033747168764641875\n",
      "Training Opt_acc epoch: 0.9828744601133595\n",
      "Training loss per 100 training steps: 0.003369275179503436\n",
      "Training Opt_acc epoch: 0.982911734521558\n",
      "Training loss per 100 training steps: 0.0033668831745317933\n",
      "Training Opt_acc epoch: 0.9829239633424064\n",
      "Training loss per 100 training steps: 0.0033720183274695797\n",
      "Training Opt_acc epoch: 0.9828833640548433\n",
      "Training loss per 100 training steps: 0.0033639114680714074\n",
      "Training Opt_acc epoch: 0.9829261186107304\n",
      "Training loss per 100 training steps: 0.0033665734754380165\n",
      "Training Opt_acc epoch: 0.9829196008816523\n",
      "Training loss per 100 training steps: 0.00335723803414141\n",
      "Training Opt_acc epoch: 0.9829469451636825\n",
      "Training loss per 100 training steps: 0.0033990844434534035\n",
      "Training Opt_acc epoch: 0.9828335720746185\n",
      "Training loss per 100 training steps: 0.0033924512361083233\n",
      "Training Opt_acc epoch: 0.9828362618755093\n",
      "Training loss epoch: 0.003387566486968782\n",
      "Training accuracy epoch: 0.9990758533429887\n",
      "Training Opt_acc epoch: 0.9828305779101681\n",
      "tr_zero_steps: 1080\n",
      "tr_opt_steps: 16420\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5e4fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy, eval_precision, eval_f1, eval_recall, eval_opt_acc, eval_opt_acc_n = 0, 0, 0, 0, 0, 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    opt_zero_steps, opt_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels) 遮罩 不會取無用\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,) 取最大機率的標記 num_label就消失 每批有效字元標記\n",
    "\n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(labels.cpu().numpy(), predictions.cpu().numpy(), labels=[0, 1]).ravel()\n",
    "            tmp_eval_opt_acc = tp/(fn+fp+tp)\n",
    "            \n",
    "            if fp !=0 or fn !=0:\n",
    "                for batch_id in range(len(ids.cpu().numpy())):\n",
    "                    print('\\n')\n",
    "                    sent = []\n",
    "                    for word_id in ids.cpu().numpy()[batch_id]:\n",
    "                        sent.append(re_vocab[word_id])\n",
    "                    print(''.join(sent))\n",
    "                    print(labels.cpu().numpy())\n",
    "                    print(predictions.cpu().numpy())\n",
    "                    print(fp,fn)\n",
    "                    \n",
    "            \n",
    "            if np.isnan(tmp_eval_opt_acc) == True:\n",
    "                tmp_eval_opt_acc = 1\n",
    "                opt_zero_steps += 1\n",
    "#                 print('opt_zero_steps:',opt_zero_steps)\n",
    "            else:\n",
    "                opt_steps += 1\n",
    "                eval_opt_acc_n += tmp_eval_opt_acc\n",
    "                \n",
    "                if idx % 100==0:\n",
    "                    loss_step = eval_loss/nb_eval_steps\n",
    "                    print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "                    opt_acc_step = eval_opt_acc_n / opt_steps\n",
    "                    print(f\"Validation Opt_acc epoch: {opt_acc_step}\")\n",
    "                \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    print(eval_opt_acc_n)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    eval_opt_acc_n = eval_opt_acc_n / opt_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation Opt_Acc_n: {eval_opt_acc_n}\")\n",
    "    print('zero_steps:',opt_zero_steps)\n",
    "    print('opt_steps:',opt_steps)\n",
    "    \n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1afd5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0013482865548943193\n",
      "Validation Opt_acc epoch: 0.9945334022038567\n",
      "Validation loss per 100 evaluation steps: 0.001603166039108992\n",
      "Validation Opt_acc epoch: 0.9926189443631303\n",
      "Validation loss per 100 evaluation steps: 0.0025635404378777714\n",
      "Validation Opt_acc epoch: 0.9908843403787223\n",
      "Validation loss per 100 evaluation steps: 0.0028955691240242654\n",
      "Validation Opt_acc epoch: 0.9905908513380837\n",
      "Validation loss per 100 evaluation steps: 0.0031650777298670666\n",
      "Validation Opt_acc epoch: 0.9907287574929714\n",
      "Validation loss per 100 evaluation steps: 0.0031012481898354868\n",
      "Validation Opt_acc epoch: 0.9903057120996052\n",
      "Validation loss per 100 evaluation steps: 0.003388801389107862\n",
      "Validation Opt_acc epoch: 0.9884572413053426\n",
      "Validation loss per 100 evaluation steps: 0.0037393818142741936\n",
      "Validation Opt_acc epoch: 0.9876095615583133\n",
      "Validation loss per 100 evaluation steps: 0.003480167423794534\n",
      "Validation Opt_acc epoch: 0.9877485153301785\n",
      "Validation loss per 100 evaluation steps: 0.003305176218714008\n",
      "Validation Opt_acc epoch: 0.9884871703310617\n",
      "Validation loss per 100 evaluation steps: 0.0031989459384576726\n",
      "Validation Opt_acc epoch: 0.9885991976109272\n",
      "Validation loss per 100 evaluation steps: 0.0032192692749236896\n",
      "Validation Opt_acc epoch: 0.9879175175181018\n",
      "Validation loss per 100 evaluation steps: 0.0029838554659300733\n",
      "Validation Opt_acc epoch: 0.9879838476446553\n",
      "Validation loss per 100 evaluation steps: 0.0030897909157084204\n",
      "Validation Opt_acc epoch: 0.9876423569996983\n",
      "Validation loss per 100 evaluation steps: 0.0029395083597235252\n",
      "Validation Opt_acc epoch: 0.9884545160288163\n",
      "Validation loss per 100 evaluation steps: 0.002843715292971275\n",
      "Validation Opt_acc epoch: 0.9887760954057311\n",
      "Validation loss per 100 evaluation steps: 0.0029896413282440894\n",
      "Validation Opt_acc epoch: 0.9878593324466731\n",
      "Validation loss per 100 evaluation steps: 0.0030796225173509873\n",
      "Validation Opt_acc epoch: 0.9876027223867467\n",
      "Validation loss per 100 evaluation steps: 0.0030348898822488723\n",
      "Validation Opt_acc epoch: 0.9877188590344963\n",
      "Validation loss per 100 evaluation steps: 0.0032591641678481782\n",
      "Validation Opt_acc epoch: 0.9858789997239599\n",
      "Validation loss per 100 evaluation steps: 0.0031568114448027006\n",
      "Validation Opt_acc epoch: 0.986235690950224\n",
      "Validation loss per 100 evaluation steps: 0.003162349047308704\n",
      "Validation Opt_acc epoch: 0.9863692440885011\n",
      "Validation loss per 100 evaluation steps: 0.003086898599938714\n",
      "Validation Opt_acc epoch: 0.9866793472310782\n",
      "Validation loss per 100 evaluation steps: 0.0030703618150963786\n",
      "Validation Opt_acc epoch: 0.9867814590609073\n",
      "Validation loss per 100 evaluation steps: 0.0030585396817872346\n",
      "Validation Opt_acc epoch: 0.986617013732748\n",
      "Validation loss per 100 evaluation steps: 0.0030001245744601776\n",
      "Validation Opt_acc epoch: 0.986843562515707\n",
      "Validation loss per 100 evaluation steps: 0.003257736886620236\n",
      "Validation Opt_acc epoch: 0.9869380408540632\n",
      "Validation loss per 100 evaluation steps: 0.0033029662526797006\n",
      "Validation Opt_acc epoch: 0.9869996590809827\n",
      "Validation loss per 100 evaluation steps: 0.0032062795238193367\n",
      "Validation Opt_acc epoch: 0.9874114146408959\n",
      "Validation loss per 100 evaluation steps: 0.003268988046787254\n",
      "Validation Opt_acc epoch: 0.9872488439987446\n",
      "Validation loss per 100 evaluation steps: 0.00324325741892686\n",
      "Validation Opt_acc epoch: 0.987241891910286\n",
      "Validation loss per 100 evaluation steps: 0.003245089802666274\n",
      "Validation Opt_acc epoch: 0.9873037971030565\n",
      "Validation loss per 100 evaluation steps: 0.0032676228522086296\n",
      "Validation Opt_acc epoch: 0.9868547409717144\n",
      "Validation loss per 100 evaluation steps: 0.0032318840369039447\n",
      "Validation Opt_acc epoch: 0.9868277833459274\n",
      "Validation loss per 100 evaluation steps: 0.00330006840985744\n",
      "Validation Opt_acc epoch: 0.9864974493813977\n",
      "Validation loss per 100 evaluation steps: 0.003249276185877853\n",
      "Validation Opt_acc epoch: 0.9867505092323489\n",
      "Validation loss per 100 evaluation steps: 0.003223243204880323\n",
      "Validation Opt_acc epoch: 0.986654143957526\n",
      "Validation loss per 100 evaluation steps: 0.003192890146155138\n",
      "Validation Opt_acc epoch: 0.9867812783140285\n",
      "Validation loss per 100 evaluation steps: 0.0031715857674896285\n",
      "Validation Opt_acc epoch: 0.9867677275531622\n",
      "Validation loss per 100 evaluation steps: 0.0031268126707508534\n",
      "Validation Opt_acc epoch: 0.986974682360707\n",
      "Validation loss per 100 evaluation steps: 0.003152919004046635\n",
      "Validation Opt_acc epoch: 0.986970204086586\n",
      "Validation loss per 100 evaluation steps: 0.0034491222021708736\n",
      "Validation Opt_acc epoch: 0.9859935349051564\n",
      "Validation loss per 100 evaluation steps: 0.0035689120330292612\n",
      "Validation Opt_acc epoch: 0.9853822315855466\n",
      "Validation loss per 100 evaluation steps: 0.0036751205381392042\n",
      "Validation Opt_acc epoch: 0.9849402463546111\n",
      "Validation loss per 100 evaluation steps: 0.003542605559205203\n",
      "Validation Opt_acc epoch: 0.9854703957963875\n",
      "Validation loss per 100 evaluation steps: 0.003469398133139352\n",
      "Validation Opt_acc epoch: 0.985781818787046\n",
      "Validation loss per 100 evaluation steps: 0.003535175693003408\n",
      "Validation Opt_acc epoch: 0.9858508367945799\n",
      "Validation loss per 100 evaluation steps: 0.0034493914654534033\n",
      "Validation Opt_acc epoch: 0.9856716493620339\n",
      "Validation loss per 100 evaluation steps: 0.0034180251982969673\n",
      "Validation Opt_acc epoch: 0.9857681777550917\n",
      "Validation loss per 100 evaluation steps: 0.00335380472172137\n",
      "Validation Opt_acc epoch: 0.9860682983868859\n",
      "Validation loss per 100 evaluation steps: 0.00333253553538505\n",
      "Validation Opt_acc epoch: 0.985765109581318\n",
      "Validation loss per 100 evaluation steps: 0.0032706811649774225\n",
      "Validation Opt_acc epoch: 0.9860860692503013\n",
      "Validation loss per 100 evaluation steps: 0.003244795324941157\n",
      "Validation Opt_acc epoch: 0.9861802702764968\n",
      "Validation loss per 100 evaluation steps: 0.0032098116986482455\n",
      "Validation Opt_acc epoch: 0.986288531971699\n",
      "Validation loss per 100 evaluation steps: 0.003210664918944145\n",
      "Validation Opt_acc epoch: 0.986280409323188\n",
      "Validation loss per 100 evaluation steps: 0.0031857878308255696\n",
      "Validation Opt_acc epoch: 0.9863194359255951\n",
      "Validation loss per 100 evaluation steps: 0.003262648095393517\n",
      "Validation Opt_acc epoch: 0.9861415242734972\n",
      "Validation loss per 100 evaluation steps: 0.003265240916249438\n",
      "Validation Opt_acc epoch: 0.9861293856849935\n",
      "Validation loss per 100 evaluation steps: 0.0032370807251485183\n",
      "Validation Opt_acc epoch: 0.9862470017734014\n",
      "Validation loss per 100 evaluation steps: 0.0032033945659567143\n",
      "Validation Opt_acc epoch: 0.9863434915060826\n",
      "Validation loss per 100 evaluation steps: 0.0033208135296595113\n",
      "Validation Opt_acc epoch: 0.9861256590044818\n",
      "Validation loss per 100 evaluation steps: 0.0032707227199605484\n",
      "Validation Opt_acc epoch: 0.9863500508295853\n",
      "Validation loss per 100 evaluation steps: 0.003248626637984622\n",
      "Validation Opt_acc epoch: 0.9864086233785163\n",
      "Validation loss per 100 evaluation steps: 0.0032702969449803954\n",
      "Validation Opt_acc epoch: 0.9862481245883048\n",
      "Validation loss per 100 evaluation steps: 0.0033170849876099184\n",
      "Validation Opt_acc epoch: 0.9860686057697399\n",
      "Validation loss per 100 evaluation steps: 0.0032968264396593316\n",
      "Validation Opt_acc epoch: 0.9861717887607249\n",
      "Validation loss per 100 evaluation steps: 0.0032407975381922786\n",
      "Validation Opt_acc epoch: 0.9864297305160025\n",
      "Validation loss per 100 evaluation steps: 0.003295453987784668\n",
      "Validation Opt_acc epoch: 0.9863758164014784\n",
      "Validation loss per 100 evaluation steps: 0.003285701531633906\n",
      "Validation Opt_acc epoch: 0.9863717808753747\n",
      "Validation loss per 100 evaluation steps: 0.0032817761135655755\n",
      "Validation Opt_acc epoch: 0.9863974652954225\n",
      "Validation loss per 100 evaluation steps: 0.0033179401057268822\n",
      "Validation Opt_acc epoch: 0.9860283801833494\n",
      "Validation loss per 100 evaluation steps: 0.0033226516763310574\n",
      "Validation Opt_acc epoch: 0.9860672895824472\n",
      "Validation loss per 100 evaluation steps: 0.003292937695427365\n",
      "Validation Opt_acc epoch: 0.9861268869242835\n",
      "Validation loss per 100 evaluation steps: 0.0032753280616109317\n",
      "Validation Opt_acc epoch: 0.9861434238383664\n",
      "Validation loss per 100 evaluation steps: 0.0032551048453864223\n",
      "Validation Opt_acc epoch: 0.9862091649540233\n",
      "Validation loss per 100 evaluation steps: 0.003291229481209106\n",
      "Validation Opt_acc epoch: 0.986183296259226\n",
      "Validation loss per 100 evaluation steps: 0.003268726173624174\n",
      "Validation Opt_acc epoch: 0.9862836062907402\n",
      "Validation loss per 100 evaluation steps: 0.0033105849605103476\n",
      "Validation Opt_acc epoch: 0.9859874264034921\n",
      "Validation loss per 100 evaluation steps: 0.0033083351269194636\n",
      "Validation Opt_acc epoch: 0.9859988863895767\n",
      "Validation loss per 100 evaluation steps: 0.0033118925637567066\n",
      "Validation Opt_acc epoch: 0.9858980959110001\n",
      "Validation loss per 100 evaluation steps: 0.003329402329955199\n",
      "Validation Opt_acc epoch: 0.9857707640104387\n",
      "Validation loss per 100 evaluation steps: 0.0034738037338724243\n",
      "Validation Opt_acc epoch: 0.985854735640797\n",
      "Validation loss per 100 evaluation steps: 0.0034790207020411507\n",
      "Validation Opt_acc epoch: 0.9855322518493821\n",
      "Validation loss per 100 evaluation steps: 0.0034412969879889133\n",
      "Validation Opt_acc epoch: 0.9855834744463653\n",
      "Validation loss per 100 evaluation steps: 0.003421722456043379\n",
      "Validation Opt_acc epoch: 0.9856467805564438\n",
      "Validation loss per 100 evaluation steps: 0.003413346815047695\n",
      "Validation Opt_acc epoch: 0.9856921507810209\n",
      "Validation loss per 100 evaluation steps: 0.003403816929742647\n",
      "Validation Opt_acc epoch: 0.9856923084023996\n",
      "Validation loss per 100 evaluation steps: 0.0034541066062568062\n",
      "Validation Opt_acc epoch: 0.9855391328350656\n",
      "Validation loss per 100 evaluation steps: 0.003455752831823983\n",
      "Validation Opt_acc epoch: 0.9854963875118369\n",
      "Validation loss per 100 evaluation steps: 0.0034223797551118833\n",
      "Validation Opt_acc epoch: 0.9855601567427298\n",
      "Validation loss per 100 evaluation steps: 0.0033983816443729727\n",
      "Validation Opt_acc epoch: 0.9856410720958496\n",
      "Validation loss per 100 evaluation steps: 0.003386127066859832\n",
      "Validation Opt_acc epoch: 0.9856883798201787\n",
      "Validation loss per 100 evaluation steps: 0.003371653285118556\n",
      "Validation Opt_acc epoch: 0.9857091016453232\n",
      "Validation loss per 100 evaluation steps: 0.0033578898561259364\n",
      "Validation Opt_acc epoch: 0.985778206634002\n",
      "Validation loss per 100 evaluation steps: 0.003358823971062692\n",
      "Validation Opt_acc epoch: 0.9855427432269872\n",
      "Validation loss per 100 evaluation steps: 0.003334162690366899\n",
      "Validation Opt_acc epoch: 0.985596187793363\n",
      "Validation loss per 100 evaluation steps: 0.003311912735147643\n",
      "Validation Opt_acc epoch: 0.9855242919325906\n",
      "Validation loss per 100 evaluation steps: 0.003280911066419446\n",
      "Validation Opt_acc epoch: 0.9856794373193262\n",
      "Validation loss per 100 evaluation steps: 0.003274590824647797\n",
      "Validation Opt_acc epoch: 0.9856644828864614\n",
      "Validation loss per 100 evaluation steps: 0.0033177994171737124\n",
      "Validation Opt_acc epoch: 0.985585665514797\n",
      "Validation loss per 100 evaluation steps: 0.0033003123060542303\n",
      "Validation Opt_acc epoch: 0.985635972014793\n",
      "Validation loss per 100 evaluation steps: 0.0032969894663635937\n",
      "Validation Opt_acc epoch: 0.9856043965522137\n",
      "Validation loss per 100 evaluation steps: 0.003316726847003893\n",
      "Validation Opt_acc epoch: 0.9855508672990171\n",
      "Validation loss per 100 evaluation steps: 0.0033188792805395537\n",
      "Validation Opt_acc epoch: 0.985559196196911\n",
      "Validation loss per 100 evaluation steps: 0.003366367755197402\n",
      "Validation Opt_acc epoch: 0.9855702809162359\n",
      "Validation loss per 100 evaluation steps: 0.003360450082800023\n",
      "Validation Opt_acc epoch: 0.9855286235624112\n",
      "Validation loss per 100 evaluation steps: 0.003346701954360199\n",
      "Validation Opt_acc epoch: 0.9855513562576215\n",
      "Validation loss per 100 evaluation steps: 0.003400799347648629\n",
      "Validation Opt_acc epoch: 0.9854635327239853\n",
      "Validation loss per 100 evaluation steps: 0.0033941145227115756\n",
      "Validation Opt_acc epoch: 0.9853977968202202\n",
      "Validation loss per 100 evaluation steps: 0.003356841477825161\n",
      "Validation Opt_acc epoch: 0.9855714126638537\n",
      "Validation loss per 100 evaluation steps: 0.0033450145273024724\n",
      "Validation Opt_acc epoch: 0.9856308636387222\n",
      "Validation loss per 100 evaluation steps: 0.0033333526185419676\n",
      "Validation Opt_acc epoch: 0.9856017812402225\n",
      "Validation loss per 100 evaluation steps: 0.0033126910983763453\n",
      "Validation Opt_acc epoch: 0.9856637620750748\n",
      "Validation loss per 100 evaluation steps: 0.0033069224431036886\n",
      "Validation Opt_acc epoch: 0.9856387096755692\n",
      "Validation loss per 100 evaluation steps: 0.0032746391776156346\n",
      "Validation Opt_acc epoch: 0.9857725926115984\n",
      "Validation loss per 100 evaluation steps: 0.0033073614694348287\n",
      "Validation Opt_acc epoch: 0.985651045893423\n",
      "Validation loss per 100 evaluation steps: 0.003310374882459856\n",
      "Validation Opt_acc epoch: 0.9856600824921076\n",
      "Validation loss per 100 evaluation steps: 0.0033174337370247137\n",
      "Validation Opt_acc epoch: 0.9856293216761103\n",
      "Validation loss per 100 evaluation steps: 0.003337281543641246\n",
      "Validation Opt_acc epoch: 0.9855520625670309\n",
      "Validation loss per 100 evaluation steps: 0.003333719327015919\n",
      "Validation Opt_acc epoch: 0.9854808351484236\n",
      "Validation loss per 100 evaluation steps: 0.00333026908192834\n",
      "Validation Opt_acc epoch: 0.9853899054177003\n",
      "Validation loss per 100 evaluation steps: 0.0033193628936580037\n",
      "Validation Opt_acc epoch: 0.9854429965044784\n",
      "Validation loss per 100 evaluation steps: 0.0033343146633226526\n",
      "Validation Opt_acc epoch: 0.98538387799798\n",
      "Validation loss per 100 evaluation steps: 0.0033129887126775487\n",
      "Validation Opt_acc epoch: 0.985420243459159\n",
      "Validation loss per 100 evaluation steps: 0.003292602542042919\n",
      "Validation Opt_acc epoch: 0.9854756300488692\n",
      "Validation loss per 100 evaluation steps: 0.0032583416764688793\n",
      "Validation Opt_acc epoch: 0.9856317850292705\n",
      "Validation loss per 100 evaluation steps: 0.0032564977243787125\n",
      "Validation Opt_acc epoch: 0.9856209132583436\n",
      "Validation loss per 100 evaluation steps: 0.003264641169029183\n",
      "Validation Opt_acc epoch: 0.9855620308498696\n",
      "Validation loss per 100 evaluation steps: 0.003255743678099833\n",
      "Validation Opt_acc epoch: 0.9855636279468047\n",
      "Validation loss per 100 evaluation steps: 0.003246638607772656\n",
      "Validation Opt_acc epoch: 0.9855853471351529\n",
      "Validation loss per 100 evaluation steps: 0.0032752696968721133\n",
      "Validation Opt_acc epoch: 0.9855067411605494\n",
      "Validation loss per 100 evaluation steps: 0.0032614832206006718\n",
      "Validation Opt_acc epoch: 0.9855397893735112\n",
      "Validation loss per 100 evaluation steps: 0.003231887336146703\n",
      "Validation Opt_acc epoch: 0.985692070975253\n",
      "Validation loss per 100 evaluation steps: 0.003220536235854423\n",
      "Validation Opt_acc epoch: 0.9857434305768974\n",
      "Validation loss per 100 evaluation steps: 0.0032110160849515754\n",
      "Validation Opt_acc epoch: 0.9857892418369547\n",
      "Validation loss per 100 evaluation steps: 0.003193800792286951\n",
      "Validation Opt_acc epoch: 0.9858293180384564\n",
      "Validation loss per 100 evaluation steps: 0.003179201501027763\n",
      "Validation Opt_acc epoch: 0.9859094653920281\n",
      "Validation loss per 100 evaluation steps: 0.0031888095860584833\n",
      "Validation Opt_acc epoch: 0.9859036148697927\n",
      "Validation loss per 100 evaluation steps: 0.003171288584325217\n",
      "Validation Opt_acc epoch: 0.9859770067544068\n",
      "Validation loss per 100 evaluation steps: 0.0031508508600667155\n",
      "Validation Opt_acc epoch: 0.9859953385771819\n",
      "Validation loss per 100 evaluation steps: 0.003154311374941277\n",
      "Validation Opt_acc epoch: 0.9858879875518529\n",
      "Validation loss per 100 evaluation steps: 0.003181880194614779\n",
      "Validation Opt_acc epoch: 0.9857823783029928\n",
      "Validation loss per 100 evaluation steps: 0.0031778722537819624\n",
      "Validation Opt_acc epoch: 0.9858282576274303\n",
      "Validation loss per 100 evaluation steps: 0.003169618205908738\n",
      "Validation Opt_acc epoch: 0.9858678278412077\n",
      "Validation loss per 100 evaluation steps: 0.0031664934061976496\n",
      "Validation Opt_acc epoch: 0.9858412574354088\n",
      "Validation loss per 100 evaluation steps: 0.0031602116261152933\n",
      "Validation Opt_acc epoch: 0.9858279806395774\n",
      "Validation loss per 100 evaluation steps: 0.0031593817970949705\n",
      "Validation Opt_acc epoch: 0.9858289091070968\n",
      "Validation loss per 100 evaluation steps: 0.0031401567310674616\n",
      "Validation Opt_acc epoch: 0.9858776681935426\n",
      "Validation loss per 100 evaluation steps: 0.0031345228891071974\n",
      "Validation Opt_acc epoch: 0.9857941521876094\n",
      "14742.856357653294\n",
      "Validation Loss: 0.003129259549147675\n",
      "Validation Accuracy: 0.9992463372497774\n",
      "Validation Opt_Acc_n: 0.9858145341125573\n",
      "zero_steps: 15045\n",
      "opt_steps: 14955\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887af8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b63b8558",
   "metadata": {},
   "source": [
    "## 讀檔 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9e478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home3/ningyuu/jpt/論文區'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138834ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "model = BertForTokenClassification.from_pretrained('cl-tohoku/bert-large-japanese', num_labels=len(labels_to_ids))\n",
    "# model.to(device)\n",
    "model1 = model.load_state_dict(torch.load('model_V-Pla-present/pytorch_model.bin'))\n",
    "model1 = model.to(device)\n",
    "model1 = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ff3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('cl-tohoku/bert-large-japanese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f923bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8775a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', '担', '当', '者', 'で', 'は', 'な', 'い', 'の', 'で', '、', 'そ', 'れ', 'に', 'つ', 'い', 'て', 'は', '分', 'か', 'り', 'ま', 'せ', 'ん', '。']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"私は担当者ではないので、それについては分かりません。\"\n",
    "se = [sent for sent in sentence]\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8340f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ま', 'た', '、', '長', '時', '間', 'の', '入', '浴', 'は', '体', '力', 'を', '消', '耗', 'す', 'る', 'の', 'で', '、', '短', '時', '間', 'に', 'と', 'ど', 'め', 'る', 'へ', 'き', 'て', 'あ', 'る', '。']\n",
      "['Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｏ', 'Ｏ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｏ', 'Ｏ', 'Ｏ', 'Ｘ', 'Ｘ', 'Ｘ', 'Ｏ', 'Ｏ', 'Ｘ']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"また、長時間の入浴は体力を消耗するので、短時間にとどめるへきてある。\"\n",
    "\n",
    "inputs = tokenizer([word for word in sentence],\n",
    "                    is_pretokenized=True, \n",
    "                    return_offsets_mapping=True, \n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=128,\n",
    "                    return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "    #only predictions on first word pieces are important\n",
    "    if mapping[0] == 0 and mapping[1] != 0:\n",
    "        prediction.append(token_pred[1])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# print(sentence.split())\n",
    "print([word for word in sentence])\n",
    "# print(inputs['input_ids'])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "088385b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>ま</td>\n",
       "      <td>た</td>\n",
       "      <td>、</td>\n",
       "      <td>長</td>\n",
       "      <td>時</td>\n",
       "      <td>間</td>\n",
       "      <td>の</td>\n",
       "      <td>入</td>\n",
       "      <td>浴</td>\n",
       "      <td>は</td>\n",
       "      <td>体</td>\n",
       "      <td>力</td>\n",
       "      <td>を</td>\n",
       "      <td>消</td>\n",
       "      <td>耗</td>\n",
       "      <td>す</td>\n",
       "      <td>る</td>\n",
       "      <td>の</td>\n",
       "      <td>で</td>\n",
       "      <td>、</td>\n",
       "      <td>短</td>\n",
       "      <td>時</td>\n",
       "      <td>間</td>\n",
       "      <td>に</td>\n",
       "      <td>と</td>\n",
       "      <td>ど</td>\n",
       "      <td>め</td>\n",
       "      <td>る</td>\n",
       "      <td>へ</td>\n",
       "      <td>き</td>\n",
       "      <td>て</td>\n",
       "      <td>あ</td>\n",
       "      <td>る</td>\n",
       "      <td>。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｘ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｏ</td>\n",
       "      <td>Ｘ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22  \\\n",
       "Tokens  ま  た  、  長  時  間  の  入  浴  は  体  力  を  消  耗  す  る  の  で  、  短  時  間   \n",
       "Tags    Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ  Ｘ   \n",
       "\n",
       "       23 24 25 26 27 28 29 30 31 32 33  \n",
       "Tokens  に  と  ど  め  る  へ  き  て  あ  る  。  \n",
       "Tags    Ｘ  Ｘ  Ｏ  Ｏ  Ｏ  Ｘ  Ｘ  Ｘ  Ｏ  Ｏ  Ｘ  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame([[word for word in sentence], prediction], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cca446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n",
      "This tutorial is completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"./model_V-Pla-present\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# save vocabulary of the tokenizer\n",
    "tokenizer.save_vocabulary(directory)\n",
    "# save the model weights and its configuration file\n",
    "model.save_pretrained(directory)\n",
    "print('All files saved')\n",
    "print('This tutorial is completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c5ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258344bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
